{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSPI_gym.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyObuM/ENmC1KyRmByXmampx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudia-viaro/optimal_switching/blob/main/LSPI_gym.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Least Square Policy Iteration with gym\n",
        "# not really doing what I want atm"
      ],
      "metadata": {
        "id": "CvrU84LlWHM5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym-stopping"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPGsyT96ezKy",
        "outputId": "7dbaf290-3131-4f16-8bd7-7ffe5d5c9743"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym-stopping\n",
            "  Downloading gym_stopping-0.0.1-py3-none-any.whl (4.3 kB)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from gym-stopping) (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym->gym-stopping) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->gym-stopping) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->gym-stopping) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym->gym-stopping) (1.21.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->gym-stopping) (0.16.0)\n",
            "Installing collected packages: gym-stopping\n",
            "Successfully installed gym-stopping-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "#Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree\n",
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "\n",
        "import gym\n",
        "import gym_stopping\n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from gym import error, spaces, utils\n",
        "import pandas.testing as tm\n",
        "gym.logger.set_level(40)\n"
      ],
      "metadata": {
        "id": "gUzrAlZqB23L"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, env, preprocess_obs=None):\n",
        "        if preprocess_obs is None:\n",
        "            preprocess_obs = lambda x: x\n",
        "\n",
        "        self.env = env\n",
        "        self.action_size = self.env.action_space.n\n",
        "        self.preprocess_obs = preprocess_obs\n",
        "        self.features_size = self.get_features_size()\n",
        "        self.init_weights()\n",
        "\n",
        "    # take initial weights to be normally distributed (could do zero as well)\n",
        "    def init_weights(self, scale=1.):\n",
        "        size = self.features_size * self.action_size\n",
        "        self.weights = np.random.normal(size=size, scale=scale)\n",
        "\n",
        "    def set_weights(self, weights):\n",
        "        self.weights = weights\n",
        "\n",
        "    def get_features_size(self):\n",
        "        obs = self.env.observation_space.sample()[0]\n",
        "        features = self.get_features(obs)\n",
        "        return len(features)\n",
        "\n",
        "    def get_features(self, obs):\n",
        "        obs = self.preprocess_obs(obs)\n",
        "        return obs\n",
        "\n",
        "    def predict(self, obs):\n",
        "        values = np.dot(\n",
        "            self.weights.reshape(self.action_size, self.features_size),\n",
        "            self.get_features(obs))\n",
        "        action = np.argmax(values)\n",
        "        return action"
      ],
      "metadata": {
        "id": "JLPcYARFB_yB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PolynomialAgent(Agent):\n",
        "    def __init__(self, env, degree, preprocess_obs=None):\n",
        "        self.poly = PolynomialFeatures(degree)\n",
        "        super(PolynomialAgent, self).__init__(env, preprocess_obs)\n",
        "\n",
        "    def _get_features(self, obs):\n",
        "        if not type(obs) in [np.ndarray, list, tuple]:\n",
        "            obs = [obs]\n",
        "        return self.poly.fit_transform([obs])[0] # fit_transform just return the matrix of transformed values according to the degree chosen"
      ],
      "metadata": {
        "id": "p7Bzb5teB-6-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RadialAgent(Agent):\n",
        "    def __init__(self, env, centers, sigma=1., preprocess_obs=None):\n",
        "        self.centers = centers\n",
        "        self.sigma2 = sigma**2\n",
        "        super(RadialAgent, self).__init__(env, preprocess_obs)\n",
        "\n",
        "    def _get_features(self, obs):\n",
        "        dists = np.power(self.centers - obs, 2)\n",
        "        rbfs = np.exp(-dists.sum(1) / (2 * self.sigma2))\n",
        "        return np.append(rbfs, [1.])\n",
        "\n",
        "    @staticmethod\n",
        "    def get_centers_from_grids(grids):\n",
        "        return np.array(list(itertools.product(*grids)))"
      ],
      "metadata": {
        "id": "Q2oMS5jPB3Hq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sample = namedtuple('Sample', ['s', 'a', 'r', 's_'])\n",
        "\n",
        "\n",
        "class LSPolicyIteration:\n",
        "    def __init__(self,\n",
        "                 env,\n",
        "                 agent,\n",
        "                 gamma,\n",
        "                 memory_size,\n",
        "                 memory_type='sample',\n",
        "                 eval_type='batch'):\n",
        "        \"\"\"Least-Squares Policy Iteration algorithm\n",
        "        Args:\n",
        "            env (gym.Env): gym environment.\n",
        "            agent (lspi.agents.Agent): features policy.\n",
        "            gamma (float): discount factor.\n",
        "            memory_size (int): number of training samples/episodes.\n",
        "            memory_type (str, optional): samples collecting method. Defaults to 'sample'.\n",
        "            eval_type (str, optional): policy evaluation method. Defaults to 'batch'.\n",
        "        \"\"\"\n",
        "        if not memory_type in ['sample', 'episode']:\n",
        "            raise ValueError(\n",
        "                \"memory_type can take values ['sample','episode']\")\n",
        "        if not eval_type in ['iterative', 'sherman_morrison', 'batch']:\n",
        "            raise ValueError(\n",
        "                \"eval_type can take values ['iterative','sherman_morrison','batch']\"\n",
        "            )\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.agent = agent\n",
        "        self.memory_size = memory_size\n",
        "        self.eval_type = eval_type\n",
        "        self.memory_type = memory_type\n",
        "\n",
        "    # for a numb of times, record some trajectories in the object memory\n",
        "    def init_memory(self):\n",
        "        self.memory = []\n",
        "        count = 0\n",
        "        done = True\n",
        "        while count < (self.memory_size + 1):\n",
        "            if done:\n",
        "                obs[0] = self.env.reset() # spot price\n",
        "                if self.memory_type == 'episode':\n",
        "                    count += 1\n",
        "            action = self.env.action_space.sample()\n",
        "            next_obs, reward, done, _ = self.env.step(action)\n",
        "            self.memory.append(Sample(obs, action, reward, next_obs[0]))\n",
        "            obs = next_obs\n",
        "            if self.memory_type == 'sample':\n",
        "                count += 1\n",
        "\n",
        "        if self.eval_type == 'batch':\n",
        "            k = self.agent.features_size # length of a sampled obs\n",
        "            nActions = self.agent.action_size\n",
        "            # matrix A\n",
        "            self.A_all = np.zeros(\n",
        "                (len(self.memory), nActions, k * nActions, k * nActions))\n",
        "            self.b_all = np.zeros(k * nActions)\n",
        "            for idx, sample in enumerate(self.memory):\n",
        "                # state features\n",
        "                feat_s = np.zeros(k * nActions)\n",
        "                a = sample.a\n",
        "                feat_s[a * k:(a + 1) * k] = self.agent.get_features(sample.s)\n",
        "                # next state features\n",
        "                feat_ = self.agent.get_features(sample.s_)\n",
        "                for a_ in range(nActions):\n",
        "                    feat_s_ = np.zeros(k * nActions)\n",
        "                    feat_s_[a_ * k:(a_ + 1) * k] = feat_\n",
        "                    self.A_all[idx, a_, :, :] = np.outer(\n",
        "                        feat_s, feat_s - self.gamma * feat_s_)\n",
        "                # reward features\n",
        "                self.b_all += sample.r * feat_s\n",
        "\n",
        "    def eval(self):\n",
        "        k = self.agent.features_size # length of a sampled obs\n",
        "        nActions = self.agent.action_size\n",
        "        if self.eval_type == 'iterative':\n",
        "            A = np.zeros((k * nActions, k * nActions))\n",
        "            b = np.zeros(k * nActions)\n",
        "            for sample in self.memory:\n",
        "                # state features\n",
        "                feat_s = np.zeros(k * nActions)\n",
        "                a = sample.a\n",
        "                feat_s[a * k:(a + 1) * k] = self.preprocess_ob(sample.s)\n",
        "                # next state features\n",
        "                feat_s_ = np.zeros(k * nActions)\n",
        "                a_ = self.agent.predict(sample.s_)\n",
        "                feat_s_[a_ * k:(a_ + 1) * k] = self.agent.preprocess_ob(\n",
        "                    sample.s_)\n",
        "                # update parameters\n",
        "                A += np.outer(feat_s, feat_s - self.gamma * feat_s_)\n",
        "                b += sample.r * feat_s\n",
        "            w = np.linalg.solve(A, b)\n",
        "        \n",
        "        elif self.eval_type == 'batch':\n",
        "            A = np.array([\n",
        "                self.A_all[idx, self.agent.predict(sample.s_)]\n",
        "                for idx, sample in enumerate(self.memory)\n",
        "            ]).sum(0)\n",
        "            b = self.b_all\n",
        "            w = np.linalg.solve(A, b)\n",
        "        return w\n",
        "\n",
        "    def train_step(self):\n",
        "        w = self.eval()\n",
        "        self.agent.set_weights(w)"
      ],
      "metadata": {
        "id": "YXDFNDmOF5uo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_policy(agent, env, max_length=1000, n_eval_episodes=10):\n",
        "    \"\"\"Runs policy for ``n_eval_episodes`` episodes.\n",
        "    \n",
        "    Adapted from :\n",
        "    https://stable-baselines.readthedocs.io/en/master/_modules/stable_baselines/common/evaluation.html\n",
        "    Args:\n",
        "        agent (lspi.agents.Agent): features policy.\n",
        "        env (gym.Env): gym environment.\n",
        "        max_length (int, optional): maximum episode length. Defaults to 1000.\n",
        "        n_eval_episodes (int, optional): number of episode to evaluate the agent. Defaults to 10.\n",
        "    Returns:\n",
        "        episode_rewards (List[float]): list of reward per episode\n",
        "        episode_lengths (List[int]): list of length per episode\n",
        "    \"\"\"\n",
        "    episode_rewards, episode_lengths = [], []\n",
        "    while len(episode_rewards) < n_eval_episodes:\n",
        "        obs[0] = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0.0\n",
        "        episode_length = 0\n",
        "        while not (done or episode_length == max_length):\n",
        "            action = agent.predict(obs)\n",
        "            obs, reward, done, info = env.step(action)\n",
        "            episode_reward += reward\n",
        "            episode_length += 1\n",
        "        episode_rewards.append(episode_reward)\n",
        "        episode_lengths.append(episode_length)\n",
        "\n",
        "    return episode_rewards, episode_lengths"
      ],
      "metadata": {
        "id": "DjwYENNTGDcK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#average numb of steps\n",
        "def score(agent):\n",
        "    _, episode_lengths = evaluate_policy(agent,\n",
        "                                         agent.env,\n",
        "                                         max_length=3000,\n",
        "                                         n_eval_episodes=10)\n",
        "    return int(np.mean(episode_lengths))"
      ],
      "metadata": {
        "id": "V_2tDE_QX6Gv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"stopping-v0\")\n",
        "obs = env.observation_space.sample()\n",
        "obs = obs[0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# RBF features\n",
        "# the agent\n",
        "grids = [[-np.pi / 4., 0., np.pi / 4], [-1., 0., 1.]]\n",
        "centers = RadialAgent.get_centers_from_grids(grids)\n",
        "sigma = 1.\n",
        "agent = RadialAgent(env, centers, sigma)\n",
        "\n",
        "# build the trainer\n",
        "gamma = 0.95\n",
        "memory_size = 1000\n",
        "memory_type = 'episode'\n",
        "eval_type = 'batch'\n",
        "baseline = LSPolicyIteration(env, agent, gamma, memory_size, memory_type, eval_type)\n",
        "\n",
        "# build the memory\n",
        "baseline.init_memory()\n",
        "print('memory size = {}'.format(len(baseline.memory)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "TaI27YavQk_K",
        "outputId": "6cfc1940-c7b3-4ac5-a70d-d7cc20eef419"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-bb7f2ab37cc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mcenters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRadialAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_centers_from_grids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRadialAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# build the trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-032b997d473a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, centers, sigma, preprocess_obs)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRadialAgent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-14690daffb1d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, preprocess_obs)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_obs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_features_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-14690daffb1d>\u001b[0m in \u001b[0;36mget_features_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.float32' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run the algorithm\n",
        "n_iter = 10\n",
        "steps = score(agent)\n",
        "print('iteration = {:02d} - average number of balancing steps : {:04d}'.format(\n",
        "    0, steps))\n",
        "for it in range(1, n_iter + 1):\n",
        "    baseline.train_step()\n",
        "    steps = score(agent)\n",
        "    print('iteration = {:02d} - average number of balancing steps : {:04d}'.\n",
        "          format(it, steps))"
      ],
      "metadata": {
        "id": "Y16Ix36ZeSve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "range_episodic_memory = np.linspace(50, 1000, 20).astype(int)\n",
        "n_trials = 10\n",
        "results = []\n",
        "for memory_size in range_episodic_memory:\n",
        "    results.append([])\n",
        "\n",
        "    # build the trainer\n",
        "    baseline = LSPolicyIteration(env, agent, gamma, memory_size,\n",
        "                                                memory_type, eval_type)\n",
        "    for _ in range(n_trials):\n",
        "\n",
        "        # build the memory\n",
        "        baseline.init_memory()\n",
        "        # initialize the agent\n",
        "        agent.init_weights()\n",
        "\n",
        "        # run the algorithm\n",
        "        for it in range(1, n_iter + 1):\n",
        "            baseline.train_step()\n",
        "        steps = score(agent)\n",
        "        results[-1].append(steps)\n",
        "\n",
        "    print('n_episodes = {:04d} - average number of balancing steps : {:04d}'.\n",
        "          format(memory_size, int(np.mean(results[-1]))))"
      ],
      "metadata": {
        "id": "Kc59ze-2eQ27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.style.use('ggplot')\n",
        "plt.title('American Option: Average balancing steps')\n",
        "plt.xlabel('Number of training episodes')\n",
        "plt.ylabel('Steps')\n",
        "plt.xlim(0, 1000)\n",
        "plt.ylim(0, 3010)\n",
        "\n",
        "x = range_episodic_memory\n",
        "\n",
        "y_mean = np.mean(results, 1)\n",
        "plt.plot(x, y_mean, color='blue')\n",
        "\n",
        "y_max = np.max(results, 1)\n",
        "plt.plot(x, y_max, '--', color='pink')\n",
        "\n",
        "y_min = np.min(results, 1)\n",
        "plt.plot(x, y_min, '--', color='pink')\n",
        "\n",
        "y_std = np.std(results, 1)\n",
        "plt.fill_between(x, y_mean - y_std, y_mean + y_std, color='blue', alpha=.1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CGL_xVdNeodL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "to check\n",
        "'''\n",
        "env = gym.make(\"stopping-v0\")\n",
        "obs = env.observation_space.sample()[0]\n",
        "print(obs)\n",
        "obs_shape = env.observation_space.shape\n",
        "print(obs_shape)\n",
        "\n",
        "\n",
        "\n",
        "preprocess_obs = lambda x: x\n",
        "features = preprocess_obs(obs)\n",
        "print(obs, obs.itemsize)\n",
        "print(env.reset()[0])\n",
        "\n",
        "\n",
        "baseline = LSPolicyIteration(env, agent, gamma, memory_size, memory_type, eval_type)\n",
        "\n",
        "baseline.init_memory()\n",
        "print(baseline.memory)\n",
        "print('memory size = {}'.format(len(baseline.memory)))"
      ],
      "metadata": {
        "id": "U_jsIcdytrxg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}