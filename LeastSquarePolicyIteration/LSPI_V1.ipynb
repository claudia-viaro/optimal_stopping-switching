{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSPI_V1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOrMpk5e1QKCYtxjJmLa1qo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudia-viaro/optimal_switching/blob/main/LSPI_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Least Square Policy Iteration\n",
        "\n",
        "<a href=\"https://www.jmlr.org/papers/volume4/lagoudakis03a/lagoudakis03a.pdf\">Lagoudakis (2003)</a>\n"
      ],
      "metadata": {
        "id": "j3uhbTtR0jdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "this is not using gym\n",
        "uses same idea behind the Jentzen's paper\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "gCcHQatEFmyN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e9934536-a8cc-4219-be85-3f6db3e1c5c7"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nthis is not using gym\\nuses same idea behind the Jentzen's paper\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math, time\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "from itertools import combinations\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "np.random.seed(234198)\n",
        "import itertools\n",
        "import random\n",
        "\n",
        "import scipy.stats\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as tdata"
      ],
      "metadata": {
        "id": "TKdnEXT_FV_p"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A policy $\\pi$ is a behavior function that maps states to actions:\n",
        "\\begin{equation}\n",
        "a = \\pi(s)\n",
        "\\end{equation}\n",
        "\n",
        "If we consider a full episode (one of our complete trajectories) that starts at time t=0 and terminates at time t=T, then we define the return R₀ from the starting state as:\n",
        "\n",
        "\\begin{equation}\n",
        "R_0 = \\sum_{t=0}^T \\gamma^t r_t\n",
        "\\end{equation}\n",
        "Where $r_t$ is the reward at time $=t$, and $\\gamma$ is a discount factor in the range $[0,1]$, which allows us to adjust the time horizon, and ensure the return over long episodes remains finite. \n",
        "\n",
        "As the transition from state $s \\rightarrow s^{\\prime}$ is generally probabilistic (starting multiple episodes from the same state will generally result in different returns), we take the expectation value of return over all allowed trajectories. By doing so, we can define the expected return, called the state-value function $V_{\\pi}$ which describes the expected return starting from state $s$ and then following policy $\\pi$:\n",
        "\n",
        "\\begin{equation}\n",
        "V_{\\pi} =  \\mathbb{E}[R_t | s_t = s_t] \n",
        "\\end{equation}\n",
        "where $R_t$ is the return of a single, specific full episode starting at time $t$. The expectation operator $\\mathbb{E}[\\cdot]$ averages over all possible individual episodes/trajectories starting from initial state $s$ and following policy $\\pi$. \n",
        "\n",
        "A closely related quantity is the action-value function $Q_{\\pi}$ which describes the expected return starting in state $s$, taking action $a$, and thereafter following policy $\\pi$:\n",
        "\n",
        "\\begin{equation}\n",
        "Q_{\\pi}(s,a) =  \\mathbb{E}_{a_t \\sim \\pi; s_t \\sim \\mathcal{P}}[R_0 | s_0 = s, a_0 = a] \n",
        "\\end{equation}\n",
        "\n",
        "In most scenarios, the underlying MDP model is not fully available. Typically, the state space, the action space, and the discount factor are available, whereas the transition model and the reward function are not known in advance. It is still desirable to be able to evaluate, or, even better, find good decision policies. Here we rely on information that comes from interaction between the decision maker and the process itself, hence tuples known as samples: \n",
        "\\begin{equation}\n",
        "(s, a, r, s^{\\prime})\n",
        "\\end{equation}\n",
        "Samples are collected from actual (sequential) episodes of interaction with the processand the algorithm learns decision policies from such samples.\n",
        "\n",
        "### Approximation\n",
        "In our algorithm, we approximate the action-value function $Q_{\\pi}(s, a)$ by a linear architecture and its actual representation consists of a compact\n",
        "description of the basis functions and a set of parameters. \n",
        "\n"
      ],
      "metadata": {
        "id": "aP-kQCbqr2C-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Action-value function approximation\n",
        "\n",
        "Let $\\hat{Q}_{\\pi}(s,a; w)$ be an approximation to $Q_{\\pi}(s,a)$ represented by a parametric approximation architecture with free parameters $w$. \n",
        "\n",
        "The main idea of value-function approximation is that the parameters $w$ can be adjusted appropriately so that the approximate values are “close enough” to the original values, and, therefore, $\\hat{Q}_{\\pi}$ can be used in place of the exact value function $Q_{\\pi}$.\n",
        "\n",
        "LSPI is only one particular example of the family of reinforcement-learning algorithms for control based on approximate policy iteration. It distinguishes itself on the basis of choices related to:\n",
        "- approximation architecture (linear)\n",
        "- policy evaluation and projection procedure (LSTD$Q$). These two steps are merged together because the target function $Q_{\\pi}$ is not known in\n",
        "advance, and must be inferred from the observed system dynamics. It is not possible to use a training set of examples of the form $\\{(s,a), Q_{\\pi}(s,a)  \\}$ that provide the value $ Q_{\\pi}(s,a)$ of the target function at certain sample points $(s,a)$.\n"
      ],
      "metadata": {
        "id": "m-Vovdow78rc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Appriximation architecture\n",
        "$Q_{\\pi}$ values are approximated by a linear parametric combination of\n",
        "$k$ basis functions (features):\n",
        "\\begin{equation}\n",
        "\\hat{Q}_{\\pi}(s,a; w) = \\sum_{j=1}^k \\phi_j (s,a) w_j\n",
        "\\end{equation}\n",
        "\n",
        "where the $w_j$’s are the parameters. The basis functions $\\phi_j (s,a)$ are fixed, but arbitrary and, in general, non-linear, functions of $s$ and $a$. We require that the basis functions $\\phi_j$ are linearly independent to ensure that there are no redundant parameters and that the matrices involved in the computations are full rank. Typical linear approximation architectures are polynomials of any degree (each basis function is a polynomial term) and\n",
        "radial basis functions (each basis function is a Gaussian with fixed mean and variance).\n",
        "\n",
        "Define $\\phi(s,a)$ to be the column vector of size $k$ where each entry $j$ is the corresponding basis function $\\phi_j$ computed at $(s,a)$:\n",
        "\\begin{align}\n",
        "\\phi(s,a) =\n",
        " \\begin{pmatrix}\n",
        "  \\phi_1(s,a) \\\\\n",
        "  \\ldots \\\\\n",
        "  \\phi_k(s,a) \n",
        " \\end{pmatrix}\n",
        "\\end{align}\n",
        "\n",
        "Now, $\\hat{Q}_{\\pi}$ can be expressed compactly as $\\hat{Q}^{\\pi} = \\mathbf{\\Phi}w^{\\pi}$, where $w^{\\pi}$ is a column vector of length $k$ with all parameters and $\\mathbf{\\Phi}$ is a $(|\\mathcal{S}||\\mathcal{A}| × k)$ matrix, where each row contains the value of all basis functions for a certain pair $(s,a)$ and each column the value of a certain basis function for all pairs $(s,a)$.\n",
        "\n",
        "### LSTD$Q$\n",
        "\n",
        "Consider the problem of learning the (weighted) least-squares fixed-point approximation $\\hat{Q}_{\\pi}$ to the state-action value function $Q_{\\pi}$ of a fixed policy $\\pi$ from samples. Assuming that there are $k$ linearly independent basis functions in the linear architecture, this problem is\n",
        "equivalent to learning the parameters $w^{\\pi}$ of $\\hat{Q}_{\\pi} = \\mathbf{\\Phi}w^{\\pi}$. The exact values for $w^{\\pi}$ can be computed from the model by solving the $(k × k)$ linear system:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{A} w^{\\pi}= b,\n",
        "\\end{equation}\n",
        "where\n",
        "\\begin{equation}\n",
        "\\mathbf{A} = \\mathbf{\\Phi}^\\intercal \\Delta_{\\mu} (\\mathbf{\\Phi} − \\gamma \\mathbf{P} \\mathbf{\\Pi}_{\\pi} \\mathbf{\\Phi})\\;\\;\\;\\; \\text{and} \\;\\;\\;\\; b =\\mathbf{\\Phi}^\\intercal \\Delta_{\\mu} \\mathcal{R} ,\n",
        "\\end{equation}\n",
        "and $\\mu$ is a probability distribution over $S × A)$ that defines the weights of the projection.\n",
        "\n",
        "For the learning problem, $\\mathbf{A}$ and $b$ cannot be determined a priori, either because the matrix $\\mathbf{P}$ and the vector $\\mathcal{R}$ are unknown, or because they are so large that they cannot be used in any practical computation. However, $\\mathbf{A}$ and $b$ can be learned using samples; the\n",
        "learned linear system can then be solved to yield the learned parameters we\n",
        "$\\pi$ which, in turn, determine the learned value function.\n",
        "\n",
        "So, given any finite set of $L$ samples:\n",
        "\\begin{equation}\n",
        "D = \\Big\\{(s_i, a_i, r_i, s^{\\prime}_i), i =1, \\ldots, L     \\Big\\}\n",
        "\\end{equation}\n",
        "\n",
        "then we can compute $\\mathbf{A}$ and b as:\n",
        "\n",
        "\\begin{equation}\n",
        "\\tilde{\\mathbf{A}} = \\frac{1}{L} \\sum_{i=1}^L \\Bigg[\\phi(s_i, a_i) \\Big(\\phi(s_i, a_i) - \\gamma \\phi(s_i^{\\prime}, \\pi(s_i^{\\prime} )   \\Big)^{\\intercal} \\Bigg]\\\\\n",
        "\\tilde{b} = \\frac{1}{L} \\sum_{i=1}^L  \\Bigg[\\phi(s_i, a_i) r_i \\Bigg]\n",
        "\\end{equation}\n",
        "assuming that the distribution $\\mu_{D}$ of the samples in $D$ over $(\\mathcal{S} \\times \\mathcal{A})$ matches the desired distribution. Clearly, the learned approximation is biased by the distribution $\\mu_{D}$ of samples. In general, the distribution $\\mu_{D}$ might be different from the desired distribution $\\mu$. This problem would be resolved trivially when a generative model is available, since samples can be drawn so that $\\mu_{D} = \\mu$.\n",
        "\n",
        "\n",
        "Given that a single sample contributes to $\\tilde{\\mathbf{A}}$ and $\\tilde{b}$ additively, it is easy to construct an incremental update rule for these. Let $\\tilde{\\mathbf{A}}^{(t)}$ and $\\tilde{b}^{(t)}$ be the current learned estimates of $\\mathbf{A}$ and $b$ for a fixed policy $\\pi$, assuming that initially $\\tilde{\\mathbf{A}}^{(0)} = 0$ and $\\tilde{b}^{(0)} = 0$. A new sample\n",
        "$(s_t, a_t, r_t,s^{\\prime}_t)$ contributes to the approximation according to the following update equations:\n",
        "\n",
        "\\begin{equation}\n",
        "\\tilde{\\mathbf{A}}^{t+1} = \\tilde{\\mathbf{A}}^{t} + \\phi(s_i, a_i) \\Big(\\phi(s_i, a_i) - \\gamma \\phi(s_i^{\\prime}, \\pi(s_i^{\\prime} )   \\Big)^{\\intercal}\\\\\n",
        "\\tilde{b}^{t+1} = \\tilde{b}^{\\prime} + \\phi(s_i, a_i) r_i \n",
        "\\end{equation}\n",
        "\n",
        "From these results we can then proceed to learn the weighted least-squares\n",
        "fixed-point approximation of the state-action value function of a fixed policy $\\pi$ from samples in a batch or in an incremental way."
      ],
      "metadata": {
        "id": "v_eElGAoI7Lp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Back to RL\n",
        "At this point we have an approximated state-value function $\\hat{Q}_{\\pi}$:\n",
        "\\begin{equation}\n",
        "\\hat{Q}_{\\pi}(s,a; w) = \\sum_{j=1}^k \\phi_j (s,a) w_j = \\phi(s,a)^{\\intercal}w\n",
        "\\end{equation}\n",
        "The greedy policy $\\pi$ over this approximate value function at any given state $s$ can be obtained through maximization of the approximate values over all actions in $\\mathcal{A}$.\n",
        "\n",
        "\\begin{equation}\n",
        "\\pi(s) =  \\arg \\max_{a \\in \\mathcal{A}} \\hat{Q}_{\\pi}(s,a) = \\arg \\max_{a \\in \\mathcal{A}} \\phi(s,a)^{\\intercal}w\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "nh1dH11wucaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BlackScholes:\n",
        "  def __init__(self, drift, sigma, delta, spot, assets,  paths, periods,\n",
        "         maturity, strike, dividend=0):\n",
        "\n",
        "    self.drift = drift - dividend\n",
        "    self.sigma = sigma\n",
        "    self.delta = delta\n",
        "    self.spot = spot\n",
        "    self.assets = assets\n",
        "    self.paths = paths\n",
        "    self.periods = periods\n",
        "    self.maturity = maturity\n",
        "    self.strike = strike\n",
        "    self.dt = self.maturity / self.periods\n",
        "    self.df = math.exp(-self.drift * self.dt)\n",
        "\n",
        "  def drift_fct(self, x, t):\n",
        "    del t\n",
        "    return self.drift * x\n",
        "\n",
        "  def diffusion_fct(self, x, t, v=0):\n",
        "    del t\n",
        "    return self.sigma * x\n",
        "\n",
        "\n",
        "\n",
        "  def generate_paths(self):\n",
        "    \"\"\"Returns a nparray (nb_paths * assets * nb_dates) with prices.\"\"\"\n",
        "    paths = self.paths\n",
        "    spot_paths = np.empty((paths, self.assets, self.periods+1))\n",
        "    spot_paths[:, :, 0] = self.spot\n",
        "    random_numbers = np.random.normal(\n",
        "        0, 1, (paths, self.assets, self.periods))\n",
        "    dW = random_numbers * np.sqrt(self.dt)\n",
        "    drift = self.drift\n",
        "    r = np.repeat(np.repeat(np.repeat(\n",
        "        np.reshape(drift, (-1, 1, 1)), paths, axis=0),\n",
        "        self.assets, axis=1), self.periods, axis=2)\n",
        "    sig = np.repeat(np.repeat(np.repeat(\n",
        "        np.reshape(self.sigma, (-1, 1, 1)), paths, axis=0),\n",
        "        self.assets, axis=1), self.periods, axis=2)\n",
        "    spot_paths[:, :,  1:] = np.repeat(\n",
        "        spot_paths[:, :, 0:1], self.periods, axis=2) * np.exp(np.cumsum(\n",
        "        (r-self.delta) * self.dt - (sig ** 2) * self.dt / 2 + sig * dW, axis=2))\n",
        "    # dimensions: [nb_paths, assets, periods+1]\n",
        "\n",
        "    return spot_paths"
      ],
      "metadata": {
        "id": "Ir_dfWMKFXqW"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate underlying stochastic process\n",
        "\n",
        "hyperparam_test_stock_models = {'drift': 0.2, 'sigma': 0.05, 'delta': 0.1,  'paths':5000, 'periods': 9, 'maturity': 3., 'strike' : 100,'assets':20,  'spot':110,}\n",
        "#S = GBM(**hyperparam_test_stock_models)\n",
        "#X=S.simulate_process()\n",
        "\n",
        "\n",
        "S=BlackScholes(**hyperparam_test_stock_models)\n",
        "X=S.generate_paths()\n",
        "print(X.shape) # (date, path, asset)   (original: paths, stocks, dates)\n",
        "\n",
        "print(X.shape)\n",
        "def draw_stock_model(stockmodel):\n",
        "    stock_paths = stockmodel\n",
        "\n",
        "    # draw a path\n",
        "    one_path = stock_paths[0, 0, :]\n",
        "    dates = np.array([i for i in range(len(one_path))])\n",
        "    plt.plot(dates, one_path, label='stock path')\n",
        "    plt.ylabel('Stock price')\n",
        "    plt.ylabel('Time')\n",
        "    plt.legend()\n",
        "    return plt.show()   \n",
        "\n",
        "draw_stock_model(X) \n",
        "\n",
        "print(X[0, 0, :])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "mvxqriblFZuF",
        "outputId": "340e62b2-e426-4684-d6bf-c2b631af5389"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 20, 10)\n",
            "(5000, 20, 10)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[110.         115.17256211 119.48101085 123.19125495 123.74954965\n",
            " 124.14983633 123.83941777 127.41153067 130.28531432 130.571288  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# basis functions\n",
        "\n",
        "class BasisFunctions:\n",
        "    def __init__(self, nb_stocks):\n",
        "        self.nb_stocks = nb_stocks\n",
        "        lst = list(range(self.nb_stocks))\n",
        "        self.combs =  [list(x) for x in combinations(lst, 2)] # all combinations of assets in pairs. e.g. [0,1], [0,2], ...\n",
        "        self.nb_base_fcts = 1 + 2 * self.nb_stocks + len(self.combs) # 66\n",
        "        # print(\"self.nb_base_fcts\", self.nb_base_fcts)\n",
        "        # print(\"nb_stocks\",  self.nb_stocks)\n",
        "\n",
        "    def base_fct(self, i, x):\n",
        "        bf=np.nan\n",
        "        if (i == 0):\n",
        "            bf = np.ones_like(x[0]) # (constant)\n",
        "        elif (i <= self.nb_stocks):\n",
        "            bf = x[i-1] # (x1, x2, ..., xn)\n",
        "        elif (self.nb_stocks < i <= 2 * self.nb_stocks):\n",
        "            k = i - self.nb_stocks - 1\n",
        "            bf = x[k] ** 2 # (x1^2, x2^2, ..., xn^2)\n",
        "        elif (i > 2 * self.nb_stocks):\n",
        "            k = i - 2*self.nb_stocks -1\n",
        "            bf = x[self.combs[k][0]] * x[self.combs[k][1]] # (x1x2, ..., xn-1xn)\n",
        "        return bf"
      ],
      "metadata": {
        "id": "azbqyzuaFbiq"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PAYOFF\n",
        "'''\n",
        "\n",
        "# Payoff\n",
        "class Payoff:\n",
        "  def __init__(self, model):\n",
        "    self.strike = model.strike\n",
        "\n",
        "  def MaxCall(self, X):\n",
        "    payoff = np.max(X, axis = 1) - self.strike\n",
        "    return payoff.clip(0, None)\n",
        "\n",
        "  def MaxPut(self, X):\n",
        "    payoff = self.strike - np.max(X, axis=1)\n",
        "    return payoff.clip(0, None)   \n",
        "\n",
        "\n",
        "  def GeometricPut(self, X):\n",
        "    dim = len(X[1])  \n",
        "    payoff = self.strike - np.prod(X, axis=1) ** (1/dim)\n",
        "    return payoff.clip(0, None)\n"
      ],
      "metadata": {
        "id": "bORGPc3MFdJW"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LeastSquarePI:\n",
        "  def __init__(self, model, payoff, epochs=20):\n",
        "    self.model = model\n",
        "    self.payoff = payoff(self.model)\n",
        "    self.epochs = epochs\n",
        "    #self.nb_base_fcts = 0\n",
        "    self.bf = BasisFunctions(self.model.assets*(1) + 2)\n",
        "    self.nb_base_fcts = self.bf.nb_base_fcts # (?)\n",
        "\n",
        "  def get_indicator_stop(self, payoff, continuation_value):\n",
        "    return payoff if payoff > continuation_value else 0\n",
        "\n",
        "\n",
        "  def get_contribution_u(self, payoff, evaluated_bases, next_evaluated_bases, discount_factor, continuation_value):\n",
        "    nb_base_fcts = len(evaluated_bases)\n",
        "    indicator_continue = (next_evaluated_bases if payoff < continuation_value\n",
        "                          else np.zeros(nb_base_fcts, dtype=float))\n",
        "    return np.outer(evaluated_bases, evaluated_bases - discount_factor * indicator_continue)\n",
        " \n",
        "  def evaluate_bases(self, stock_price, path, date, nb_dates):\n",
        "      time = date/nb_dates\n",
        "      stock_price_path_date = np.concatenate([stock_price[path, :, date], np.array([time, 1 - time])])\n",
        "      return np.array([self.bf.base_fct(i, stock_price_path_date)\n",
        "                        for i in range(self.bf.nb_base_fcts)])\n",
        "\n",
        "  def price(self):\n",
        "\n",
        "    stock_paths = self.model.generate_paths()\n",
        "    payoffs = self.payoff.MaxCall(stock_paths)\n",
        "    stock_paths_with_payoff = np.concatenate([stock_paths, np.expand_dims(payoffs, axis=1)], axis=1)\n",
        "    \n",
        "\n",
        "    matrixA = np.zeros((self.nb_base_fcts, self.nb_base_fcts), dtype=float)\n",
        "    vectorb = np.zeros(self.nb_base_fcts, dtype=float)\n",
        "    weights = np.zeros(self.nb_base_fcts, dtype=float)\n",
        "    deltaT = self.model.maturity / (self.model.periods + 1)\n",
        "    discount_factor = math.exp(-self.model.drift * deltaT)\n",
        "    paths = stock_paths\n",
        "    \n",
        "    for epoch in range(self.epochs):\n",
        "      for i_path, path in enumerate(range(self.model.paths)):\n",
        "        for date in range(self.model.periods):\n",
        "          payoff = self.payoff.MaxCall([stock_paths[path, :, date+1]])[0]\n",
        "          evaluated_bases = self.evaluate_bases(paths, path, date, (self.model.periods + 1))\n",
        "          next_evaluated_bases  = self.evaluate_bases(paths, path,  date+1, (self.model.periods + 1))\n",
        "          continuation_value = np.inner(weights, next_evaluated_bases) # kind of predict (in the other method we used np.dot. np.dot and np.outer are not doing the same matrix mult)\n",
        "          indicator_stop = self.get_indicator_stop(payoff, continuation_value)\n",
        "          contribution_u = self.get_contribution_u(payoff, evaluated_bases, next_evaluated_bases, discount_factor, continuation_value)\n",
        "          matrixA += contribution_u\n",
        "          vectorb += evaluated_bases * np.asarray(discount_factor) * np.asarray(indicator_stop)\n",
        "\n",
        "      weights = np.linalg.solve(matrixA, vectorb) # solve linear system and get coefficients (similar to OLS regression)\n",
        "\n",
        "    # now we use the weights we have produced before\n",
        "    prices = np.zeros(self.model.paths, dtype=float)\n",
        "    for path in range(self.model.paths):\n",
        "      for date in range((self.model.periods + 1)):\n",
        "        evaluated_bases = self.evaluate_bases(paths, path, date, (self.model.periods + 1))\n",
        "        payoff = self.payoff.MaxCall([stock_paths[path, :, date]])[0]\n",
        "        continuation_value = np.inner(weights, evaluated_bases) # kind of predict (in the other method we used np.dot. np.dot and np.outer are not doing the same matrix mult)\n",
        "        continuation_value = max(continuation_value, 0)\n",
        "        if payoff > continuation_value or (date == self.model.periods):\n",
        "          prices[path] = payoff * (discount_factor ** date)\n",
        "          break\n",
        "    return np.round(np.mean(prices),3) # take mean across all paths"
      ],
      "metadata": {
        "id": "rkjV9dpnEO7S"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Stock = BlackScholes(**hyperparam_test_stock_models)\n",
        "t_begin = time.time()\n",
        "pricing_LeastSquare = LeastSquarePI(model = Stock, payoff= Payoff, epochs = 20 )"
      ],
      "metadata": {
        "id": "pfNc-Ie5EShb"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_LeastSquare = pricing_LeastSquare.price()\n",
        "duration = time.time() - t_begin\n",
        "print(result_LeastSquare)\n",
        "print(round(duration, 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpJYXZh_EUDa",
        "outputId": "708148f3-a512-4aad-9add-9ca8d70d93dc"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42.954\n",
            "1179.293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict ={}\n",
        " \n",
        "# Insert data into dictionary\n",
        "dict1 = {\n",
        "     1: [\"2\", 90, 16.015, 118.778],\n",
        "     2: [\"2\", 100, 24.27, 112.346],\n",
        "     3: [\"2\", 110, 32.525, 104.045],\n",
        "     4: [\"3\", 90, 17.821, 116.851],\n",
        "     5: [\"3\", 100, 26.278, 117.143],\n",
        "     6: [\"3\", 110, 34.734, 118.504],\n",
        "     7: [\"4\", 90, 19.075, 128.681],\n",
        "     8: [\"4\", 100, 27.655, 130.233],\n",
        "     9: [\"4\", 110, 36.254, 159.904],\n",
        "     10: [\"5\", 90, 19.97, 174.879],\n",
        "     11: [\"5\", 100, 28.664, 179.928],\n",
        "     12: [\"5\", 110, 37.361, 170.0],\n",
        "     13: [\"10\", 90, 22.438, 314.409],\n",
        "     14: [\"10\", 100, 12.037, 304.515],\n",
        "     15: [\"10\", 110, 40.33, 312.012],\n",
        "     16: [\"20\", 90, 24.15, 1141.407],\n",
        "     17: [\"20\", 100, 33.671, 1146.562],\n",
        "     18: [\"20\", 110, 42.954, 1179.293],\n",
        "     }\n",
        " \n",
        "# Print the names of the columns.\n",
        "print (\"{:<10} {:<10} {:<10} {:<10}\".format('assets', 'spot', 'L', 'timeL'))\n",
        " \n",
        "# print each data item.\n",
        "for key, value in dict1.items():\n",
        "    assets, spot, L, timeL = value\n",
        "    print (\"{:<10} {:<10} {:<10} {:<10}\".format(assets, spot, L, timeL))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLNGlLShOkBS",
        "outputId": "397972d5-6727-4fa5-aa5a-b327cb037fff"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assets     spot       L          timeL     \n",
            "2          90         16.015     118.778   \n",
            "2          100        24.27      112.346   \n",
            "2          110        32.525     104.045   \n",
            "3          90         17.821     116.851   \n",
            "3          100        26.278     117.143   \n",
            "3          110        34.734     118.504   \n",
            "4          90         19.075     128.681   \n",
            "4          100        27.655     130.233   \n",
            "4          110        36.254     159.904   \n",
            "5          90         19.97      174.879   \n",
            "5          100        28.664     179.928   \n",
            "5          110        37.361     170.0     \n",
            "10         90         22.438     314.409   \n",
            "10         100        12.037     304.515   \n",
            "10         110        40.33      312.012   \n",
            "20         90         24.15      1141.407  \n",
            "20         100        33.671     1146.562  \n",
            "20         110        42.954     1179.293  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "to check\n",
        "\n",
        "'''\n",
        "\n",
        "'''\n",
        "model = BlackScholes(**hyperparam_test_stock_models)\n",
        "stock_paths = model.generate_paths()\n",
        "payoff = Payoff(model)\n",
        "bf = BasisFunctions(model.assets*(1) + 2)\n",
        "nb_base_fcts = bf.nb_base_fcts\n",
        "\n",
        "payoffs = payoff.MaxCall(stock_paths)\n",
        "stock_paths_with_payoff = np.concatenate([stock_paths, np.expand_dims(payoffs, axis=1)], axis=1)\n",
        "matrixA = np.zeros((nb_base_fcts, nb_base_fcts), dtype=float)\n",
        "vectorb = np.zeros(nb_base_fcts, dtype=float)\n",
        "weights = np.zeros(nb_base_fcts, dtype=float)\n",
        "deltaT = model.maturity / (model.periods + 1)\n",
        "discount_factor = math.exp(-model.drift * deltaT)\n",
        "paths = stock_paths\n",
        "'''"
      ],
      "metadata": {
        "id": "J9-pzUlTFP3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def get_indicator_stop(payoff, continuation_value):\n",
        "    return payoff if payoff > continuation_value else 0\n",
        "\n",
        "\n",
        "def get_contribution_u(spayoff, evaluated_bases, next_evaluated_bases, discount_factor, continuation_value):\n",
        "  nb_base_fcts = len(evaluated_bases)\n",
        "  indicator_continue = (next_evaluated_bases if payoff < continuation_value\n",
        "                        else np.zeros(nb_base_fcts, dtype=float))\n",
        "  return np.outer(evaluated_bases, evaluated_bases - discount_factor * indicator_continue)\n",
        "\n",
        "def evaluate_bases(stock_price, path, date, nb_dates):\n",
        "    time = date/nb_dates\n",
        "    stock_price_path_date = np.concatenate([stock_price[path, :, date], np.array([time, 1 - time])])\n",
        "    return np.array([bf.base_fct(i, stock_price_path_date)\n",
        "                      for i in range(bf.nb_base_fcts)])\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "78_UJNPwFRIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "for i_path, path in enumerate(range(model.paths)):\n",
        "  for date in range(model.periods):\n",
        "    payoff = Payoff(model).MaxCall([stock_paths[path, :, date+1]])[0]\n",
        "    evaluated_bases = evaluate_bases(paths, path, date, (model.periods + 1))\n",
        "    next_evaluated_bases  = evaluate_bases(paths, path,  date+1, (model.periods + 1))\n",
        "    continuation_value = np.inner(weights, next_evaluated_bases) # kind of predict (in the other method we used np.dot. np.dot and np.outer are not doing the same matrix mult)\n",
        "    indicator_stop = get_indicator_stop(payoff, continuation_value)\n",
        "    contribution_u = get_contribution_u(payoff, evaluated_bases, next_evaluated_bases, discount_factor, continuation_value)\n",
        "    matrixA += contribution_u\n",
        "    vectorb += evaluated_bases * np.asarray(discount_factor) * np.asarray(indicator_stop)\n",
        "'''"
      ],
      "metadata": {
        "id": "6l5cvtccFSkD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
