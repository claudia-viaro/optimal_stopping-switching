{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "opt_switching_V3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPxymc3kEH70PEqoTrUU7S5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudia-viaro/optimal_stopping-switching/blob/main/opt_switching_V3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we fix the final regime of the process"
      ],
      "metadata": {
        "id": "1sjUHwhxLQUC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsjSbJxO9nQV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "np.random.seed(234198)\n",
        "import itertools\n",
        "import random\n",
        "import time\n",
        "import scipy.stats\n",
        "\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as tdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BlackScholes:\n",
        "  def __init__(self, drift, sigma, delta, spot, assets,  paths, periods,\n",
        "         maturity, strike, dividend=0):\n",
        "\n",
        "    self.drift = drift - dividend\n",
        "    self.sigma = sigma\n",
        "    self.delta = delta\n",
        "    self.spot = spot\n",
        "    self.assets = assets\n",
        "    self.paths = paths\n",
        "    self.periods = periods\n",
        "    self.maturity = maturity\n",
        "    self.strike = strike\n",
        "    self.dt = self.maturity / self.periods\n",
        "    self.df = math.exp(-self.drift * self.dt)\n",
        "\n",
        "  def drift_fct(self, x, t):\n",
        "    del t\n",
        "    return self.drift * x\n",
        "\n",
        "  def diffusion_fct(self, x, t, v=0):\n",
        "    del t\n",
        "    return self.sigma * x\n",
        "\n",
        "\n",
        "\n",
        "  def simulate_process(self):\n",
        "    \"\"\"Returns a nparray (nb_paths * assets * nb_dates) with prices.\"\"\"\n",
        "    paths = self.paths\n",
        "    spot_paths = np.empty((self.periods+1, paths, self.assets ))\n",
        "\n",
        "    spot_paths[0, :, :] = self.spot\n",
        "    random_numbers = np.random.normal(\n",
        "        0, 1, (self.periods, paths, self.assets ))\n",
        "    dW = random_numbers * np.sqrt(self.dt)\n",
        "    drift = self.drift\n",
        "    r = np.repeat(np.repeat(np.repeat(\n",
        "        np.reshape(drift, (-1, 1, 1)), self.periods, axis=0),\n",
        "        paths, axis=1), self.assets, axis=2)\n",
        "    sig = np.ones((self.periods, paths, self.assets))*self.sigma\n",
        "    #sig = np.repeat(np.repeat(np.repeat(\n",
        "    #    np.reshape(self.sigma, (-1, 1, 1)), self.periods+1, axis=2),\n",
        "    #    paths, axis=1), self.assets, axis=0)\n",
        "    \n",
        "    spot_paths[1:, :,  :] = np.repeat(\n",
        "        spot_paths[0:1, :, :], self.periods, axis=0)* np.exp(np.cumsum((r-self.delta) * self.dt - (sig ** 2) * self.dt / 2 + sig * dW, axis=0))\n",
        "\n",
        "    return spot_paths #.reshape(spot_paths.shape[2], spot_paths.shape[0], spot_paths.shape[1])\n",
        "\n",
        "\n",
        "'''\n",
        "PLOT\n",
        "'''\n",
        "\n",
        "def draw_stock_model(stockmodel):\n",
        "    stock_paths = stockmodel\n",
        "\n",
        "    # draw a path\n",
        "    one_path = stock_paths[:, 0, 0]\n",
        "    dates = np.array([i for i in range(len(one_path))])\n",
        "    plt.plot(dates, one_path, label='stock path')\n",
        "    plt.ylabel('Stock price')\n",
        "    plt.ylabel('Time')\n",
        "    plt.legend()\n",
        "    return plt.show()\n",
        "\n",
        "\n",
        "\n",
        "class Ftheta_NN(nn.Module):\n",
        "  def __init__(self, assets):\n",
        "    super(Ftheta_NN, self).__init__()\n",
        "    H = assets + 40\n",
        "    self.bn0 = nn.BatchNorm1d(num_features=assets)\n",
        "    self.layer1 = nn.Linear(assets, H)\n",
        "    self.leakyReLU = nn.LeakyReLU(0.5)\n",
        "    self.Softplus = nn.Softplus()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.tanh = nn.Tanh()\n",
        "    self.relu = nn.ReLU()\n",
        "    self.bn1 = nn.BatchNorm1d(num_features=H)\n",
        "    self.layer2 = nn.Linear(H, H)\n",
        "    self.bn2 = nn.BatchNorm1d(num_features=H)\n",
        "    self.layer3 = nn.Linear(H, 1)\n",
        "    self.bn3 = nn.BatchNorm1d(num_features=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.bn0(x)\n",
        "    x = self.layer1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.bn2(x)\n",
        "    x = self.layer3(x)\n",
        "    x = self.sigmoid(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "class Ftheta_NN(nn.Module):\n",
        "  def __init__(self, assets, hidden_size):\n",
        "    super(Ftheta_NN, self).__init__()\n",
        "    self.l1 = nn.Linear(assets, hidden_size) \n",
        "    self.relu = nn.ReLU()\n",
        "    self.l2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.l3 = nn.Linear(hidden_size, 1)  \n",
        "    self.sigmoid=nn.Sigmoid()\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = self.l1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.l2(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.l3(out)\n",
        "    out = self.sigmoid(out)\n",
        "    return out\n",
        "'''\n",
        "# set initial weights of a linear layer of the NN with uniform values and bias=0.01 (or choose zero initial weights)\n",
        "def init_weights(m):\n",
        "  if isinstance(m, torch.nn.Linear):\n",
        "    torch.manual_seed(42)\n",
        "    # torch.nn.init.zeros_(m.weight)\n",
        "    torch.nn.init.xavier_uniform_(m.weight)\n",
        "    m.bias.data.fill_(0.01)\n",
        "\n",
        "\n",
        "\n",
        "class OptimizationPart(object):\n",
        "\n",
        "  def __init__(self, assets, paths, epochs=50, batch_size=2000):\n",
        "    self.assets = assets\n",
        "    self.paths = paths\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = batch_size\n",
        "    self.network = Ftheta_NN(self.assets).double()\n",
        "    self.network.apply(init_weights)\n",
        "\n",
        "\n",
        "  def train_network(self,  stock_values, current_payoff,\n",
        "                    future_payoff):\n",
        "    optimizer = optim.Adam(self.network.parameters())\n",
        "    #future_payoff = torch.from_numpy(future_payoff).double()\n",
        "    #current_payoff = torch.from_numpy(current_payoff).double()\n",
        "    X_inputs = torch.from_numpy(stock_values).double()\n",
        "\n",
        "    self.network.train(True)\n",
        "    ones = torch.ones(self.paths)\n",
        "    for epoch in range(self.epochs):\n",
        "      optimizer.zero_grad()\n",
        "      outputs = self.network(X_inputs).reshape(-1) # probabilities\n",
        "      reward = (current_payoff * outputs ) +future_payoff * (ones - outputs) # reward function\n",
        "      loss = -torch.mean(reward) # loss function\n",
        "      loss.backward() # gradient calculation of the loss function\n",
        "      optimizer.step() # gradient descent update\n",
        "\n",
        "  def evaluate_network(self, X_inputs):\n",
        "    self.network.train(False)\n",
        "    X_inputs = torch.from_numpy(X_inputs).double()\n",
        "    outputs = self.network(X_inputs)\n",
        "    return outputs.view(X_inputs.size()[0]).detach().numpy(), self.network\n",
        "\n"
      ],
      "metadata": {
        "id": "kRvtM2Fl9qTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Profit_training:\n",
        "  def __init__(self, model):\n",
        "    self.strike = model.strike\n",
        "    self.model = model\n",
        "    \n",
        "\n",
        "  def terminal(self, X):\n",
        "    payoff = np.max(X, axis=1) - self.strike\n",
        "    return payoff.clip(0, None)\n",
        "\n",
        "  def g(self, date,path,X):\n",
        "    X=torch.from_numpy(X).float()\n",
        "    max1=torch.max(X[int(date) , path , : ].float()-self.strike)\n",
        "    return np.exp(-self.model.drift*self.model.dt*date)*torch.max(max1,torch.tensor([0.0])) \n",
        " \n",
        "\n",
        "  def running(self, Y, X):\n",
        "    gamma = np.array([-self.terminal(X), self.terminal(X) + 0.7]) # there are two rows, the first for \\gamma_{0,1}, the second for \\gamma_{1,0}\n",
        "    r_benefit = self.terminal(X)\n",
        "    return torch.from_numpy(r_benefit+Y-gamma)  \n",
        " \n",
        "class Profit_testing:\n",
        "  def __init__(self, model):\n",
        "    self.strike = model.strike\n",
        "    self.model = model\n",
        "\n",
        "  def terminal(self, X):\n",
        "    terminal = np.max(X, axis=1) - self.strike\n",
        "    return terminal.clip(0, None)\n",
        "\n",
        "  def g(self, date,path,X):\n",
        "    X=torch.from_numpy(X).float()\n",
        "    max1=torch.max(X[int(date) , path , : ].float()-self.strike)\n",
        "    return np.exp(-self.model.drift*self.model.dt*date)*torch.max(max1,torch.tensor([0.0])) \n",
        "\n",
        "\n",
        "  # switch is F_theta_train \n",
        "  def running(self, Y, date, path, S, X, switch, gamma):\n",
        "    val=Y[date+1, path]- gamma  \n",
        "    k = np.array([0.4, 0.7])\n",
        "    r_benefit = self.g(date, path, X)\n",
        "    return val*int(switch[date, path])+r_benefit.numpy()"
      ],
      "metadata": {
        "id": "4vdhJjVlyJbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4DMLlCe1vqCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently, the parameters $F^{\\theta}$ are found by maximizing the reward function that takes as inputs:\n",
        "- current payoff: a payoff that considers both possibilities of switching or not at that timestep (as we dont know theta yet)\n",
        "- future payoff (same thing)\n",
        "\n",
        "\n",
        "possibly not a good idea, instead:\n",
        "- future payoff could consider both cases only for N-1\n",
        "- current payoff probably should stay as it is"
      ],
      "metadata": {
        "id": "BN1DV9Hcvqha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Training:\n",
        "  def __init__(self, model, training, testing, nb_epochs=50):\n",
        "\n",
        "    self.model = model # argument is S    \n",
        "    self.neural_stopping = OptimizationPart(model.assets, model.paths) \n",
        "    self.profit_training = Profit_training(self.model)\n",
        "    self.profit_testing = Profit_testing(self.model)\n",
        "\n",
        "  def price(self):\n",
        "    model = self.model\n",
        "    stock_paths = self.model.simulate_process()    \n",
        "    disc_factor = np.math.exp((-model.drift) * model.maturity/(model.periods))\n",
        "    \n",
        "    # create empty objects to store values\n",
        "    k = np.array([0.4, 0.7])\n",
        "    regimes = [0, 1]\n",
        "    regime_path=np.zeros((model.periods+1, model.paths)) # record at which regime we're at at each n\n",
        "    Y_train=np.zeros((model.periods+1, model.paths))\n",
        "    F_theta_train=np.zeros((model.periods+1,model.paths)) # record switching events for each n\n",
        "    mods=[None]*model.periods # record the models of the NN for testing\n",
        "\n",
        "    # at maturity N\n",
        "    final_payoff = np.array([self.profit_training.terminal(stock_paths[-1, :, :]), self.profit_training.terminal(stock_paths[-1, :, :])])   # payoff of the last date for each path.\n",
        "    future_payoff = torch.from_numpy(final_payoff*disc_factor).double() \n",
        "    Y_train[model.periods, :]= final_payoff[0]\n",
        "    F_theta_train[model.periods,:]=1 # at maturity we switch (does it matter?)\n",
        "    regime_path[model.periods, :] = random.sample(regimes, 1)[0] # sample a regime at maturity\n",
        "    values = Y_train[model.periods, :]\n",
        "    # recursive calc. before maturity\n",
        "    for date in range(stock_paths.shape[0] - 2, 0, -1): \n",
        "      current_payoff = self.profit_training.running(Y_train[date+1, :], stock_paths[date, :, :])\n",
        "\n",
        "      stopping_probability, networks = self.stop(stock_paths[date, : , :], \n",
        "                                current_payoff,\n",
        "                                future_payoff)\n",
        "      \n",
        "      \n",
        "      F_theta_train[date,:]=(stopping_probability > 0.5)*1.0   # transform stopping probabilities in 0-1 decision\n",
        "      which = stopping_probability > 0.5\n",
        "\n",
        "      for m in range(0,model.paths-1):\n",
        "        old_regime = regime_path[date +1, m]\n",
        "        regime_path[date, m] = int(which[m])\n",
        "        if which[m] == True:\n",
        "          if int(old_regime) - int(which[m])>0:  #gamma 0-1\n",
        "            gamma = -self.profit_testing.g(date, m, stock_paths)+0.7\n",
        "          else: gamma = -self.profit_testing.g(date, m, stock_paths) #gamma 1-0  \n",
        "        else:\n",
        "          gamma = 0 \n",
        "        Y_train[date, m] = Y_train[date+1, m]- gamma\n",
        "\n",
        "\n",
        "      immediate_exercise_value = Y_train[date, :]       \n",
        "      values[which] = immediate_exercise_value[which] # when we switch we take the current profit\n",
        "      values[~which] *= disc_factor           # when we don't switch we take final profit discounted \n",
        "      #Y_train[date, :] = values\n",
        "      mods[date]=networks    \n",
        "    \n",
        "    return mods\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "  def stop(self, stock_values, current_payoff,\n",
        "           future_payoff):\n",
        "    \n",
        "    self.neural_stopping.train_network(\n",
        "      stock_values,\n",
        "      current_payoff ,\n",
        "      future_payoff)\n",
        "\n",
        "    inputs = stock_values\n",
        "    stopping_probability , networks   = self.neural_stopping.evaluate_network(inputs)\n",
        "    return stopping_probability , networks  \n"
      ],
      "metadata": {
        "id": "7UvqI1aXKWAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simulate paths Y\n",
        "# goal of this phase is to be able to get stopping decisions f_theta_n\n",
        "hyperparam_training = {'drift': 0.2, 'sigma': 0.05, 'delta': 0.1,  'paths':1024, 'periods': 9, 'maturity': 3., 'strike' : 100,'assets':2,  'spot':90,}\n",
        "S_train=BlackScholes(**hyperparam_training)\n",
        "\n",
        "\n",
        "pricing = Training(S_train, Profit_training, Profit_testing, nb_epochs=3000)\n",
        "'''\n",
        "arguments are:\n",
        "- path process\n",
        "- Profit training and profit testing classes\n",
        "- number of epochs to be used for the gradient descent algorithm\n",
        "\n",
        "'''\n",
        "\n",
        "mods = pricing.price()\n"
      ],
      "metadata": {
        "id": "sp-KE2XpKsKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lower bound\n",
        "\n",
        "the stopping time $\\tau^{\\Theta}$ gives a lower bound $L=\\mathbb{E}g(\\tau^{\\Theta}, X_{\\tau^{\\Theta}})$ for the optimal value $V_0= \\sup_{\\tau \\in \\mathcal{T}}\\mathbb{E}g(\\tau, X_{\\tau})$.\n",
        "\n",
        "Simulate \n",
        "- $K_L = 1024$ paths $(y_n^k)_{n=0}^N$, $k=1, \\ldots, K_L$, of $(X_n)_{n=0}^N$ and assume these are drawn independently from the realizations $(x_n^k)_{n=0}^N$, $k=1, \\ldots, K$.\n",
        "\n",
        "The unbiased estimate of the lower bound $L$ is given by\n",
        "\\begin{equation}\n",
        "\\hat{L}=\\frac{1}{K_L} \\sum_{k=1}^{K_L} g(l^k, y_{l^k}^k)\n",
        "\\end{equation}\n",
        "where $l^k = l(y_0^k, \\ldots, y_{N-1}^k)$"
      ],
      "metadata": {
        "id": "k8JSTFLsNMD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing phase - Lower bound\n",
        "\n",
        "# sample y from the process (Y)\n",
        "hyperparam_testing_L = {'drift': 0.2, 'sigma': 0.05, 'delta': 0.1,  'paths':4, 'periods': 9, 'maturity': 3., 'strike' : 100,'assets':2,  'spot':90,}\n",
        "S_test_L=BlackScholes(**hyperparam_testing_L)\n",
        "\n",
        "# now we can compute all the stopping times recursively"
      ],
      "metadata": {
        "id": "eBdQnHLJNOMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "\n",
        "class Testing_Lower:\n",
        "  def __init__(self, model, training, testing, mods):   \n",
        "    self.model = model # argument is S   \n",
        "    self.neural_stopping = OptimizationPart(model.assets, model.paths) \n",
        "    self.profit_training = Profit_training(self.model)\n",
        "    self.profit_testing = Profit_testing(self.model)\n",
        "    self.mods = mods\n",
        "\n",
        "  def price(self):\n",
        "    model = self.model\n",
        "    disc_factor = np.math.exp((-model.drift) * model.maturity/(model.periods))\n",
        "    stock_paths = self.model.simulate_process()\n",
        "    k = np.array([0.4, 0.7])\n",
        "    regimes = [0, 1]\n",
        "    regime_path=np.zeros((model.periods+1, model.paths)) # record at which regime we're at at each n\n",
        "    Y_train=np.zeros((model.periods+1, model.paths))\n",
        "    F_theta_train=np.zeros((model.periods+1,model.paths)) # record switching events for each n\n",
        " \n",
        "    # at maturity N\n",
        "    final_payoff = np.array([self.profit_training.terminal(stock_paths[-1, :, :]), self.profit_training.terminal(stock_paths[-1, :, :])])   # payoff of the last date for each path.\n",
        "    future_payoff = torch.from_numpy(final_payoff*disc_factor).double() \n",
        "    Y_train[model.periods, :]= final_payoff[0]\n",
        "    F_theta_train[model.periods,:]=1 # at maturity we switch (does it matter?)\n",
        "    regime_path[model.periods, :] = random.sample(regimes, 1)[0] # sample a regime at maturity\n",
        "    values = Y_train[model.periods, :]\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    # recursive calc. before maturity\n",
        "         \n",
        "    for date in range(stock_paths.shape[0] - 2, 0, -1):\n",
        "      current_payoff = self.profit_training.running(Y_train[date+1, :], stock_paths[date, :, :])\n",
        "      mod_curr=self.mods[date]\n",
        "      probs=mod_curr(torch.from_numpy(stock_paths[date])) \n",
        "      np_probs=probs.detach().numpy().reshape(self.model.paths)\n",
        "      \n",
        "      F_theta_train[date,:]=(np_probs > 0.5)*1.0   # transform stopping probabilities in 0-1 decision\n",
        "      which = np_probs > 0.5\n",
        "\n",
        "      for m in range(0,model.paths-1):\n",
        "        old_regime = regime_path[date +1, m]\n",
        "        regime_path[date, m] = int(which[m])\n",
        "        if which[m] == True:\n",
        "          if int(old_regime) - int(which[m])>0:  #gamma 0-1\n",
        "            gamma = -self.profit_testing.g(date, m, stock_paths)+0.7\n",
        "          else: gamma = -self.profit_testing.g(date, m, stock_paths) #gamma 1-0  \n",
        "        else:\n",
        "          gamma = 0 \n",
        "        Y_train[date, m] = Y_train[date+1, m]- gamma\n",
        "\n",
        "\n",
        "      immediate_exercise_value = Y_train[date, :]       \n",
        "      values[which] = immediate_exercise_value[which] # when we switch we take the current profit\n",
        "      values[~which] *= disc_factor           # when we don't switch we take final profit discounted \n",
        "\n",
        "      #Y_train[date, :] = values\n",
        "      print(\"date\", date, np.mean(values))\n",
        "\n",
        "    \n",
        "    return round(np.mean(values)* disc_factor, 3), Y_train\n",
        "\n"
      ],
      "metadata": {
        "id": "PpoI_K_MNO6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "price_testing = Testing_Lower(S_test_L, Profit_training, Profit_testing, mods)\n",
        "\n",
        "Y_test_mean, Y_train = price_testing.price()\n",
        "print(Y_test_mean)"
      ],
      "metadata": {
        "id": "u3CX-89ERC3J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc2940c7-ee75-49c2-c65f-f781905a5ba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date 8 30.638559341430664\n",
            "date 7 38.668416023254395\n",
            "date 6 45.94314098358154\n",
            "date 5 52.68890953063965\n",
            "date 4 57.284738540649414\n",
            "date 3 58.2815055847168\n",
            "date 2 58.2815055847168\n",
            "date 1 58.2815055847168\n",
            "54.523\n"
          ]
        }
      ]
    }
  ]
}