{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "opt_switching_V4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMFHL/axARs5cdt6sTeFAKF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudia-viaro/optimal_stopping-switching/blob/main/opt_switching_V4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we consider both possible final regimes of the process"
      ],
      "metadata": {
        "id": "xAtXzBgLLgwK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3tK6JCs9Fidl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "np.random.seed(234198)\n",
        "import itertools\n",
        "import random\n",
        "import time\n",
        "import scipy.stats\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as tdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BlackScholes:\n",
        "  def __init__(self, drift, sigma, delta, spot, assets,  paths, periods,\n",
        "         maturity, strike, dividend=0):\n",
        "\n",
        "    self.drift = drift - dividend\n",
        "    self.sigma = sigma\n",
        "    self.delta = delta\n",
        "    self.spot = spot\n",
        "    self.assets = assets\n",
        "    self.paths = paths\n",
        "    self.periods = periods\n",
        "    self.maturity = maturity\n",
        "    self.strike = strike\n",
        "    self.dt = self.maturity / self.periods\n",
        "    self.df = math.exp(-self.drift * self.dt)\n",
        "\n",
        "  def drift_fct(self, x, t):\n",
        "    del t\n",
        "    return self.drift * x\n",
        "\n",
        "  def diffusion_fct(self, x, t, v=0):\n",
        "    del t\n",
        "    return self.sigma * x\n",
        "\n",
        "\n",
        "\n",
        "  def simulate_process(self):\n",
        "    \"\"\"Returns a nparray (nb_paths * assets * nb_dates) with prices.\"\"\"\n",
        "    paths = self.paths\n",
        "    spot_paths = np.empty((self.periods+1, paths, self.assets ))\n",
        "\n",
        "    spot_paths[0, :, :] = self.spot\n",
        "    random_numbers = np.random.normal(\n",
        "        0, 1, (self.periods, paths, self.assets ))\n",
        "    dW = random_numbers * np.sqrt(self.dt)\n",
        "    drift = self.drift\n",
        "    r = np.repeat(np.repeat(np.repeat(\n",
        "        np.reshape(drift, (-1, 1, 1)), self.periods, axis=0),\n",
        "        paths, axis=1), self.assets, axis=2)\n",
        "    sig = np.ones((self.periods, paths, self.assets))*self.sigma\n",
        "    #sig = np.repeat(np.repeat(np.repeat(\n",
        "    #    np.reshape(self.sigma, (-1, 1, 1)), self.periods+1, axis=2),\n",
        "    #    paths, axis=1), self.assets, axis=0)\n",
        "    \n",
        "    spot_paths[1:, :,  :] = np.repeat(\n",
        "        spot_paths[0:1, :, :], self.periods, axis=0)* np.exp(np.cumsum((r-self.delta) * self.dt - (sig ** 2) * self.dt / 2 + sig * dW, axis=0))\n",
        "\n",
        "    return spot_paths #.reshape(spot_paths.shape[2], spot_paths.shape[0], spot_paths.shape[1])\n",
        "\n",
        "\n",
        "'''\n",
        "PLOT\n",
        "'''\n",
        "\n",
        "def draw_stock_model(stockmodel):\n",
        "    stock_paths = stockmodel\n",
        "\n",
        "    # draw a path\n",
        "    one_path = stock_paths[:, 0, 0]\n",
        "    dates = np.array([i for i in range(len(one_path))])\n",
        "    plt.plot(dates, one_path, label='stock path')\n",
        "    plt.ylabel('Stock price')\n",
        "    plt.ylabel('Time')\n",
        "    plt.legend()\n",
        "    return plt.show()\n",
        "\n",
        "\n",
        "\n",
        "class Ftheta_NN(nn.Module):\n",
        "  def __init__(self, assets):\n",
        "    super(Ftheta_NN, self).__init__()\n",
        "    H = assets + 40\n",
        "    self.bn0 = nn.BatchNorm1d(num_features=assets)\n",
        "    self.layer1 = nn.Linear(assets, H)\n",
        "    self.leakyReLU = nn.LeakyReLU(0.5)\n",
        "    self.Softplus = nn.Softplus()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.tanh = nn.Tanh()\n",
        "    self.relu = nn.ReLU()\n",
        "    self.bn1 = nn.BatchNorm1d(num_features=H)\n",
        "    self.layer2 = nn.Linear(H, H)\n",
        "    self.bn2 = nn.BatchNorm1d(num_features=H)\n",
        "    self.layer3 = nn.Linear(H, 1)\n",
        "    self.bn3 = nn.BatchNorm1d(num_features=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.bn0(x)\n",
        "    x = self.layer1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.bn2(x)\n",
        "    x = self.layer3(x)\n",
        "    x = self.sigmoid(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "class Ftheta_NN(nn.Module):\n",
        "  def __init__(self, assets, hidden_size):\n",
        "    super(Ftheta_NN, self).__init__()\n",
        "    self.l1 = nn.Linear(assets, hidden_size) \n",
        "    self.relu = nn.ReLU()\n",
        "    self.l2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.l3 = nn.Linear(hidden_size, 1)  \n",
        "    self.sigmoid=nn.Sigmoid()\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = self.l1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.l2(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.l3(out)\n",
        "    out = self.sigmoid(out)\n",
        "    return out\n",
        "'''\n",
        "# set initial weights of a linear layer of the NN with uniform values and bias=0.01 (or choose zero initial weights)\n",
        "def init_weights(m):\n",
        "  if isinstance(m, torch.nn.Linear):\n",
        "    torch.manual_seed(42)\n",
        "    # torch.nn.init.zeros_(m.weight)\n",
        "    torch.nn.init.xavier_uniform_(m.weight)\n",
        "    m.bias.data.fill_(0.01)\n"
      ],
      "metadata": {
        "id": "DqtK6h69Fkp4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Profit:\n",
        "  def __init__(self, model):\n",
        "    self.strike = model.strike\n",
        "    self.model = model\n",
        "    \n",
        "  def terminal_(self, X):\n",
        "    terminal = np.max(X, axis=1) - self.strike\n",
        "    return terminal.clip(0, None)\n",
        "\n",
        "  def g(self, date,path,X):\n",
        "    X=torch.from_numpy(X).float()\n",
        "    max1=torch.max(X[int(date) , path , : ].float()-self.strike)\n",
        "    return np.exp(-self.model.drift*self.model.dt*date)*torch.max(max1,torch.tensor([0.0])) \n",
        "\n",
        "\n",
        "  def terminal(self, X):\n",
        "    payoff = np.max(X) - self.strike\n",
        "    return payoff.clip(0, None)\n",
        "\n",
        "  def running(self, Y, X, switch_to):\n",
        "    \n",
        "    r_benefit = self.terminal(X)\n",
        "    for i in range(0, self.model.paths):\n",
        "      if switch_to[i] == 0:\n",
        "        gamma = -self.terminal(X[i]) # there are two rows, the first for \\gamma_{0,1}, the second for \\gamma_{1,0}\n",
        "      else:\n",
        "        gamma: self.terminal(X[i]) + 0.7  \n",
        "\n",
        "    return torch.from_numpy(r_benefit+Y-gamma)  \n",
        "\n",
        "  def current_payoff(self, X, Y, regime_path):\n",
        "  # X is stock_paths[date, :, :]\n",
        "  # Y is Y_train_i[date+1, :] or Y_train_j\n",
        "  # regime_path is regime_path_i[date+1, :]\n",
        "  \n",
        "    current_payoff = np.zeros((self.model.paths))\n",
        "    \n",
        "    for i in range(0, self.model.paths):\n",
        "      if int(regime_path[i]) == 0:\n",
        "        gamma = -self.terminal(X[i, :])\n",
        "      else:\n",
        "        gamma= self.terminal(X[i, :]) + 0.7  \n",
        "      current_payoff[i]=self.terminal(X[i, :])-gamma+Y[i]    \n",
        "\n",
        "    return current_payoff   \n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "wtfGw1amGAwh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Optimization(object):\n",
        "\n",
        "  def __init__(self, assets, paths, epochs=50, batch_size=2000):\n",
        "    self.assets = assets\n",
        "    self.paths = paths\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = batch_size\n",
        "    self.network = Ftheta_NN(self.assets).double()\n",
        "    self.network.apply(init_weights)\n",
        "\n",
        "\n",
        "  def train_network(self,  stock_values, current_payoff,\n",
        "                    future_payoff):\n",
        "    optimizer = optim.Adam(self.network.parameters())\n",
        "    future_payoff = torch.from_numpy(future_payoff).double()\n",
        "    current_payoff = torch.from_numpy(current_payoff).double()\n",
        "    X_inputs = torch.from_numpy(stock_values).double()\n",
        "\n",
        "    self.network.train(True)\n",
        "    ones = torch.ones(self.paths)\n",
        "    for epoch in range(self.epochs):\n",
        "      optimizer.zero_grad()\n",
        "      outputs = self.network(X_inputs).reshape(-1) # probabilities\n",
        "      reward = (current_payoff * outputs ) +future_payoff * (ones - outputs) # reward function\n",
        "      loss = -torch.mean(reward) # loss function\n",
        "      loss.backward() # gradient calculation of the loss function\n",
        "      optimizer.step() # gradient descent update\n",
        "\n",
        "  def evaluate_network(self, X_inputs):\n",
        "    self.network.train(False)\n",
        "    X_inputs = torch.from_numpy(X_inputs).double()\n",
        "    outputs = self.network(X_inputs)\n",
        "    return outputs.view(X_inputs.size()[0]).detach().numpy(), self.network"
      ],
      "metadata": {
        "id": "25OHkXSYGCle"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i=0, j=1\n",
        "\n",
        "class Training_large:\n",
        "  def __init__(self, model, payoff, nb_epochs=50):\n",
        "\n",
        "    self.model = model # argument is S    \n",
        "    self.neural_stopping = Optimization(self.model.assets, self.model.paths) \n",
        "    self.payoff_function = payoff(self.model)\n",
        "\n",
        "  def price(self):\n",
        "    model = self.model\n",
        "    stock_paths = self.model.simulate_process()    \n",
        "    disc_factor = np.math.exp((-model.drift) * model.maturity/(model.periods))\n",
        "    \n",
        "    # create empty objects to store values\n",
        "    models = [None]*self.model.periods, [None]*self.model.periods\n",
        "\n",
        "    regimes = [0, 1]\n",
        "    regime_path_i=np.zeros((self.model.periods+1, self.model.paths)) # record at which regime we're at at each n\n",
        "    regime_path_j=np.zeros((self.model.periods+1, self.model.paths))\n",
        "    regimes_path=np.array([regime_path_i, regime_path_j]) #you can call each regime by regimes_path[0]\n",
        "\n",
        "    Y_train_i=np.zeros((self.model.periods+1, self.model.paths))\n",
        "    Y_train_j=np.zeros((self.model.periods+1, self.model.paths))\n",
        "    Y_train = np.array([Y_train_i, Y_train_j])\n",
        "\n",
        "    F_theta_i=np.zeros((self.model.periods+1,self.model.paths))\n",
        "    F_theta_j=np.zeros((self.model.periods+1,self.model.paths))\n",
        "    F_theta=np.array([F_theta_i, F_theta_j])\n",
        "    F_theta[1][self.model.periods, 0:self.model.paths] = 1\n",
        "\n",
        "    values = np.array([np.zeros(self.model.paths), np.zeros(self.model.paths)])\n",
        "\n",
        "    # at n=N\n",
        "    final_payoff = np.array([self.payoff_function.terminal(stock_paths[-1, :, :])])\n",
        "    future_payoff = final_payoff*disc_factor\n",
        "\n",
        "\n",
        "    for r in regimes:\n",
        "      # still at maturity\n",
        "      Y_train[r][self.model.periods, :]=final_payoff\n",
        "      regimes_path[r][self.model.periods, :] = regimes[r]\n",
        "      values[r] = Y_train[r][self.model.periods, :]\n",
        "      print(\"final regime\", r, \"date\", self.model.periods, \":\", 1.00,\" , \", 1.00, \" , \", self.model.paths, \"value\", round(np.mean(values[r]),3))\n",
        "\n",
        "      for date in range(stock_paths.shape[0] - 2, 0, -1):\n",
        "        current_payoff = np.array([np.zeros(self.model.paths), np.zeros(self.model.paths)])\n",
        "        current_payoff[r] = self.payoff_function.current_payoff(stock_paths[date, :, :], Y_train[r][date+1, :], regimes_path[r][date+1, :])\n",
        "        stopping_probability=np.array([np.zeros(self.model.paths), np.zeros(self.model.paths)])\n",
        "        Nnetworks=np.array([np.zeros(self.model.paths), np.zeros(self.model.paths)])\n",
        "        stopping_probability[r], models[r][date] = self.stop(stock_paths[date, : , :], \n",
        "                                    current_payoff[r],\n",
        "                                    future_payoff)\n",
        "        \n",
        "        F_theta[r][date,:]=(stopping_probability[r] > 0.5)*1.00   # transform stopping probabilities in 0-1 decision\n",
        "        #which_i = stopping_probability_i > 0.5\n",
        "\n",
        "        for m in range(0,self.model.paths-1):\n",
        "          old_regime = regimes_path[r][date +1, m]\n",
        "          regimes_path[r][date, m] = int(F_theta[r][date,m])   #int(which_i[m])   #current regime using probabilities just obtained\n",
        "\n",
        "          if int(old_regime) - regimes_path[r][date, m]>0:  #gamma 0-1\n",
        "            gamma = self.payoff_function.g(date, m, stock_paths)+0.7\n",
        "          elif int(old_regime) - regimes_path[r][date, m]<0: \n",
        "            gamma = - self.payoff_function.g(date, m, stock_paths) #gamma 1-0  \n",
        "          else:\n",
        "            gamma = 0 \n",
        "          Y_train[r][date, m] = Y_train[r][date+1, m]- gamma\n",
        "\n",
        "        immediate_exercise_value = Y_train[r][date, :]       \n",
        "        values[r][int(regimes_path[r][date, m])] = immediate_exercise_value[int(regimes_path[r][date, m])] # when we switch we take the current profit\n",
        "        values[r][~int(regimes_path[r][date, m])] *= disc_factor           # when we don't switch we take final profit discounted \n",
        "        print(\"final regime\", r, \"date\", date, \":\", round(np.min(stopping_probability[r]), 3),\" , \", round(np.max(stopping_probability[r]), 3), \" , \", len([1 for l in stopping_probability[r] if l > 0.5]), \"value\", round(np.mean(values[r]),3))          \n",
        "\n",
        "\n",
        "    return models\n",
        "\n",
        "  def stop(self, stock_values, current_payoff,\n",
        "           future_payoff):\n",
        "     \n",
        "    self.neural_stopping.train_network(\n",
        "      stock_values,\n",
        "      current_payoff ,\n",
        "      future_payoff)\n",
        "\n",
        "    inputs = stock_values\n",
        "    stopping_probability , networks   = self.neural_stopping.evaluate_network(inputs)\n",
        "    return stopping_probability , networks  "
      ],
      "metadata": {
        "id": "s7id_0bEGGMm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparam_training = {'drift': 0.2, 'sigma': 0.05, 'delta': 0.1,  'paths':1024, 'periods': 9, 'maturity': 3., 'strike' : 100,'assets':2,  'spot':90,}\n",
        "S_training=BlackScholes(**hyperparam_training)\n",
        "\n",
        "\n",
        "Price_training = Training_large(S_training, Profit, nb_epochs=3000)\n",
        "\n",
        "'''\n",
        "arguments are:\n",
        "- path process\n",
        "- payoff class of functions\n",
        "- number of epochs to be used for the gradient descent algorithm\n",
        "'''\n",
        "\n",
        "Models = Price_training.price()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gU4v0ICIGKaE",
        "outputId": "5a1d4c45-00f1-458b-ebac-8584d80a6bd4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final regime 0 date 9 : 1.0  ,  1.0  ,  1024 value 60.301\n",
            "final regime 0 date 8 : 0.0  ,  0.96  ,  759 value 60.297\n",
            "final regime 0 date 7 : 0.0  ,  0.986  ,  871 value 60.299\n",
            "final regime 0 date 6 : 0.0  ,  0.988  ,  943 value 60.295\n",
            "final regime 0 date 5 : 0.0  ,  0.994  ,  991 value 60.291\n",
            "final regime 0 date 4 : 0.0  ,  0.998  ,  999 value 60.288\n",
            "final regime 0 date 3 : 0.0  ,  0.999  ,  1016 value 60.285\n",
            "final regime 0 date 2 : 0.997  ,  1.0  ,  1024 value 60.282\n",
            "final regime 0 date 1 : 0.999  ,  1.0  ,  1024 value 60.279\n",
            "final regime 1 date 9 : 1.0  ,  1.0  ,  1024 value 60.301\n",
            "final regime 1 date 8 : 1.0  ,  1.0  ,  1024 value 60.297\n",
            "final regime 1 date 7 : 1.0  ,  1.0  ,  1024 value 60.293\n",
            "final regime 1 date 6 : 1.0  ,  1.0  ,  1024 value 60.29\n",
            "final regime 1 date 5 : 0.0  ,  1.0  ,  1023 value 60.287\n",
            "final regime 1 date 4 : 1.0  ,  1.0  ,  1024 value 60.284\n",
            "final regime 1 date 3 : 1.0  ,  1.0  ,  1024 value 60.281\n",
            "final regime 1 date 2 : 1.0  ,  1.0  ,  1024 value 60.279\n",
            "final regime 1 date 1 : 1.0  ,  1.0  ,  1024 value 60.276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing bound\n",
        "\n",
        "class Testing_large:\n",
        "  def __init__(self, model, payoff, models):   \n",
        "    self.model = model # argument is S   \n",
        "    self.neural_stopping = Optimization(model.assets, model.paths) \n",
        "    self.payoff_function = payoff(self.model)\n",
        "    self.models = models\n",
        "\n",
        "  def price(self):\n",
        "    model = self.model\n",
        "    disc_factor = np.math.exp((-model.drift) * model.maturity/(model.periods))\n",
        "    stock_paths = self.model.simulate_process()\n",
        "\n",
        "\n",
        "    # create empty objects to store values\n",
        "\n",
        "    regimes = [0, 1]\n",
        "    regime_path_i=np.zeros((self.model.periods+1, self.model.paths)) # record at which regime we're at at each n\n",
        "    regime_path_j=np.zeros((self.model.periods+1, self.model.paths))\n",
        "    regimes_path=np.array([regime_path_i, regime_path_j]) #you can call each regime by regimes_path[0]\n",
        "\n",
        "    Y_train_i=np.zeros((self.model.periods+1, self.model.paths))\n",
        "    Y_train_j=np.zeros((self.model.periods+1, self.model.paths))\n",
        "    Y_train = np.array([Y_train_i, Y_train_j])\n",
        "\n",
        "    F_theta_i=np.zeros((self.model.periods+1,self.model.paths))\n",
        "    F_theta_j=np.zeros((self.model.periods+1,self.model.paths))\n",
        "    F_theta=np.array([F_theta_i, F_theta_j])\n",
        "    F_theta[1][self.model.periods, 0:self.model.paths] = 1\n",
        "\n",
        "    values = np.array([np.zeros(self.model.paths), np.zeros(self.model.paths)])\n",
        "\n",
        "    # at n=N\n",
        "    final_payoff = np.array([self.payoff_function.terminal(stock_paths[-1, :, :])])\n",
        "    future_payoff = final_payoff*disc_factor\n",
        "\n",
        "    for r in regimes:\n",
        "      # still at maturity\n",
        "      Y_train[r][self.model.periods, :]=final_payoff\n",
        "      regimes_path[r][self.model.periods, :] = regimes[r]\n",
        "      values[r] = Y_train[r][self.model.periods, :]\n",
        "      print(\"final regime\", r, \"date\",  self.model.periods, \":\", round(np.mean(values[r]),3))\n",
        "\n",
        "      for date in range(stock_paths.shape[0] - 2, 0, -1):\n",
        "        current_payoff = np.array([np.zeros(self.model.paths), np.zeros(self.model.paths)])\n",
        "        current_payoff[r] = self.payoff_function.current_payoff(stock_paths[date, :, :], Y_train[r][date+1, :], regimes_path[r][date+1, :])\n",
        "        current_model=self.models[r][date]\n",
        "        probs=current_model(torch.from_numpy(stock_paths[date])) \n",
        "        np_probs=probs.detach().numpy().reshape(self.model.paths)\n",
        "      \n",
        "        F_theta[r][date,:]=(np_probs > 0.5)*1.0   # transform stopping probabilities in 0-1 decision\n",
        "        #which = np_probs > 0.5        \n",
        "        \n",
        "\n",
        "        for m in range(0,self.model.paths-1):\n",
        "          old_regime = regimes_path[r][date +1, m]\n",
        "          regimes_path[r][date, m] = int(F_theta[r][date,m])   #int(which_i[m])   #current regime using probabilities just obtained\n",
        "\n",
        "          if int(old_regime) - regimes_path[r][date, m]>0:  #gamma 0-1\n",
        "            gamma = self.payoff_function.g(date, m, stock_paths)+0.7\n",
        "          elif int(old_regime) - regimes_path[r][date, m]<0: \n",
        "            gamma = - self.payoff_function.g(date, m, stock_paths) #gamma 1-0  \n",
        "          else:\n",
        "            gamma = 0 \n",
        "          Y_train[r][date, m] = Y_train[r][date+1, m]- gamma\n",
        "\n",
        "        immediate_exercise_value = Y_train[r][date, :]       \n",
        "        values[r][int(regimes_path[r][date, m])] = immediate_exercise_value[int(regimes_path[r][date, m])] # when we switch we take the current profit\n",
        "        values[r][~int(regimes_path[r][date, m])] *= disc_factor           # when we don't switch we take final profit discounted \n",
        "        print(\"final regime\", r, \"date\", date, \":\", round(np.mean(values[r]),3))          \n",
        "\n",
        "\n",
        "    return round(np.mean(values)* disc_factor, 3), Y_train"
      ],
      "metadata": {
        "id": "H1adTO26GREl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparam_testing = {'drift': 0.2, 'sigma': 0.05, 'delta': 0.1,  'paths':50000, 'periods': 9, 'maturity': 3., 'strike' : 100,'assets':2,  'spot':90,}\n",
        "S_testing=BlackScholes(**hyperparam_testing)"
      ],
      "metadata": {
        "id": "THcI7TzFJM3G"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "price_testing = Testing_large(S_testing, Profit, Models)\n",
        "\n",
        "Y_test_mean, Y_train = price_testing.price()\n",
        "print(Y_test_mean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ssl7gcThJouF",
        "outputId": "b0a81153-a256-47b5-93ba-d78ec3d3e2b7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final regime 0 date 9 : 76.327\n",
            "final regime 0 date 8 : 76.327\n",
            "final regime 0 date 7 : 76.327\n",
            "final regime 0 date 6 : 76.327\n",
            "final regime 0 date 5 : 76.327\n",
            "final regime 0 date 4 : 76.327\n",
            "final regime 0 date 3 : 76.327\n",
            "final regime 0 date 2 : 76.327\n",
            "final regime 0 date 1 : 76.327\n",
            "final regime 1 date 9 : 76.327\n",
            "final regime 1 date 8 : 76.327\n",
            "final regime 1 date 7 : 76.327\n",
            "final regime 1 date 6 : 76.327\n",
            "final regime 1 date 5 : 76.327\n",
            "final regime 1 date 4 : 76.327\n",
            "final regime 1 date 3 : 76.327\n",
            "final regime 1 date 2 : 76.327\n",
            "final regime 1 date 1 : 76.327\n",
            "71.404\n"
          ]
        }
      ]
    }
  ]
}