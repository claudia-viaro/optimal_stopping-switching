{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudia-viaro/optimal_stopping-switching/blob/main/optimal_stopping/Copy_of_optimal_stopping_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aX5-o2Sa7nC"
      },
      "source": [
        "#Problem Formulation\n",
        "Let $(\\Omega, \\mathcal{F}, \\mathbb{P})$ be a fixed probability space on which an adapted stochastic process is defined $X=(X_t)_{0 \\leq t \\leq T}$ whose natural filtration is $(\\mathcal{F}_t^0 := \\sigma \\{ X_s, s \\leq t \\})_{0 \\leq t \\leq T}$. Let $\\mathbf{F}=(\\mathcal{F}_0)_{0 \\leq t \\leq t}$ be the complete filtration of $(\\mathcal{F}_t^0 := \\sigma \\{ X_s, s \\leq t \\})_{0 \\leq t \\leq T}$. with $P$-null sets of $\\mathcal{F}$.\n",
        "\n",
        "The stochastic process $X$ is $\\mathbb{R}^d$-valued and represents the market price of $d$ financial assets (Bermudan call options) that influence the production of power. Assume $(X^i)_{i=1}^d$ follows a geometric Brownian motion satisfying the SDE:\n",
        "\\begin{equation}\n",
        "dX_t^i = (b-\\delta_i)dt + \\sigma_i dW_t^i\n",
        "\\end{equation}\n",
        "where $W$ is a standard Brownian otion on a filtered probability space $(\\Omega, \\mathcal{F}, (\\mathcal{F}_t)_{t \\geq 0}, \\mathbb{P})$ and $b$, $d_i$, $\\sigma_i >0$ are the drift. dividend yield and volatility of the system at time $t$.\n",
        "\n",
        "We will consider a discrete time approximization (Euler schema) on an equidistant time grid $0=t_0 < t_1 < \\ldots < t_N = T$, where $t_n = n \\cdot T/N$. For $i = 1, \\ldots, d$ we simulate $p$ paths\n",
        "\\begin{equation}\n",
        "x^p_{n,i} = x_{0,i} \\cdot \\exp \\Big\\{ \\sum_{k=0}^n \\big( (b-\\delta_i - \\sigma^2_i /2)\\Delta t + \\sigma_{i} \\sqrt{\\Delta t} \\cdot Z_{k, i}^p \\big)     \\Big\\}\n",
        "\\end{equation}\n",
        "where $\\Delta t = T/N$ and $Z_{k, i}^{p} \\sim \\mathcal{N} (0,1)$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yEZA-EFaBd0w"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "np.random.seed(234198)\n",
        "import itertools\n",
        "import random\n",
        "import time\n",
        "import scipy.stats\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as tdata\n",
        "from google.colab import files\n",
        "import helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xJbxC0rzBhrT"
      },
      "outputs": [],
      "source": [
        "class BlackScholes:\n",
        "  def __init__(self, drift, sigma, delta, spot, assets,  paths, periods,\n",
        "         maturity, strike, dividend=0):\n",
        "\n",
        "    self.drift = drift - dividend\n",
        "    self.sigma = sigma\n",
        "    self.delta = delta\n",
        "    self.spot = spot\n",
        "    self.assets = assets\n",
        "    self.paths = paths\n",
        "    self.periods = periods\n",
        "    self.maturity = maturity\n",
        "    self.strike = strike\n",
        "    self.dt = self.maturity / self.periods\n",
        "    self.df = math.exp(-self.drift * self.dt)\n",
        "\n",
        "  def drift_fct(self, x, t):\n",
        "    del t\n",
        "    return self.drift * x\n",
        "\n",
        "  def diffusion_fct(self, x, t, v=0):\n",
        "    del t\n",
        "    return self.sigma * x\n",
        "\n",
        "\n",
        "\n",
        "  def simulate_process(self):\n",
        "    \"\"\"Returns a nparray (nb_paths * assets * nb_dates) with prices.\"\"\"\n",
        "    paths = self.paths\n",
        "    spot_paths = np.empty((self.periods+1, paths, self.assets ))\n",
        "\n",
        "    spot_paths[0, :, :] = self.spot\n",
        "    random_numbers = np.random.normal(\n",
        "        0, 1, (self.periods, paths, self.assets ))\n",
        "    dW = random_numbers * np.sqrt(self.dt)\n",
        "    drift = self.drift\n",
        "    r = np.repeat(np.repeat(np.repeat(\n",
        "        np.reshape(drift, (-1, 1, 1)), self.periods, axis=0),\n",
        "        paths, axis=1), self.assets, axis=2)\n",
        "    sig = np.ones((self.periods, paths, self.assets))*self.sigma\n",
        "    #sig = np.repeat(np.repeat(np.repeat(\n",
        "    #    np.reshape(self.sigma, (-1, 1, 1)), self.periods+1, axis=2),\n",
        "    #    paths, axis=1), self.assets, axis=0)\n",
        "    \n",
        "    spot_paths[1:, :,  :] = np.repeat(\n",
        "        spot_paths[0:1, :, :], self.periods, axis=0)* np.exp(np.cumsum((r-self.delta) * self.dt - (sig ** 2) * self.dt / 2 + sig * dW, axis=0))\n",
        "\n",
        "    return spot_paths #.reshape(spot_paths.shape[2], spot_paths.shape[0], spot_paths.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "v1c9hUTGBzcz"
      },
      "outputs": [],
      "source": [
        "def draw_stock_model(stockmodel):\n",
        "    stock_paths = stockmodel\n",
        "\n",
        "    # draw a path\n",
        "    \n",
        "    one_path = stock_paths[:, 0, 0]\n",
        "    dates = np.array([i for i in range(len(one_path))])\n",
        "    plt.plot(dates, one_path, label='stock path')\n",
        "    plt.ylabel('Stock price')\n",
        "    plt.ylabel('Time')\n",
        "    plt.legend()\n",
        "    return plt.show()   "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class \"Payoff\"\n",
        "This class specifies the payoff (some alternatives) that is received when selling/buying the option. \n",
        "\n",
        "- MaxCall\n",
        "\\begin{equation}\n",
        "\\pi(n) = \\Big(\\max_{d \\in \\{1, \\ldots, D \\}} x_n^d - K   \\Big)^{+} \\tag{1}\n",
        "\\end{equation}\n",
        "where $K$ is the strike price"
      ],
      "metadata": {
        "id": "aGaMrPaHkF9H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FRYgMuM2C_K0"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "PAYOFF\n",
        "'''\n",
        "class Payoff_:\n",
        "  def __init__(self, model):\n",
        "    self.strike = model.strike\n",
        "    self.model = model\n",
        "\n",
        "  \n",
        "  def MaxCall_(self, n, X): # already get tensor\n",
        "    a = torch.tensor(())\n",
        "    for m in range(0, self.model.paths):\n",
        "      date = n[m]\n",
        "      tensorX= torch.from_numpy(X[int(date),m,:])\n",
        "      max1=torch.max(tensorX-100)  \n",
        "      i = torch.tensor([max1]).float()\n",
        "      a = torch.cat((a, i), 0)\n",
        "    return a"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class \"Ftheta_NN\"\n",
        "\n",
        "We employ a neural network to approximate the stopping decision functions $\\{f_n\\}_{n=0}^N$ by constructing a sequence of neural networks of the form $f^{\\theta_n}:\\mathbb{R}^d → \\{0,1\\}$ with parameters $\\theta_n \\in \\mathbb{R}^q$ to approximate $f_n$.\n",
        "\n",
        "The neural network used here takes the form $F^{\\theta}: \\mathbb{R}^d → (0,1)$ for $\\theta \\in \\{\\theta_0, \\ldots, \\theta_N  \\}$, that is the parameters are trained via a neural network that outputs probabilities in the interval $(0,1)$. This is due to the fact that the G-B optimization algorithm is to be applied to a continuous function with respect to $\\theta_n$, which $f^{\\theta_n}$ is not. Hence, the multi-layer, feed-forward neural network takes the form:\n",
        "\n",
        "\\begin{equation}\n",
        "F^{\\theta}= \\psi \\circ a_3^{\\theta} \\circ \\phi_{q_2} \\circ a_2^{\\theta} \\circ \\phi_{q_1} \\circ a_1^{\\theta}\n",
        "\\end{equation}\n",
        "where \n",
        "\n",
        "-  $q_1, q_2$ are the number of nodes in the hidden layers\n",
        "- $a_1^{\\theta} : \\mathbb{R}^d → \\mathbb{R}^{q_1}, a_2^{\\theta}: \\mathbb{R}^{q_1} → \\mathbb{R}^{q_2}$ are linear transformation functions: $a_i^{\\theta}(x)=W_i x + b_i$ with matrices $W_1 \\in \\mathbb{R}^{q_1 \\times d}, W_2 \\in \\mathbb{R}^{q_2 \\times q_1}, W_3 \\in \\mathbb{R}^{q_2 \\times 1}$ and vectors $b_1 \\in \\mathbb{R}^{q_1}, b_2 \\in \\mathbb{R}^{q_2}, b_3 \\in \\mathbb{R}^{1}$.\n",
        "- $\\phi_{q_i}: \\mathbb{R}^{q_i}$ is the ReLU activation function: $\\phi_{q_1}(x_i, \\ldots, x_{q_i})=(x_i^{+}, \\ldots, x_{q_i}^{+})$\n",
        "- $\\psi = \\mathbb{R} → \\mathbb{R}$ is the logistic sigmoid function: $\\psi(x)=1/(1+ e^{-x})$.\n",
        "Between the layers a batch normalization is also added, it takes the output from the previous layer and normalizes it before sending it to the next layer. This has the effect of stabilizing the neural network. \n",
        "\n",
        "The parameters will comprise $\\theta = \\{W_1, W_2,, W_3, b_1, b_2, b_3\\}\\in \\mathbb{R}^q$, where $q=q_1(d+q_2+1)+2q_2+1$. The value of $d$ stands for the dimension, that is the number of assets and will be varied among $d=\\{2,4, 5, 10, 20\\}$. \n"
      ],
      "metadata": {
        "id": "1eZ7Tt2TlARg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MpTiRUxLBj5L"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Neural network\n",
        "'''\n",
        "\n",
        "class Ftheta_NN(nn.Module):\n",
        "  def __init__(self, assets):\n",
        "    super(Ftheta_NN, self).__init__()\n",
        "    H = assets + 40\n",
        "    self.bn0 = nn.BatchNorm1d(num_features=assets)\n",
        "    self.a1 = nn.Linear(assets, H)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.bn1 = nn.BatchNorm1d(num_features=H)\n",
        "    self.a2 = nn.Linear(H, H)\n",
        "    self.bn2 = nn.BatchNorm1d(num_features=H)\n",
        "    self.a3 = nn.Linear(H, 1)\n",
        "    self.bn3 = nn.BatchNorm1d(num_features=1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, input):\n",
        "    out = self.bn0(input)\n",
        "    out = self.a1(out)\n",
        "    out = self.relu(out)\n",
        "    #out = self.bn1(out)\n",
        "    #out = self.a2(out)    \n",
        "    #out = self.relu(out)\n",
        "    out = self.bn2(out)\n",
        "    out = self.a3(out)    \n",
        "    out = self.sigmoid(out)\n",
        "    return out\n",
        "\n",
        "# set initial weights of a linear layer of the NN with uniform values and bias=0.01 (or choose zero initial weights)\n",
        "def init_weights(m):\n",
        "  if isinstance(m, torch.nn.Linear):\n",
        "    torch.manual_seed(42)\n",
        "    # torch.nn.init.zeros_(m.weight)\n",
        "    torch.nn.init.xavier_uniform_(m.weight)\n",
        "    m.bias.data.fill_(0.01) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class \"Training_network\"\n",
        "\n",
        "The NN is used to approximate the optimal stopping decisions $f_n: \\mathbb{R}^d \\rightarrow \\{0, 1 \\}$, $n = \\{ 1, \\ldots, N-1 \\}$, at each date by a neural network $f^{\\theta}: \\mathbb{R}^d \\rightarrow \\{0, 1 \\}$ with parameter $\\theta \\in \\mathbb{R}^q$. \n",
        "\n",
        "We choose $\\theta_N \\in \\mathbb{R}^q$ such that $f^{\\theta}_N \\equiv 1$ and determine $\\theta_n \\in \\mathbb{R}^q$ for $n \\leq N-1$ by recursion of the form:\n",
        "\n",
        "\\begin{equation}\n",
        "\\tau_{n+1} = \\sum_{m=n+1}^N m f^{\\theta_m}(X_m) \\prod _{j=n+1}^{m-1} (1-f^{\\theta_j}(X_j))\n",
        "\\end{equation}\n",
        "\n",
        "Since $f^{\\theta}$ takes values in $\\{ 0,1 \\}$, hence not appropriate for a gradient-descent optimization method, the neural network includes a layer performing a logistic transformation such that we have a continuous output function $F^{\\theta}: \\mathbb{R}^d \\rightarrow (0,1)$.\n",
        "\n",
        "At each time step, for each epoch we compute $F^{\\theta_n}$ using the $\\theta_n$ from the previous epoch. Then, the parameter $\\theta_n$ is update via backpropagation by the gradient of the loss function (Adam optimization algorithm \\citep{kingma2014adam}), which is specified as:\n",
        "\n",
        "\\begin{equation}\n",
        "    Loss = - \\mathbb{E}[g(n, X_n)F^{\\theta_n}(X_n) + g(\\tau_{n+1}, X_{\\tau_{n+1}})(1-F^{\\theta_n}(X_n))]\n",
        "\\end{equation}\n",
        "\n",
        "The aim is to determine $\\theta_n \\in \\mathbb{R}^q$ so that the negative of the loss function is close to the supremum $\\sup_{\\theta \\in \\mathbb{R}^q}\\mathbb{E}[g(n, X_n)F^{\\theta}(X_n) + g(\\tau_{n+1}, X_{\\tau_{n+1}})(1-F^{\\theta}(X_n))   ]$. \n",
        "\n",
        "Looking at the formula for the loss function, it takes as inputs:\n",
        "- _current payoff:_ payoff of the option computed at time $n$ for all paths if it is exercised at time $n$. this is the value of the option at time $n$ when it is exercised\n",
        "- _future payoff:_ expected value of the future payoff, computed at a stopping time observed in the future ($\\tau+1$), this is the value of the option at time $n$ when it is not exercised (continuation value)\n",
        "\n",
        "The NN takes as inputs:\n",
        "- _stock prices:_ the prices of the stock at time $n$ across multiple paths\n",
        "\n",
        "and as outputs:\n",
        "- values in $\\{ 0,1 \\}$, representing the probability of stopping the process (exercising the option). "
      ],
      "metadata": {
        "id": "DkGCjRMDJuP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Training_network(object):\n",
        "\n",
        "  def __init__(self, assets,  epochs=400, batch_size=2000):\n",
        "    self.assets = assets\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = batch_size\n",
        "    self.network = Ftheta_NN(self.assets).double()\n",
        "    self.network.apply(init_weights)\n",
        "\n",
        "  # training part\n",
        "  def train_network(self,  stock_values, current_payoff,\n",
        "                    future_payoff):\n",
        "        \n",
        "    # several optimization methods are available (here Adam algorithm). as argument input the parameters to be optimized    \n",
        "    optimizer = optim.Adam(self.network.parameters())\n",
        "    \n",
        "    # set values for the NN inputs (stock_values) \n",
        "    X_inputs = torch.from_numpy(stock_values).double() \n",
        "\n",
        "    # set values for the loss\n",
        "    # current_payoff, future_payoff are already tensors\n",
        "    ones = torch.ones(len(future_payoff)) # we need a vector of 1's in the loss function\n",
        "\n",
        "    self.network.train(True) # set training mode ON    \n",
        "    losses = []\n",
        "    \n",
        "    for epoch in range(self.epochs):\n",
        "      running_loss = 0.0\n",
        "      for batch in tdata.BatchSampler(\n",
        "              tdata.RandomSampler(range(len(X_inputs)), replacement=False),\n",
        "              batch_size=self.batch_size, drop_last=False):\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(True):\n",
        "          F_theta = self.network.forward((X_inputs[batch])).reshape(-1) \n",
        "          reward = (current_payoff[batch].reshape(-1)* F_theta + \n",
        "                    future_payoff[batch] * (ones[batch] - F_theta)) \n",
        "          \n",
        "          # compute loss function\n",
        "          loss = -torch.mean(reward)\n",
        "          \n",
        "          # compute gradients\n",
        "          loss.backward()\n",
        "          # take a step, updating the parameters \n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss.item() * self.batch_size\n",
        "      epoch_loss = running_loss /  len(tdata.BatchSampler(\n",
        "              tdata.RandomSampler(range(len(X_inputs)), replacement=False),\n",
        "              batch_size=self.batch_size, drop_last=False).sampler)\n",
        "      losses.append(epoch_loss)          \n",
        "    \n",
        "    # I would like to retain params computed at each each date so that I can access them later\n",
        "    torch.save(self.network.state_dict(), 'checkpoint.pth') \n",
        "\n",
        "    return F_theta, self.network, losses  \n",
        "  \n",
        "  # function to inform the NN to perform a testing phase\n",
        "  def evaluate_network(self, X_inputs):\n",
        "    state_dict = torch.load('checkpoint.pth')\n",
        "    self.network.load_state_dict(state_dict)\n",
        "\n",
        "    self.network.eval()\n",
        "    X_inputs = torch.from_numpy(X_inputs).double()\n",
        "    \n",
        "    # the output is a probability for each date and path\n",
        "    # it is obtained by feeding the NN with the dimension of the assets (at a specific date for all paths), \n",
        "    outputs = self.network(X_inputs)\n",
        "    return outputs.view(X_inputs.size()).detach().numpy()"
      ],
      "metadata": {
        "id": "ZbIYQqM3bKnb"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training\n",
        "\n",
        "We conduct $3000+d$ training steps and for each we generate a batch of $8192$ paths of $(X_n)_{n=0}^N$.\n",
        "\n",
        "The resulting output will be \n",
        "- the stopping decisions $f_n: \\mathbb{R}^d \\rightarrow \\{0, 1 \\}$, $n = \\{ 1, \\ldots, N-1 \\}$\n",
        "- ideally the parameters $\\theta_n$ \n"
      ],
      "metadata": {
        "id": "F67WgZCjmvmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# simulate paths \n",
        "hyperparam_training = {'drift': 0.2, 'sigma': 0.05, 'delta': 0.1,  'paths':4, 'periods': 9, 'maturity': 1., 'strike' : 100,'assets':2,  'spot':90,}\n",
        "S_train=BlackScholes(**hyperparam_training)"
      ],
      "metadata": {
        "id": "67CEvqrDYbYI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class \"Training\"\n",
        "\n",
        "This is a recursion. It starts by initializing:\n",
        "- model, that is the simulated stock prices\n",
        "- payoff class\n",
        "- NN **too early? should it go at every time step instead**\n",
        "\n",
        "Create some matrices to store values that will be accessed to at different time steps\n",
        "- _mods_ records the models at each time step, basically it appends, by date, the networks (not sure if it is enough to say that it stores the optimized parameters)\n",
        "- _loss functions_ just to plot\n",
        "- _tau dates:_ it is a matrix $(n \\times m)$, it records for each date when there is a stopping time (can only be at such date or higher)\n",
        "- _F theta:_ it is a matrix $(n \\times m)$, it records the outputs of the NN (0,1 values), only used to obtain the tau_dates\n",
        "\n",
        "The backward induction goes as:\n",
        "1. at maturity\n",
        "  - compute the terminal payoff\n",
        "  - no need to compute the stopping times/stopping decision functions because by construction $f_N \\equiv 1$\n",
        "  - hence set F_theta[N,:]=1 and tau_dates[N,:]=N\n",
        "\n",
        "2. before maturity\n",
        "- compute values for training the NN, hence current payoff $g(n, X_n)$ and future payoff $g(\\tau_{n+1}, X_{\\tau_{n+1}})$. The future payoff needs as time variable $\\tau_{n+1}$, we get it from the matrix tau_dates, taking value at $n+1$. The future payoff, as it refers to a value at time $\\tau_{n+1}$, needs to be discounted to $n$ **not done well here, just one timestep**\n",
        "- feed in the values in the function neural_stopping.train_network()\n",
        "- using the outputs returned by this function, we record values in matrices F_theta and tau_dates\n",
        "\n",
        "This is repeated until $n=1$"
      ],
      "metadata": {
        "id": "_Mizg-2Xr71G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Training:\n",
        "  def __init__(self, model, payoff_function):\n",
        "\n",
        "    self.model = model\n",
        "    self.payoff = payoff_function(self.model)\n",
        "    self.neural_stopping = Training_network(self.model.assets)\n",
        "\n",
        "  def value(self):\n",
        "    model = self.model\n",
        "    stock_paths = self.model.simulate_process()  \n",
        "    disc_factor = np.exp(-model.drift*model.maturity/model.periods)\n",
        "\n",
        "    # lists to store some values\n",
        "    mods=[None]*model.periods # record the models of the NN for testing\n",
        "    loss_functions = [None]*model.periods # record loss\n",
        "    tau_dates=np.zeros((model.periods+1,model.paths)) # record stopping times\n",
        "    tau_dates[model.periods,:]=model.periods    \n",
        "    F_theta_train=np.zeros((model.periods+1,model.paths))\n",
        "    F_theta_train[model.periods,:]=1\n",
        "    \n",
        "    # AT MATURITY N\n",
        "    #final_dates = [model.periods]*model.paths\n",
        "    final_dates =  tau_dates[model.periods,:]\n",
        "    terminal_payoff = self.payoff.MaxCall_(final_dates, stock_paths) # payoff of the last date\n",
        "    print(\"date\", model.periods, \",\", model.paths)\n",
        " \n",
        "\n",
        "    # from n=N-1 to 0 with steps of -1\n",
        "\n",
        "    for date in range(stock_paths.shape[0] - 2, 0, -1):      \n",
        "      tau_date_plus_one = tau_dates[date+1, :]\n",
        "      \n",
        "      continuation_value =  self.payoff.MaxCall_(tau_date_plus_one, stock_paths)\n",
        "      current_value =  self.payoff.MaxCall_([date]*model.paths, stock_paths)\n",
        "      \n",
        "      stopping_rule , networks, loss = self.neural_stopping.train_network(stock_paths[date, : , :], \n",
        "                                                  current_value,\n",
        "                                                  continuation_value*disc_factor)\n",
        "      mods[date]=networks\n",
        "      loss_functions[date]=loss\n",
        "      F_theta_train[date,:]=(stopping_rule > 0.5)*1.0 \n",
        "      tau_dates[date,:]=np.argmax(F_theta_train, axis=0)\n",
        "\n",
        "      print(\"date\", date, \",\", len([1 for l in stopping_rule if l > 0.5]), \" mean reward \", (np.mean(loss))*(-1))\n",
        "      \n",
        "\n",
        "    return mods, loss_functions\n",
        "\n",
        "  def stop(self, stock_values, current_payoff, future_payoff, train=True):\n",
        "    if train:\n",
        "      stopping_probability, networks, losses = self.neural_stopping.train_network(stock_values,\n",
        "                                                                          current_payoff,\n",
        "                                                                          future_payoff)\n",
        "      #inputs = stock_values\n",
        "      #stopping_probability , networks   = self.neural_stopping.evaluate_network(inputs)\n",
        "    return stopping_probability, networks, losses  "
      ],
      "metadata": {
        "id": "T6mxhgqPaQj5"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pricing = Training(S_train, Payoff_)\n",
        "mods, loss_function = pricing.value()"
      ],
      "metadata": {
        "id": "2XEhZY0obdbh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a90d218d-f45c-4454-8192-29a00f0edbc7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date 9 , 4\n",
            "date 8 , 1  mean reward  621.0027823356265\n",
            "date 7 , 1  mean reward  556.2443074572526\n",
            "date 6 , 1  mean reward  26.101753460204808\n",
            "date 5 , 1  mean reward  -389.5036166931548\n",
            "date 4 , 0  mean reward  -189.20875046008703\n",
            "date 3 , 0  mean reward  -182.47124312378895\n",
            "date 2 , 0  mean reward  -182.2346240697439\n",
            "date 1 , 0  mean reward  -182.2130569136398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_list = list(filter(None, loss_function))\n",
        "legend = [\"n = 1\", \"n = 2\", \"n = 3\", \"n = 4\", \"n = 5\", \"n = 6\", \"n = 7\", \"n = 8\"]\n",
        "\n",
        "for i in range(len(filtered_list)):\n",
        "  epochs = np.array([i for i in range(len(filtered_list[0]))])\n",
        "  plt.plot(epochs, filtered_list[i], label='loss funciton')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epochs')\n",
        "  plt.legend(legend)\n",
        "  plt.title('Loss curves across time periods')\n",
        "  plt.plot()\n"
      ],
      "metadata": {
        "id": "6IewJAQSf8OT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "b4e7a8dd-f19e-4c20-9592-dc23e87d8d1c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "class Payoff_:\n",
        "  def __init__(self, model):\n",
        "    self.strike = model.strike\n",
        "    self.model = model\n",
        "\n",
        "  def MaxCall(self, X):\n",
        "    payoff = np.max(X, axis=1) - self.strike\n",
        "    return payoff.clip(0, None)\n",
        "\n",
        "  \n",
        "  def MaxCall_(self, n, X): # already get tensor\n",
        "    a = torch.tensor(())\n",
        "    for m in range(0, self.model.paths):\n",
        "      date = int(n[m])\n",
        "      tensorX= torch.from_numpy(X[date,m,:])\n",
        "      max1=torch.max(tensorX-100)  \n",
        "      i = torch.tensor([max1]).float()\n",
        "      a = torch.cat((a, i), 0)\n",
        "    return a\n",
        "\n",
        "'''    \n",
        "\n",
        "\n",
        "def loss(y_pred,S, X, n, tau): # input tau is tau_mat_test[date+1]\n",
        "    payoff = Payoff_(model)\n",
        "    r_n=torch.zeros((S.paths))\n",
        "    for m in range(0,S.paths):\n",
        "\n",
        "        r_n[m]=-(payoff.MaxCall(n,m,X)*y_pred[m] + payoff.MaxCall(tau[m],m,X)*(1-y_pred[m]))\n",
        "    \n",
        "    return(r_n.mean())\n",
        "\n",
        "'''\n",
        "# CHECK TRAINING\n",
        "\n",
        "# check training\n",
        "hyperparam_training = {'drift': 0.2, 'sigma': 0.05, 'delta': 0.1,  'paths':7, 'periods': 9, 'maturity': 3., 'strike' : 100,'assets':2,  'spot':90,}\n",
        "model=BlackScholes(**hyperparam_training)\n",
        "neural_stopping = Training_network(model.assets)\n",
        "stock_paths = model.simulate_process()    \n",
        "disc_factor = np.math.exp((-model.drift) * model.maturity/(model.periods))\n",
        "\n",
        "payoff = Payoff_(model)\n",
        "# some objects\n",
        "mods=[None]*model.periods\n",
        "tau_dates=np.zeros((model.periods+1,model.paths)) # record stopping times\n",
        "tau_dates[model.periods,:]=model.periods\n",
        "loss_functions = [None]*model.periods # record loss\n",
        "F_theta_train=np.zeros((model.periods+1,model.paths))\n",
        "F_theta_train[model.periods,:]=1\n",
        "    \n",
        "    \n",
        "# AT MATURITY N\n",
        "\n",
        "terminal_payoff = payoff.MaxCall_(tau_dates[model.periods,:], stock_paths) # payoff of the last date\n",
        "#print(\"terminal_payoff\", terminal_payoff)\n",
        "print(\"tau end\", tau_dates[model.periods,:])\n",
        "m=1\n",
        "b = torch.tensor(())\n",
        "n=tau_dates[model.periods,:]\n",
        "print(\"n\", n)\n",
        "date = n[m]\n",
        "print(\"date\", date)\n",
        "tensorX= torch.from_numpy(stock_paths[int(date),m,:])\n",
        "max1=torch.max(tensorX-100)  \n",
        "print(\"max1\", max1)\n",
        "\n",
        "# recursive calc. before maturity\n",
        "date = stock_paths.shape[0] - 2\n",
        "tau_date_plus_one = tau_dates[date+1, :]\n",
        "print(\"tau_date_plus_one\", tau_date_plus_one)\n",
        "date1 = int(tau_date_plus_one[m])\n",
        "print(date1)\n",
        "\n",
        "continuation_value =  payoff.MaxCall_(tau_date_plus_one, stock_paths)\n",
        "current_value =  payoff.MaxCall_([date]*model.paths, stock_paths)\n",
        "print([date]*model.paths)\n",
        "#print(\"continuation_value\", continuation_value)\n",
        "print(\"current_value\", current_value)\n",
        "\n",
        "a = torch.tensor(())\n",
        "for m in range(0, model.paths):\n",
        "  date = int(tau_date_plus_one[m])\n",
        "  tensorX= torch.from_numpy(stock_paths[date,m,:])\n",
        "  max1=torch.max(tensorX-100)  \n",
        "  i = torch.tensor([max1]).float()\n",
        "  a = torch.cat((a, i), 0)\n",
        "print(\"a\", a)\n",
        "\n",
        "X_inputs = torch.from_numpy(stock_paths[date, : , :]).double() # input to the NN must be a tensor\n",
        "\n",
        "print(X_inputs.shape) # 7 paths, 2 assets for that date\n",
        "#print(\"X_inputs\", X_inputs)\n",
        "\n",
        "\n",
        "\n",
        "stopping_rule , networks, loss = neural_stopping.train_network(stock_paths[date, : , :], \n",
        "                                                  current_value,\n",
        "                                                  continuation_value*disc_factor)\n",
        "mods[date]=networks\n",
        "loss_functions[date]=loss\n",
        "print(\"date\", date, \",\", len([1 for l in stopping_rule if l > 0.5]), \" mean loss \", np.mean(loss))\n",
        "print(stopping_rule.shape)\n",
        "F_theta_train[date,:]=(stopping_rule > 0.5)*1.0 \n",
        "#print(F_theta_train[date,:])\n",
        "tau_dates[date,:]=np.argmax(F_theta_train, axis=0)\n",
        "#print(tau_dates[date,:].shape)\n",
        "\n",
        "#print(tau_dates)\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "5oB2_W0Yi5p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lower bound\n",
        "\n",
        "the stopping time $\\tau^{\\Theta}$ gives a lower bound $L=\\mathbb{E}g(\\tau^{\\Theta}, X_{\\tau^{\\Theta}})$ for the optimal value $V_0= \\sup_{\\tau \\in \\mathcal{T}}\\mathbb{E}g(\\tau, X_{\\tau})$.\n",
        "\n",
        "Simulate \n",
        "- $K_L = 1024$ paths $(y_n^k)_{n=0}^N$, $k=1, \\ldots, K_L$, of $(X_n)_{n=0}^N$ and assume these are drawn independently from the realizations $(x_n^k)_{n=0}^N$, $k=1, \\ldots, K$.\n",
        "\n",
        "The unbiased estimate of the lower bound $L$ is given by\n",
        "\\begin{equation}\n",
        "\\hat{L}=\\frac{1}{K_L} \\sum_{k=1}^{K_L} g(l^k, y_{l^k}^k)\n",
        "\\end{equation}\n",
        "where $l^k = l(y_0^k, \\ldots, y_{N-1}^k)$"
      ],
      "metadata": {
        "id": "u6ULiVMClRqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing phase - Lower bound\n",
        "\n",
        "# sample y from the process (Y)\n",
        "hyperparam_testing_L = {'drift': 0.2, 'sigma': 0.05, 'delta': 0.1,  'paths':40, 'periods': 9, 'maturity': 1., 'strike' : 100,'assets':2,  'spot':90,}\n",
        "S_test_L=BlackScholes(**hyperparam_testing_L)\n",
        "\n",
        "# now we can compute all the stopping times recursively"
      ],
      "metadata": {
        "id": "fFwBlDkLbnxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "model = S_test_L\n",
        "stock_paths = model.simulate_process()\n",
        "date = stock_paths.shape[0] - 2\n",
        "\n",
        "mod_curr = mods[date]     \n",
        "probs=mod_curr(torch.from_numpy(stock_paths[date])) \n",
        "np_probs=probs.detach().numpy().reshape(model.paths)     \n",
        "which = np_probs > 0.5\n",
        "print(which)\n",
        "\n",
        "neural_stopping = Training_network(model.assets, model.paths)\n",
        "stopping_probability , networks   = neural_stopping.evaluate_network(stock_paths[date])\n",
        "print(stopping_probability  > 0.5)\n",
        "'''"
      ],
      "metadata": {
        "id": "MW7B4vELqcXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "\n",
        "class Testing_Lower:\n",
        "  def __init__(self, model, payoff, mods):   \n",
        "    self.model = model # argument is S   \n",
        "    self.payoff = payoff(self.model)\n",
        "    #self.neural_stopping = Training_network(self.model.assets)\n",
        "    self.mods = mods\n",
        "\n",
        "\n",
        "\n",
        "  def price(self):\n",
        "    model = self.model    \n",
        "    stock_paths = self.model.simulate_process()\n",
        "    \n",
        " \n",
        "    # at maturity N\n",
        "    final_payoff = self.payoff.MaxCall(stock_paths[-1, :, :]) # payoff of the last date\n",
        "    payoff_0 = self.payoff.MaxCall(stock_paths[0, :, :])  \n",
        "    values = final_payoff\n",
        "    print(\"date\", model.periods, \":\", 1.0,\" , \", 1.0, \" , \", model.paths, \"value\", round(np.mean(values), 3))\n",
        "\n",
        "\n",
        "    # recursive calc. before maturity\n",
        "         \n",
        "    for date in range(stock_paths.shape[0] - 2, 0, -1):\n",
        "      current_payoff = self.payoff.MaxCall(stock_paths[date, :, :])\n",
        "      mod_curr=self.mods[date]\n",
        "\n",
        "      #probs = self.neural_stopping.evaluate_network(stock_paths[date, : , :])\n",
        "      \n",
        "      probs=mod_curr(torch.from_numpy(stock_paths[date, :, :])) \n",
        "      np_probs=probs.detach().numpy().reshape(self.model.paths)     \n",
        "      which = np_probs > 0.5\n",
        "\n",
        "      values[which] = current_payoff[which]\n",
        "      values[~which] *= (np.math.exp((-model.drift) * (model.periods-date)/model.periods))\n",
        "      print(\"date\", date, \":\", round(np.min(np_probs), 3),\" , \", round(np.max(np_probs), 3), \" , \", len([1 for l in np_probs if l > 0.5]), \"value\", round(np.mean(values), 3))\n",
        "\n",
        "    \n",
        "    return round(payoff_0[0], 3), round(np.mean(values)*(np.math.exp((-model.drift) * (date/model.periods))) , 3)\n",
        "\n"
      ],
      "metadata": {
        "id": "aNegNcskb5gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "price_testing = Testing_Lower(S_test_L, Payoff, mods)\n",
        "\n",
        "Y_test_mean, MC_estimate = price_testing.price()\n",
        "print(Y_test_mean, MC_estimate)"
      ],
      "metadata": {
        "id": "1oRLovDLcEwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upper Bound\n",
        "\n",
        "For every $(\\mathcal{F}_n)$-martingale $(M_n)_{n=0}^N$ starting from $0$ and each sequence of integrable error terms $(\\epsilon_n)_{m=0}^N$ satisfying $\\mathbb{E}[\\epsilon_n | \\mathcal{F}_n]=0$ for all $n$, the following expression provides an upper bound for $V_0$, which is also tight if $M=M^H$ and $\\epsilon \\equiv 0$.\n",
        "\\begin{equation}\n",
        "U = \\mathbb{E} \\Big[ \\max_{0 \\leq n \\leq N} [g(n, X_n) - M_n^{\\Theta} - \\epsilon_n ]  \\Big]\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "We need an expression for $M^H$ and use the Doob-Meyer decomposition on the Snell envelope of the reward process:\n",
        "\\begin{equation}\n",
        "H_n = \\text{ess} \\sup_{\\tau \\in \\mathcal{T}_n} \\mathbb{E}[g(\\tau)| \\mathcal{F}_n], \\;\\;\\;\\ n=0, 1, \\ldots, N\n",
        "\\end{equation}\n",
        "where its Doob-Meyer deomposition is given by:\n",
        "\\begin{equation}\n",
        "H_n = H_0 + M_n^H - A_n^H\n",
        "\\end{equation}\n",
        "and\n",
        "\\begin{equation}\n",
        "M_0^H = 0\\;\\;\\;\\; \\text{and} \\;\\; M_n^H-M_{n-1}^H=H_n-\\mathbb{E}[H_n | \\mathcal{F}_{n-1}], \\;\\;\\; n=1, \\ldots, N\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\n",
        "We use $\\tau^{\\Theta}$ to construct a martingale close to $M^H$. The martingale part of $(H_n^{\\Theta})_{n=0}^N$ is given by:\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "&M_0^{\\Theta}\\\\\n",
        "&M_N^{\\Theta}- M_{n-1}^{\\Theta} = H_n^{\\Theta}-\\mathbb{E}[H_n^{\\Theta} | \\mathcal{F}_{n-1}] = f^{\\theta_n}(X_n)g(n, X_n) + (1- f^{\\theta_n})) C_n^{\\Theta}-C_{n-1}^{\\Theta}, \\;\\; n \\geq 1\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "and the continuation value is:\n",
        "\\begin{equation}\n",
        "C_n^{\\Theta}=\\mathbb{E}[g(\\tau_{n+1}^{\\Theta}, X_{\\tau_{n+1}^{\\Theta}})| \\mathcal{F}_n] = \\mathbb{E}[g(\\tau_{n+1}^{\\Theta}, X_{\\tau_{n+1}^{\\Theta}})| X_n], \\;\\;\\;\\ n=0, 1, \\ldots, N-1\n",
        "\\end{equation}\n",
        "there is no need to specify $C_N^{\\Theta}$ because $(1- f^{\\theta_N}(X_N))$ is always $0$.\n",
        "\n",
        "\n",
        "Simulate \n",
        "- $K_U = 1024$ paths $(z_n^k)_{n=0}^N$, $k=1, \\ldots, K_U$, of $(X_n)_{n=0}^N$\n",
        "- $K_U \\times J$ realizations $(v_n^{k,j})_{n=0}^N$, $k=1, \\ldots, K_U$, $j=1, \\ldots, J$, of $(W_{t_n} - W_{t_n - 1})_{n=1}^N$ with $J=16384$\n",
        "- for all $n$ and $k$, generate the $i$-th component of the $j$-th continuation path departing from  $z_n^k$ according to:\n",
        "\\begin{equation}\n",
        "\\tilde{z}_n^{i,k,j}=z_n^{i,k} \\exp \\Big([r- \\delta_i - \\sigma_i^2 /2] (m-n)\\Delta t + \\sigma_i [v_{n+1}^{i,k,j} + \\ldots, v_{m}^{i,k,j}]  \\Big), \\;\\;\\;\\; m=n+1, \\ldots, N\n",
        "\\end{equation}\n",
        "we assume that $\\tilde{z}_{n+1}^{k,j}, \\ldots, \\tilde{z}_{N}^{k,j}$ are conditionally independent of each other and of $z_{n+1}^{k}, \\ldots, z_{N}^{k}$ \n"
      ],
      "metadata": {
        "id": "ffDydd-UaBXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing phase - Upper bound\n",
        "\n",
        "# sample Z from the process (X)\n",
        "hyperparam_testing_U = {'drift': 0.2, 'sigma': 0.05, 'delta': 0.1,  'paths':1024, 'periods': 4, 'maturity': 3., 'strike' : 100,'assets':1,  'spot':90,}\n",
        "S_test_U=BlackScholes(**hyperparam_testing_U)\n",
        "stock_paths = S_test_U.simulate_process()\n",
        "print(stock_paths.shape) #(5, 1024, 1)"
      ],
      "metadata": {
        "id": "vSpdEj0XXLuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# need to generate v values\n",
        "\n",
        "class Zvalues:\n",
        "  def __init__(self, drift, sigma, delta, spot, assets,  paths, periods,\n",
        "         maturity, strike, dividend=0):\n",
        "\n",
        "    self.drift = drift - dividend\n",
        "    self.sigma = sigma\n",
        "    self.delta = delta\n",
        "    self.spot = spot\n",
        "    self.assets = assets\n",
        "    self.paths = paths\n",
        "    self.periods = periods\n",
        "    self.maturity = maturity\n",
        "    self.strike = strike\n",
        "    self.dt = self.maturity / self.periods\n",
        "    self.df = math.exp(-self.drift * self.dt)\n",
        "\n",
        "  def drift_fct(self, x, t):\n",
        "    del t\n",
        "    return self.drift * x\n",
        "\n",
        "  def diffusion_fct(self, x, t, v=0):\n",
        "    del t\n",
        "    return self.sigma * x\n",
        "\n",
        "\n",
        "\n",
        "  def simulate_process(self):\n",
        "    \"\"\"Returns a nparray (nb_paths * assets * nb_dates) with prices.\"\"\"\n",
        "    paths = self.paths\n",
        "    spot_paths = np.empty((self.periods+1, paths))\n",
        "\n",
        "    spot_paths[0, :] = self.spot\n",
        "    random_numbers = np.random.normal(\n",
        "        0, 1, (self.periods, paths))\n",
        "    dW = random_numbers * np.sqrt(self.dt)\n",
        "    drift = self.drift\n",
        "    r = np.repeat(np.repeat(\n",
        "        np.reshape(drift, (-1, 1)), self.periods, axis=0),\n",
        "        paths, axis=1)\n",
        "    sig = np.ones((self.periods, paths))*self.sigma\n",
        "    #sig = np.repeat(np.repeat(np.repeat(\n",
        "    #    np.reshape(self.sigma, (-1, 1, 1)), self.periods+1, axis=2),\n",
        "    #    paths, axis=1), self.assets, axis=0)\n",
        "    \n",
        "    spot_paths[1:, :] = np.repeat(\n",
        "        spot_paths[0:1, :], self.periods, axis=0)* np.exp(np.cumsum((r-self.delta) * self.dt - (sig ** 2) * self.dt / 2 + sig * dW, axis=0))\n",
        "\n",
        "    return spot_paths #.reshape(spot_paths.shape[2], spot_paths.shape[0], spot_paths.shape[1])"
      ],
      "metadata": {
        "id": "TshW4LLYDkEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparam_V = {'drift': 0.2, 'sigma': 0.05, 'delta': 0.1,  'paths':7, 'periods': 4, 'maturity': 3., 'strike' : 100,  'assets': 2,'spot':90,}\n",
        "S_test_U=Zvalues(**hyperparam_V)\n",
        "stock_paths_Z = S_test_U.simulate_process()\n",
        "print(stock_paths_Z)"
      ],
      "metadata": {
        "id": "ngTGSEUHZQk3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Copy of optimal_stopping_V1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}