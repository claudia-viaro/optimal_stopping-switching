{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudia-viaro/optimal_stopping-switching/blob/main/optimal_stopping/optimal_stopping_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aX5-o2Sa7nC"
      },
      "source": [
        "#Problem Formulation\n",
        "Let $(\\Omega, \\mathcal{F}, \\mathbb{P})$ be a fixed probability space on which an adapted stochastic process is defined $X=(X_t)_{0 \\leq t \\leq T}$ whose natural filtration is $(\\mathcal{F}_t^0 := \\sigma \\{ X_s, s \\leq t \\})_{0 \\leq t \\leq T}$. Let $\\mathbf{F}=(\\mathcal{F}_0)_{0 \\leq t \\leq t}$ be the complete filtration of $(\\mathcal{F}_t^0 := \\sigma \\{ X_s, s \\leq t \\})_{0 \\leq t \\leq T}$. with $P$-null sets of $\\mathcal{F}$.\n",
        "\n",
        "The stochastic process $X$ is $\\mathbb{R}^d$-valued and represents the market price of $d$ financial assets (Bermudan call options) that influence the production of power. Assume $(X^i)_{i=1}^d$ follows a geometric Brownian motion satisfying the SDE:\n",
        "\\begin{equation}\n",
        "dX_t^i = (b-\\delta_i)dt + \\sigma_i dW_t^i\n",
        "\\end{equation}\n",
        "where $W$ is a standard Brownian otion on a filtered probability space $(\\Omega, \\mathcal{F}, (\\mathcal{F}_t)_{t \\geq 0}, \\mathbb{P})$ and $b$, $d_i$, $\\sigma_i >0$ are the drift. dividend yield and volatility of the system at time $t$.\n",
        "\n",
        "We will consider a discrete time approximization (Euler schema) on an equidistant time grid $0=t_0 < t_1 < \\ldots < t_N = T$, where $t_n = n \\cdot T/N$. For $i = 1, \\ldots, d$ we simulate $p$ paths\n",
        "\\begin{equation}\n",
        "x^p_{n,i} = x_{0,i} \\cdot \\exp \\Big\\{ \\sum_{k=0}^n \\big( (b-\\delta_i - \\sigma^2_i /2)\\Delta t + \\sigma_{i} \\sqrt{\\Delta t} \\cdot Z_{k, i}^p \\big)     \\Big\\}\n",
        "\\end{equation}\n",
        "where $\\Delta t = T/N$ and $Z_{k, i}^{p} \\sim \\mathcal{N} (0,1)$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "yEZA-EFaBd0w"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "np.random.seed(234198)\n",
        "import itertools\n",
        "import random\n",
        "import time\n",
        "import scipy.stats\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as tdata\n",
        "from google.colab import files\n",
        "import helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "xJbxC0rzBhrT"
      },
      "outputs": [],
      "source": [
        "class BlackScholes:\n",
        "  def __init__(self, drift, sigma, delta, spot, assets,  paths, periods,\n",
        "         maturity, strike, dividend=0):\n",
        "\n",
        "    self.drift = drift - dividend\n",
        "    self.sigma = sigma\n",
        "    self.delta = delta\n",
        "    self.spot = spot\n",
        "    self.assets = assets\n",
        "    self.paths = paths\n",
        "    self.periods = periods\n",
        "    self.maturity = maturity\n",
        "    self.strike = strike\n",
        "    self.dt = self.maturity / self.periods\n",
        "    self.df = math.exp(-self.drift * self.dt)\n",
        "\n",
        "  def drift_fct(self, x, t):\n",
        "    del t\n",
        "    return self.drift * x\n",
        "\n",
        "  def diffusion_fct(self, x, t, v=0):\n",
        "    del t\n",
        "    return self.sigma * x\n",
        "\n",
        "\n",
        "\n",
        "  def simulate_process(self):\n",
        "    \"\"\"Returns a nparray (nb_paths * assets * nb_dates) with prices.\"\"\"\n",
        "    paths = self.paths\n",
        "    spot_paths = np.empty((self.periods+1, paths, self.assets ))\n",
        "\n",
        "    spot_paths[0, :, :] = self.spot\n",
        "    random_numbers = np.random.normal(\n",
        "        0, 1, (self.periods, paths, self.assets ))\n",
        "    dW = random_numbers * np.sqrt(self.dt)\n",
        "    drift = self.drift\n",
        "    r = np.repeat(np.repeat(np.repeat(\n",
        "        np.reshape(drift, (-1, 1, 1)), self.periods, axis=0),\n",
        "        paths, axis=1), self.assets, axis=2)\n",
        "    sig = np.ones((self.periods, paths, self.assets))*self.sigma\n",
        "    #sig = np.repeat(np.repeat(np.repeat(\n",
        "    #    np.reshape(self.sigma, (-1, 1, 1)), self.periods+1, axis=2),\n",
        "    #    paths, axis=1), self.assets, axis=0)\n",
        "    \n",
        "    spot_paths[1:, :,  :] = np.repeat(\n",
        "        spot_paths[0:1, :, :], self.periods, axis=0)* np.exp(np.cumsum((r-self.delta) * self.dt - (sig ** 2) * self.dt / 2 + sig * dW, axis=0))\n",
        "\n",
        "    return spot_paths #.reshape(spot_paths.shape[2], spot_paths.shape[0], spot_paths.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "v1c9hUTGBzcz"
      },
      "outputs": [],
      "source": [
        "def draw_stock_model(stockmodel):\n",
        "    stock_paths = stockmodel\n",
        "\n",
        "    # draw a path\n",
        "    \n",
        "    one_path = stock_paths[:, 0, 0]\n",
        "    dates = np.array([i for i in range(len(one_path))])\n",
        "    plt.plot(dates, one_path, label='stock path')\n",
        "    plt.ylabel('Stock price')\n",
        "    plt.ylabel('Time')\n",
        "    plt.legend()\n",
        "    return plt.show()   "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class \"Payoff\"\n",
        "This class specifies the payoff (some alternatives) that is received when selling/buying the option. \n",
        "\n",
        "- MaxCall\n",
        "\\begin{equation}\n",
        "\\pi(n) = \\Big(\\max_{d \\in \\{1, \\ldots, D \\}} x_n^d - K   \\Big)^{+} \\tag{1}\n",
        "\\end{equation}\n",
        "where $K$ is the strike price"
      ],
      "metadata": {
        "id": "aGaMrPaHkF9H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "FRYgMuM2C_K0"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "PAYOFF\n",
        "'''\n",
        "class Payoff_:\n",
        "  def __init__(self, model):\n",
        "    self.strike = model.strike\n",
        "    self.model = model\n",
        "\n",
        "  \n",
        "  def MaxCall_(self, n, X): # already get tensor\n",
        "    a = torch.tensor(())\n",
        "    for m in range(0, self.model.paths):\n",
        "      date = n[m]\n",
        "      tensorX= torch.from_numpy(X[int(date),m,:])\n",
        "      max1=torch.max(tensorX-100) \n",
        "      max2 = torch.max(max1, torch.tensor([0.0])).float()  \n",
        "      i = torch.tensor([max2]).float()\n",
        "      a = torch.cat((a, i), 0)\n",
        "    return a"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class \"Ftheta_NN\"\n",
        "\n",
        "We employ a neural network to approximate the stopping decision functions $\\{f_n\\}_{n=0}^N$ by constructing a sequence of neural networks of the form $f^{\\theta_n}:\\mathbb{R}^d → \\{0,1\\}$ with parameters $\\theta_n \\in \\mathbb{R}^q$ to approximate $f_n$.\n",
        "\n",
        "The neural network used here takes the form $F^{\\theta}: \\mathbb{R}^d → (0,1)$ for $\\theta \\in \\{\\theta_0, \\ldots, \\theta_N  \\}$, that is the parameters are trained via a neural network that outputs probabilities in the interval $(0,1)$. This is due to the fact that the G-B optimization algorithm is to be applied to a continuous function with respect to $\\theta_n$, which $f^{\\theta_n}$ is not. Hence, the multi-layer, feed-forward neural network takes the form:\n",
        "\n",
        "\\begin{equation}\n",
        "F^{\\theta}= \\psi \\circ a_3^{\\theta} \\circ \\phi_{q_2} \\circ a_2^{\\theta} \\circ \\phi_{q_1} \\circ a_1^{\\theta}\n",
        "\\end{equation}\n",
        "where \n",
        "\n",
        "-  $q_1, q_2$ are the number of nodes in the hidden layers\n",
        "- $a_1^{\\theta} : \\mathbb{R}^d → \\mathbb{R}^{q_1}, a_2^{\\theta}: \\mathbb{R}^{q_1} → \\mathbb{R}^{q_2}$ are linear transformation functions: $a_i^{\\theta}(x)=W_i x + b_i$ with matrices $W_1 \\in \\mathbb{R}^{q_1 \\times d}, W_2 \\in \\mathbb{R}^{q_2 \\times q_1}, W_3 \\in \\mathbb{R}^{q_2 \\times 1}$ and vectors $b_1 \\in \\mathbb{R}^{q_1}, b_2 \\in \\mathbb{R}^{q_2}, b_3 \\in \\mathbb{R}^{1}$.\n",
        "- $\\phi_{q_i}: \\mathbb{R}^{q_i}$ is the ReLU activation function: $\\phi_{q_1}(x_i, \\ldots, x_{q_i})=(x_i^{+}, \\ldots, x_{q_i}^{+})$\n",
        "- $\\psi = \\mathbb{R} → \\mathbb{R}$ is the logistic sigmoid function: $\\psi(x)=1/(1+ e^{-x})$.\n",
        "Between the layers a batch normalization is also added, it takes the output from the previous layer and normalizes it before sending it to the next layer. This has the effect of stabilizing the neural network. \n",
        "\n",
        "The parameters will comprise $\\theta = \\{W_1, W_2,, W_3, b_1, b_2, b_3\\}\\in \\mathbb{R}^q$, where $q=q_1(d+q_2+1)+2q_2+1$. The value of $d$ stands for the dimension, that is the number of assets and will be varied among $d=\\{2,4, 5, 10, 20\\}$. \n"
      ],
      "metadata": {
        "id": "1eZ7Tt2TlARg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "MpTiRUxLBj5L"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Neural network\n",
        "'''\n",
        "\n",
        "class Ftheta_NN(nn.Module):\n",
        "  def __init__(self, assets):\n",
        "    super(Ftheta_NN, self).__init__()\n",
        "    H = assets + 40\n",
        "    self.bn0 = nn.BatchNorm1d(num_features=assets)\n",
        "    self.a1 = nn.Linear(assets, H)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.bn1 = nn.BatchNorm1d(num_features=H)\n",
        "    self.a2 = nn.Linear(H, H)\n",
        "    self.bn2 = nn.BatchNorm1d(num_features=H)\n",
        "    self.a3 = nn.Linear(H, 1)\n",
        "    self.bn3 = nn.BatchNorm1d(num_features=1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, input):\n",
        "    out = self.bn0(input)\n",
        "    out = self.a1(out)\n",
        "    out = self.relu(out)\n",
        "    #out = self.bn1(out)\n",
        "    #out = self.a2(out)    \n",
        "    #out = self.relu(out)\n",
        "    out = self.bn2(out)\n",
        "    out = self.a3(out)    \n",
        "    out = self.sigmoid(out)\n",
        "    return out\n",
        "\n",
        "# set initial weights of a linear layer of the NN with uniform values and bias=0.01 (or choose zero initial weights)\n",
        "def init_weights(m):\n",
        "  if isinstance(m, torch.nn.Linear):\n",
        "    torch.manual_seed(42)\n",
        "    # torch.nn.init.zeros_(m.weight)\n",
        "    torch.nn.init.xavier_uniform_(m.weight)\n",
        "    m.bias.data.fill_(0.01) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class \"Training_network\"\n",
        "\n",
        "The NN is used to approximate the optimal stopping decisions $f_n: \\mathbb{R}^d \\rightarrow \\{0, 1 \\}$, $n = \\{ 1, \\ldots, N-1 \\}$, at each date by a neural network $f^{\\theta}: \\mathbb{R}^d \\rightarrow \\{0, 1 \\}$ with parameter $\\theta \\in \\mathbb{R}^q$. \n",
        "\n",
        "We choose $\\theta_N \\in \\mathbb{R}^q$ such that $f^{\\theta}_N \\equiv 1$ and determine $\\theta_n \\in \\mathbb{R}^q$ for $n \\leq N-1$ by recursion of the form:\n",
        "\n",
        "\\begin{equation}\n",
        "\\tau_{n+1} = \\sum_{m=n+1}^N m f^{\\theta_m}(X_m) \\prod _{j=n+1}^{m-1} (1-f^{\\theta_j}(X_j))\n",
        "\\end{equation}\n",
        "\n",
        "Since $f^{\\theta}$ takes values in $\\{ 0,1 \\}$, hence not appropriate for a gradient-descent optimization method, the neural network includes a layer performing a logistic transformation such that we have a continuous output function $F^{\\theta}: \\mathbb{R}^d \\rightarrow (0,1)$.\n",
        "\n",
        "At each time step, for each epoch we compute $F^{\\theta_n}$ using the $\\theta_n$ from the previous epoch. Then, the parameter $\\theta_n$ is update via backpropagation by the gradient of the loss function (Adam optimization algorithm \\citep{kingma2014adam}), which is specified as:\n",
        "\n",
        "\\begin{equation}\n",
        "    Loss = - \\mathbb{E}[g(n, X_n)F^{\\theta_n}(X_n) + g(\\tau_{n+1}, X_{\\tau_{n+1}})(1-F^{\\theta_n}(X_n))]\n",
        "\\end{equation}\n",
        "\n",
        "The aim is to determine $\\theta_n \\in \\mathbb{R}^q$ so that the negative of the loss function is close to the supremum $\\sup_{\\theta \\in \\mathbb{R}^q}\\mathbb{E}[g(n, X_n)F^{\\theta}(X_n) + g(\\tau_{n+1}, X_{\\tau_{n+1}})(1-F^{\\theta}(X_n))   ]$. \n",
        "\n",
        "Looking at the formula for the loss function, it takes as inputs:\n",
        "- _current payoff:_ payoff of the option computed at time $n$ for all paths if it is exercised at time $n$. this is the value of the option at time $n$ when it is exercised\n",
        "- _future payoff:_ expected value of the future payoff, computed at a stopping time observed in the future ($\\tau+1$), this is the value of the option at time $n$ when it is not exercised (continuation value)\n",
        "\n",
        "The NN takes as inputs:\n",
        "- _stock prices:_ the prices of the stock at time $n$ across multiple paths\n",
        "\n",
        "and as outputs:\n",
        "- values in $\\{ 0,1 \\}$, representing the probability of stopping the process (exercising the option). "
      ],
      "metadata": {
        "id": "DkGCjRMDJuP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Training_network(object):\n",
        "\n",
        "  def __init__(self, assets,  epochs=400, batch_size=2000):\n",
        "    self.assets = assets\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = batch_size\n",
        "    self.network = Ftheta_NN(self.assets).double()\n",
        "    self.network.apply(init_weights)\n",
        "\n",
        "  # training part\n",
        "  def train_network(self,  stock_values, current_payoff,\n",
        "                    future_payoff):\n",
        "    \n",
        "    # set values for the NN inputs (stock_values) \n",
        "    X_inputs = torch.from_numpy(stock_values).double() \n",
        "\n",
        "    # set values for the loss\n",
        "    # current_payoff, future_payoff are already tensors\n",
        "    ones = torch.ones(len(future_payoff)) # we need a vector of 1's in the loss function\n",
        "\n",
        "    # several optimization methods are available (here Adam algorithm). as argument input the parameters to be optimized    \n",
        "    # try diff learning rates\n",
        "    optimizer = optim.Adam(self.network.parameters())\n",
        "    self.network.train(True) # set training mode ON    \n",
        "    losses = []\n",
        "    #weights = []\n",
        "    \n",
        "    for epoch in range(self.epochs):\n",
        "      running_loss = 0.0\n",
        "      for batch in tdata.BatchSampler(\n",
        "              tdata.RandomSampler(range(len(X_inputs)), replacement=False),\n",
        "              batch_size=self.batch_size, drop_last=False):\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(True):\n",
        "          F_theta = self.network.forward((X_inputs[batch])).reshape(-1) \n",
        "          reward = (current_payoff[batch].reshape(-1)* F_theta + \n",
        "                    future_payoff[batch] * (ones[batch] - F_theta)) \n",
        "          \n",
        "          # compute loss function\n",
        "          loss = -torch.mean(reward)\n",
        "          \n",
        "          # compute gradients\n",
        "          loss.backward()\n",
        "          # take a step, updating the parameters \n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss.item() * self.batch_size\n",
        "      epoch_loss = running_loss /  len(tdata.BatchSampler(\n",
        "              tdata.RandomSampler(range(len(X_inputs)), replacement=False),\n",
        "              batch_size=self.batch_size, drop_last=False).sampler)\n",
        "      losses.append(epoch_loss*(-1))          \n",
        "    \n",
        "    #weights.append(self.network.state_dict())\n",
        "    # I would like to retain params computed at each each date so that I can access them later\n",
        "    #torch.save(weights, 'checkpoint.pth') # edit here to store them\n",
        "\n",
        "    return F_theta, self.network, losses, self.network.state_dict()\n",
        "\n",
        "  def evaluate_network(self, X_inputs, date):\n",
        "    self.network.load_state_dict(torch.load(f\"test{date}.pth\"))\n",
        "    self.network.eval()\n",
        "    X_inputs = torch.from_numpy(X_inputs).double()\n",
        "    outputs = self.network(X_inputs)\n",
        "\n",
        "    \n",
        "    return outputs.view(len(X_inputs)).detach().numpy()  "
      ],
      "metadata": {
        "id": "ZbIYQqM3bKnb"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training\n",
        "\n",
        "We conduct $3000+d$ training steps and for each we generate a batch of $8192$ paths of $(X_n)_{n=0}^N$.\n",
        "\n",
        "The resulting output will be \n",
        "- the stopping decisions $f_n: \\mathbb{R}^d \\rightarrow \\{0, 1 \\}$, $n = \\{ 1, \\ldots, N-1 \\}$\n",
        "- ideally the parameters $\\theta_n$ \n"
      ],
      "metadata": {
        "id": "F67WgZCjmvmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# simulate paths \n",
        "hyperparam_training = {'drift': 0.2, 'sigma': 0.05, 'delta': 0.1,  'paths':200, 'periods': 9, 'maturity': 1., 'strike' : 100,'assets':4,  'spot':90,}\n",
        "S_train=BlackScholes(**hyperparam_training)"
      ],
      "metadata": {
        "id": "67CEvqrDYbYI"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class \"Training\"\n",
        "\n",
        "This is a recursion. It starts by initializing:\n",
        "- model, that is the simulated stock prices\n",
        "- payoff class\n",
        "- NN \n",
        "\n",
        "Create some matrices to store values that will be accessed to at different time steps\n",
        "- _mods_ records the models at each time step, basically it appends, by date, the networks (not sure if it is enough to say that it stores the optimized parameters)\n",
        "- _loss functions_ just to plot\n",
        "- _tau dates:_ it is a matrix $(n \\times m)$, it records for each date when there is a stopping time (can only be at such date or higher)\n",
        "- _F theta:_ it is a matrix $(n \\times m)$, it records the outputs of the NN (0,1 values), only used to obtain the tau_dates\n",
        "\n",
        "The backward induction goes as:\n",
        "1. at maturity\n",
        "  - compute the terminal payoff\n",
        "  - no need to compute the stopping times/stopping decision functions because by construction $f_N \\equiv 1$\n",
        "  - hence set F_theta[N,:]=1 and tau_dates[N,:]=N\n",
        "\n",
        "2. before maturity\n",
        "- compute values for training the NN, hence current payoff $g(n, X_n)$ and future payoff $g(\\tau_{n+1}, X_{\\tau_{n+1}})$. The future payoff needs as time variable $\\tau_{n+1}$, we get it from the matrix tau_dates, taking value at $n+1$. The future payoff, as it refers to a value at time $\\tau_{n+1}$, needs to be discounted to $n$ \n",
        "- feed in the values in the function neural_stopping.train_network()\n",
        "- using the outputs returned by this function, we record values in matrices F_theta and tau_dates\n",
        "\n",
        "This is repeated until $n=1$"
      ],
      "metadata": {
        "id": "_Mizg-2Xr71G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Training:\n",
        "  def __init__(self, model, payoff_function):\n",
        "\n",
        "    self.model = model\n",
        "    self.payoff = payoff_function(self.model)\n",
        "    self.neural_stopping = Training_network(self.model.assets)\n",
        "\n",
        "  def value(self):\n",
        "    model = self.model\n",
        "    stock_paths = self.model.simulate_process()  \n",
        "\n",
        "    # lists to store some values\n",
        "    mods=[None]*model.periods # to record the models of the NN for testing\n",
        "    parameters=[None]*model.periods\n",
        "    loss_functions = [None]*model.periods # to record loss\n",
        "    tau_dates=np.zeros((model.periods+1,model.paths)) # to record stopping times    \n",
        "    F_theta_train=np.zeros((model.periods+1,model.paths)) # to record indicator of stopping times\n",
        "    \n",
        "    \n",
        "    # AT MATURITY N\n",
        "    tau_dates[model.periods,:]=model.periods   \n",
        "    final_dates =  tau_dates[model.periods,:]\n",
        "    F_theta_train[model.periods,:]=1\n",
        "    \n",
        "    terminal_payoff = self.payoff.MaxCall_(final_dates, stock_paths) # payoff of the last date\n",
        "\n",
        "    print(\"date\", model.periods, \", # switches\", model.paths)\n",
        " \n",
        "\n",
        "    # BEFORE MATURITY\n",
        "    for date in range(stock_paths.shape[0] - 2, 0, -1):      \n",
        "      tau_date_plus_one = tau_dates[date+1, :]\n",
        "      discount_factor = np.exp(-model.drift*model.dt*(tau_date_plus_one-date))\n",
        "\n",
        "      continuation_value =  self.payoff.MaxCall_(tau_date_plus_one, stock_paths)\n",
        "      current_value =  self.payoff.MaxCall_([date]*model.paths, stock_paths)\n",
        "      \n",
        "      stopping_rule , networks, loss, params = self.neural_stopping.train_network(stock_paths[date, : , :], \n",
        "                                                  current_value,\n",
        "                                                  continuation_value*discount_factor)\n",
        "      mods[date]=networks\n",
        "      loss_functions[date]=loss\n",
        "      F_theta_train[date,:]=(stopping_rule > 0.5)*1.0 \n",
        "      tau_dates[date,:]=np.argmax(F_theta_train, axis=0)\n",
        "\n",
        "      print(\"date\", date, \", # switches\", len([1 for l in stopping_rule if l > 0.5]), \" mean reward \", np.mean(loss))\n",
        "      parameters[date]=params\n",
        "      torch.save(params, f\"test{date}.pth\")\n",
        "      \n",
        "    return mods, loss_functions"
      ],
      "metadata": {
        "id": "T6mxhgqPaQj5"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pricing = Training(S_train, Payoff_)\n",
        "mods, loss_function = pricing.value()"
      ],
      "metadata": {
        "id": "2XEhZY0obdbh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "409fd0dc-218d-4b93-da63-34dba8e21a8c"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date 9 , # switches 200\n",
            "date 8 , # switches 29  mean reward  39.44008204728479\n",
            "date 7 , # switches 10  mean reward  38.33466072438475\n",
            "date 6 , # switches 7  mean reward  36.72408461136175\n",
            "date 5 , # switches 4  mean reward  34.79365579536397\n",
            "date 4 , # switches 1  mean reward  33.74188666316317\n",
            "date 3 , # switches 1  mean reward  32.97382390153115\n",
            "date 2 , # switches 1  mean reward  31.61048352472356\n",
            "date 1 , # switches 1  mean reward  30.855488476061183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_list = list(filter(None, loss_function))\n",
        "legend = [f't={i}' for i in range(len(loss_function))]\n",
        "\n",
        "for i in range(len(filtered_list)):\n",
        "  epochs = np.array([i for i in range(len(filtered_list[0]))])\n",
        "  plt.plot(epochs, filtered_list[i], label='reward funciton')\n",
        "  plt.ylabel('reward')\n",
        "  plt.xlabel('epochs')\n",
        "  plt.legend(legend)\n",
        "  plt.title('Reward curves across time periods')\n",
        "  plt.plot()\n"
      ],
      "metadata": {
        "id": "6IewJAQSf8OT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "b5d5eed7-27b5-43c5-d828-7490b8a15687"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lower bound\n",
        "\n",
        "the stopping time $\\tau^{\\Theta}$ gives a lower bound $L=\\mathbb{E}g(\\tau^{\\Theta}, X_{\\tau^{\\Theta}})$ for the optimal value $V_0= \\sup_{\\tau \\in \\mathcal{T}}\\mathbb{E}g(\\tau, X_{\\tau})$.\n",
        "\n",
        "Simulate \n",
        "- $K_L = 1024$ paths $(y_n^k)_{n=0}^N$, $k=1, \\ldots, K_L$, of $(X_n)_{n=0}^N$ and assume these are drawn independently from the realizations $(x_n^k)_{n=0}^N$, $k=1, \\ldots, K$.\n",
        "\n",
        "The unbiased estimate of the lower bound $L$ is given by\n",
        "\\begin{equation}\n",
        "\\hat{L}=\\frac{1}{K_L} \\sum_{k=1}^{K_L} g(l^k, y_{l^k}^k)\n",
        "\\end{equation}\n",
        "where $l^k = l(y_0^k, \\ldots, y_{N-1}^k)$"
      ],
      "metadata": {
        "id": "u6ULiVMClRqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "\n",
        "class Testing_Lower2:\n",
        "  def __init__(self, model, payoff, mods):   \n",
        "    self.model = model # argument is S   \n",
        "    self.payoff = payoff(self.model)\n",
        "    self.neural_stopping = Training_network(self.model.assets)\n",
        "    self.mods = mods\n",
        "\n",
        "\n",
        "\n",
        "  def price(self):\n",
        "    model = self.model    \n",
        "    stock_paths = model.simulate_process()\n",
        "\n",
        "    loss_functions = [None]*model.periods # to record loss\n",
        "    tau_dates=np.zeros((model.periods+1,model.paths)) # to record stopping times    \n",
        "    F_theta_train=np.zeros((model.periods+1,model.paths)) # to record indicator of stopping times\n",
        "    \n",
        "    \n",
        "    # AT MATURITY N\n",
        "    \n",
        "    tau_dates[model.periods,:]=model.periods   \n",
        "    final_dates =  tau_dates[model.periods,:]\n",
        "    F_theta_train[model.periods,:]=1\n",
        "    \n",
        "    terminal_payoff = self.payoff.MaxCall_(final_dates, stock_paths) # payoff of the last date\n",
        "\n",
        "    print(\"date\", model.periods, \", # switches\", model.paths)\n",
        " \n",
        "\n",
        "    # BEFORE MATURITY\n",
        "         \n",
        "    for date in range(stock_paths.shape[0] - 2, 0, -1):\n",
        "      tau_date_plus_one = tau_dates[date+1, :]\n",
        "      discount_factor = np.exp(-model.drift*model.dt*(tau_date_plus_one-date))\n",
        "\n",
        "      continuation_value =  self.payoff.MaxCall_(tau_date_plus_one, stock_paths)\n",
        "      current_value =  self.payoff.MaxCall_([date]*model.paths, stock_paths)\n",
        "\n",
        "      np_probs = self.neural_stopping.evaluate_network(stock_paths[date, :, :], date)   \n",
        "      which = np_probs > 0.5\n",
        "\n",
        "      F_theta_train[date,:]=(np_probs > 0.5)*1.0 \n",
        "      tau_dates[date,:]=np.argmax(F_theta_train, axis=0)\n",
        "\n",
        "      # continuation value\n",
        "      tau = tau_dates[date,:]\n",
        "      disc_factor = np.exp(-model.drift*model.dt*(tau-date))\n",
        "      cont_value = ((self.payoff.MaxCall_(tau, stock_paths)).detach().numpy())*disc_factor\n",
        "\n",
        "      # stop value\n",
        "      # current_value\n",
        "\n",
        "      values = current_value.detach().numpy()\n",
        "      for i in range(0, model.paths):\n",
        "        if which[i]==1:\n",
        "          values[i]=current_value[i]\n",
        "        else:\n",
        "          values[i]=cont_value[i]\n",
        "      \n",
        "      # produce test loss curves\n",
        "      #ones = torch.ones(len(terminal_payoff))\n",
        "      #reward_test = (current_value.reshape(-1)* np_probs + continuation_value * (ones - np_probs))\n",
        "    \n",
        "\n",
        "      # payoff at 0\n",
        "      payoff_0 = (self.payoff.MaxCall_([0]*model.paths, stock_paths)).detach().numpy()\n",
        "      print(\"date\", date, \" , # switches, \", len([1 for l in np_probs if l > 0.5]), \"value\", round(np.mean(values), 3))\n",
        "\n",
        "    \n",
        "    return max(np.mean(payoff_0), np.mean(values)), np.mean(payoff_0)\n",
        "\n"
      ],
      "metadata": {
        "id": "Pju2V_si86rg"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing phase - Lower bound\n",
        "\n",
        "# sample y from the process (Y)\n",
        "hyperparam_testing_L = {'drift': 0.2, 'sigma': 0.05, 'delta': 0.1,  'paths':200, 'periods': 9, 'maturity': 1., 'strike' : 100,'assets':4,  'spot':90,}\n",
        "S_test_L=BlackScholes(**hyperparam_testing_L)\n",
        "\n",
        "# now we can compute all the stopping times recursively\n",
        "price_testing = Testing_Lower2(S_test_L, Payoff_, mods)\n",
        "\n",
        "MC_estimate, payoff_0 = price_testing.price()\n",
        "print(\"Monte Carlo estimate\", MC_estimate)"
      ],
      "metadata": {
        "id": "fFwBlDkLbnxg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f28c941f-e92f-44e2-b8b2-be0a19c07331"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date 9 , # switches 200\n",
            "date 8  , # switches,  34 value 4.526\n",
            "date 7  , # switches,  11 value 4.382\n",
            "date 6  , # switches,  8 value 4.262\n",
            "date 5  , # switches,  2 value 4.168\n",
            "date 4  , # switches,  6 value 4.011\n",
            "date 3  , # switches,  7 value 3.869\n",
            "date 2  , # switches,  5 value 3.754\n",
            "date 1  , # switches,  4 value 3.634\n",
            "Monte Carlo estimate 3.6336896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# produce a table\n",
        "dict ={}\n",
        " \n",
        "# Insert data into dictionary\n",
        "dict1 = {\n",
        "     1: [\"2\", 90, 97.339, 0.009],\n",
        "     2: [\"2\", 100, 205.426, 0.006],\n",
        "     3: [\"2\", 110, 315.878, 0.007],\n",
        "     7: [\"4\", 90, 130.082, 0.008],\n",
        "     8: [\"4\", 100, 235.951, 0.008],\n",
        "     9: [\"4\", 110, 334.079, 0.005],\n",
        "     10: [\"5\", 90, 134.486, 0.008],\n",
        "     11: [\"5\", 100, 224.051, 0.006],\n",
        "     12: [\"5\", 110, 282.737, 0.006],\n",
        "     13: [\"10\", 90, 158.875, 0.005],\n",
        "     14: [\"10\", 100, 273.452, 0.008],\n",
        "     15: [\"10\", 110, 391.043, 0.015],\n",
        "     16: [\"20\", 90, 100.447, 0.008],\n",
        "     17: [\"20\", 100, 192.448, 0.01],\n",
        "     18: [\"20\", 110, 301.107, 0.009],\n",
        "     }\n",
        " \n",
        "# Print the names of the columns.\n",
        "print (\"{:<10} {:<10} {:<10} {:<10}\".format('assets', 'spot', 'L', 'timeL'))\n",
        " \n",
        "# print each data item.\n",
        "for key, value in dict1.items():\n",
        "    assets, spot, L, timeL = value\n",
        "    print (\"{:<10} {:<10} {:<10} {:<10}\".format(assets, spot, L, timeL))\n",
        "\n",
        "'''    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "xT2b8j6kOCV8",
        "outputId": "21969de9-74e0-4eb3-839b-3efea59a1db6"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# produce a table\\ndict ={}\\n \\n# Insert data into dictionary\\ndict1 = {\\n     1: [\"2\", 90, 97.339, 0.009],\\n     2: [\"2\", 100, 205.426, 0.006],\\n     3: [\"2\", 110, 315.878, 0.007],\\n     7: [\"4\", 90, 130.082, 0.008],\\n     8: [\"4\", 100, 235.951, 0.008],\\n     9: [\"4\", 110, 334.079, 0.005],\\n     10: [\"5\", 90, 134.486, 0.008],\\n     11: [\"5\", 100, 224.051, 0.006],\\n     12: [\"5\", 110, 282.737, 0.006],\\n     13: [\"10\", 90, 158.875, 0.005],\\n     14: [\"10\", 100, 273.452, 0.008],\\n     15: [\"10\", 110, 391.043, 0.015],\\n     16: [\"20\", 90, 100.447, 0.008],\\n     17: [\"20\", 100, 192.448, 0.01],\\n     18: [\"20\", 110, 301.107, 0.009],\\n     }\\n \\n# Print the names of the columns.\\nprint (\"{:<10} {:<10} {:<10} {:<10}\".format(\\'assets\\', \\'spot\\', \\'L\\', \\'timeL\\'))\\n \\n# print each data item.\\nfor key, value in dict1.items():\\n    assets, spot, L, timeL = value\\n    print (\"{:<10} {:<10} {:<10} {:<10}\".format(assets, spot, L, timeL))\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "# check\n",
        "\n",
        "\n",
        "model=S_test_L\n",
        "stock_paths = model.simulate_process()\n",
        "payoff = Payoff_(model)\n",
        "neural_stopping = Training_network(model.assets)\n",
        "loss_functions = [None]*model.periods # to record loss\n",
        "tau_dates=np.zeros((model.periods+1,model.paths)) # to record stopping times    \n",
        "F_theta_train=np.zeros((model.periods+1,model.paths)) # to record indicator of stopping times\n",
        "\n",
        "\n",
        "# AT MATURITY N\n",
        "\n",
        "tau_dates[model.periods,:]=model.periods   \n",
        "final_dates =  tau_dates[model.periods,:]\n",
        "F_theta_train[model.periods,:]=1\n",
        "\n",
        "terminal_payoff = payoff.MaxCall_(final_dates, stock_paths) # payoff of the last date\n",
        "\n",
        "print(\"date\", model.periods, \", # switches\", model.paths)\n",
        "\n",
        "\n",
        "# BEFORE MATURITY\n",
        "      \n",
        "date = stock_paths.shape[0] - 2\n",
        "tau_date_plus_one = tau_dates[date+1, :]\n",
        "discount_factor = np.exp(-model.drift*model.dt*(tau_date_plus_one-date))\n",
        "\n",
        "continuation_value =  payoff.MaxCall_(tau_date_plus_one, stock_paths)\n",
        "current_value =  payoff.MaxCall_([date]*model.paths, stock_paths)\n",
        "print(\"current\", current_value)\n",
        "print(stock_paths[date, :, :].shape[0])\n",
        "np_probs = neural_stopping.evaluate_network(stock_paths[date, :, :], date)   \n",
        "which = np_probs > 0.5\n",
        "print(len(which), \"type which\", type(which))\n",
        "\n",
        "F_theta_train[date,:]=(np_probs > 0.5)*1.0 \n",
        "tau_dates[date,:]=np.argmax(F_theta_train, axis=0)\n",
        "\n",
        "# continuation value\n",
        "tau_dates = tau_dates[date,:]\n",
        "disc_factor = np.exp(-model.drift*model.dt*(tau_dates-date))\n",
        "cont_value = ((payoff.MaxCall_(tau_dates, stock_paths)).detach().numpy())*disc_factor\n",
        "print(\"len cont\", len(cont_value), cont_value[3])\n",
        "\n",
        "\n",
        "# stop value\n",
        "# current_value\n",
        "\n",
        "values = current_value.detach().numpy()\n",
        "print(\"values\", values, values[3], type(values))\n",
        "\n",
        "for i in range(0, model.paths):\n",
        "  if which[i]==1:\n",
        "    values[i]=current_value[i]\n",
        "  else:\n",
        "    values[i]=cont_value[i]\n",
        "\n",
        "\n",
        "#values[which] = current_value[which] # stop\n",
        "#values[~which] = cont_value[~which]*discount_factor     # continue\n",
        "payoff_0 = (payoff.MaxCall_([0]*model.paths, stock_paths)).detach().numpy()\n",
        "print(payoff_0)\n",
        "print(max(np.mean(payoff_0), np.mean(values)))\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "iuS35DAxJrDR",
        "outputId": "76e4ae12-7d24-4787-ced0-280c1949b6b4"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n# check\\n\\n\\nmodel=S_test_L\\nstock_paths = model.simulate_process()\\npayoff = Payoff_(model)\\nneural_stopping = Training_network(model.assets)\\nloss_functions = [None]*model.periods # to record loss\\ntau_dates=np.zeros((model.periods+1,model.paths)) # to record stopping times    \\nF_theta_train=np.zeros((model.periods+1,model.paths)) # to record indicator of stopping times\\n\\n\\n# AT MATURITY N\\n\\ntau_dates[model.periods,:]=model.periods   \\nfinal_dates =  tau_dates[model.periods,:]\\nF_theta_train[model.periods,:]=1\\n\\nterminal_payoff = payoff.MaxCall_(final_dates, stock_paths) # payoff of the last date\\n\\nprint(\"date\", model.periods, \", # switches\", model.paths)\\n\\n\\n# BEFORE MATURITY\\n      \\ndate = stock_paths.shape[0] - 2\\ntau_date_plus_one = tau_dates[date+1, :]\\ndiscount_factor = np.exp(-model.drift*model.dt*(tau_date_plus_one-date))\\n\\ncontinuation_value =  payoff.MaxCall_(tau_date_plus_one, stock_paths)\\ncurrent_value =  payoff.MaxCall_([date]*model.paths, stock_paths)\\nprint(\"current\", current_value)\\nprint(stock_paths[date, :, :].shape[0])\\nnp_probs = neural_stopping.evaluate_network(stock_paths[date, :, :], date)   \\nwhich = np_probs > 0.5\\nprint(len(which), \"type which\", type(which))\\n\\nF_theta_train[date,:]=(np_probs > 0.5)*1.0 \\ntau_dates[date,:]=np.argmax(F_theta_train, axis=0)\\n\\n# continuation value\\ntau_dates = tau_dates[date,:]\\ndisc_factor = np.exp(-model.drift*model.dt*(tau_dates-date))\\ncont_value = ((payoff.MaxCall_(tau_dates, stock_paths)).detach().numpy())*disc_factor\\nprint(\"len cont\", len(cont_value), cont_value[3])\\n\\n\\n# stop value\\n# current_value\\n\\nvalues = current_value.detach().numpy()\\nprint(\"values\", values, values[3], type(values))\\n\\nfor i in range(0, model.paths):\\n  if which[i]==1:\\n    values[i]=current_value[i]\\n  else:\\n    values[i]=cont_value[i]\\n\\n\\n#values[which] = current_value[which] # stop\\n#values[~which] = cont_value[~which]*discount_factor     # continue\\npayoff_0 = (payoff.MaxCall_([0]*model.paths, stock_paths)).detach().numpy()\\nprint(payoff_0)\\nprint(max(np.mean(payoff_0), np.mean(values)))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upper Bound\n",
        "\n",
        "For every $(\\mathcal{F}_n)$-martingale $(M_n)_{n=0}^N$ starting from $0$ and each sequence of integrable error terms $(\\epsilon_n)_{m=0}^N$ satisfying $\\mathbb{E}[\\epsilon_n | \\mathcal{F}_n]=0$ for all $n$, the following expression provides an upper bound for $V_0$, which is also tight if $M=M^H$ and $\\epsilon \\equiv 0$.\n",
        "\\begin{equation}\n",
        "U = \\mathbb{E} \\Big[ \\max_{0 \\leq n \\leq N} [g(n, X_n) - M_n^{\\Theta} - \\epsilon_n ]  \\Big]\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "We need an expression for $M^H$ and use the Doob-Meyer decomposition on the Snell envelope of the reward process:\n",
        "\\begin{equation}\n",
        "H_n = \\text{ess} \\sup_{\\tau \\in \\mathcal{T}_n} \\mathbb{E}[g(\\tau)| \\mathcal{F}_n], \\;\\;\\;\\ n=0, 1, \\ldots, N\n",
        "\\end{equation}\n",
        "where its Doob-Meyer deomposition is given by:\n",
        "\\begin{equation}\n",
        "H_n = H_0 + M_n^H - A_n^H\n",
        "\\end{equation}\n",
        "and\n",
        "\\begin{equation}\n",
        "M_0^H = 0\\;\\;\\;\\; \\text{and} \\;\\; M_n^H-M_{n-1}^H=H_n-\\mathbb{E}[H_n | \\mathcal{F}_{n-1}], \\;\\;\\; n=1, \\ldots, N\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\n",
        "We use $\\tau^{\\Theta}$ to construct a martingale close to $M^H$. The martingale part of $(H_n^{\\Theta})_{n=0}^N$ is given by:\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "&M_0^{\\Theta}\\\\\n",
        "&M_N^{\\Theta}- M_{n-1}^{\\Theta} = H_n^{\\Theta}-\\mathbb{E}[H_n^{\\Theta} | \\mathcal{F}_{n-1}] = f^{\\theta_n}(X_n)g(n, X_n) + (1- f^{\\theta_n})) C_n^{\\Theta}-C_{n-1}^{\\Theta}, \\;\\; n \\geq 1\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "and the continuation value is:\n",
        "\\begin{equation}\n",
        "C_n^{\\Theta}=\\mathbb{E}[g(\\tau_{n+1}^{\\Theta}, X_{\\tau_{n+1}^{\\Theta}})| \\mathcal{F}_n] = \\mathbb{E}[g(\\tau_{n+1}^{\\Theta}, X_{\\tau_{n+1}^{\\Theta}})| X_n], \\;\\;\\;\\ n=0, 1, \\ldots, N-1\n",
        "\\end{equation}\n",
        "there is no need to specify $C_N^{\\Theta}$ because $(1- f^{\\theta_N}(X_N))$ is always $0$.\n",
        "\n",
        "\n",
        "Simulate \n",
        "- $K_U = 1024$ paths $(z_n^k)_{n=0}^N$, $k=1, \\ldots, K_U$, of $(X_n)_{n=0}^N$\n",
        "- $K_U \\times J$ realizations $(v_n^{k,j})_{n=0}^N$, $k=1, \\ldots, K_U$, $j=1, \\ldots, J$, of $(W_{t_n} - W_{t_n - 1})_{n=1}^N$ with $J=16384$\n",
        "- for all $n$ and $k$, generate the $i$-th component of the $j$-th continuation path departing from  $z_n^k$ according to:\n",
        "\\begin{equation}\n",
        "\\tilde{z}_n^{i,k,j}=z_n^{i,k} \\exp \\Big([r- \\delta_i - \\sigma_i^2 /2] (m-n)\\Delta t + \\sigma_i [v_{n+1}^{i,k,j} + \\ldots, v_{m}^{i,k,j}]  \\Big), \\;\\;\\;\\; m=n+1, \\ldots, N\n",
        "\\end{equation}\n",
        "we assume that $\\tilde{z}_{n+1}^{k,j}, \\ldots, \\tilde{z}_{N}^{k,j}$ are conditionally independent of each other and of $z_{n+1}^{k}, \\ldots, z_{N}^{k}$ \n"
      ],
      "metadata": {
        "id": "ffDydd-UaBXq"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "optimal_stopping_V1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}