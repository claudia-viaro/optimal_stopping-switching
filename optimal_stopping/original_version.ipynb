{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "original_version.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMjE2Tp0jLLulr/ixvcrklv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudia-viaro/optimal_stopping-switching/blob/main/optimal_stopping/original_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "qX-bb2Mp-bs2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import torch.optim as optim\n",
        "from google.colab import files\n",
        "import helper\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "import torch.utils.data as tdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Model:\n",
        "  def __init__(self, drift, sigma, delta, spot, nb_stocks,\n",
        "               nb_paths, nb_dates, maturity, dividend=0):\n",
        "\n",
        "    self.drift = drift - dividend\n",
        "    self.rate = drift\n",
        "    self.delta = delta\n",
        "    self.dividend = dividend\n",
        "    self.sigma = sigma\n",
        "    self.spot = spot\n",
        "    self.nb_stocks = nb_stocks\n",
        "    self.nb_paths = nb_paths\n",
        "    self.nb_dates = nb_dates\n",
        "    self.maturity = maturity\n",
        "    self.dt = self.maturity / self.nb_dates\n",
        "    self.df = math.exp(-self.rate * self.dt)\n",
        "\n",
        "  def disc_factor(self, date_begin, date_end):\n",
        "    time = (date_end - date_begin) * self.dt\n",
        "    return math.exp(-self.drift * time)\n",
        "\n",
        "  def drift_fct(self, x, t):\n",
        "    raise NotImplemented()\n",
        "\n",
        "  def diffusion_fct(self, x, t, v=0):\n",
        "    raise NotImplemented()\n",
        "\n",
        "  def generate_one_path(self):\n",
        "      raise NotImplemented()\n",
        "\n",
        "  def generate_paths(self, nb_paths=None):\n",
        "    \"\"\"Returns a nparray (nb_paths * nb_stocks * nb_dates) with prices.\"\"\"\n",
        "    nb_paths = nb_paths or self.nb_paths\n",
        "    return np.array([self.generate_one_path() for i in range(nb_paths)])\n",
        "\n",
        "\n",
        "class BlackScholes(Model):\n",
        "  def __init__(self, drift, sigma, delta, spot, nb_stocks,\n",
        "               nb_paths, nb_dates, maturity, dividend=0):\n",
        "    super(BlackScholes, self).__init__(\n",
        "        drift=drift, sigma=sigma, delta = delta, spot=spot,\n",
        "        nb_stocks=nb_stocks, nb_paths=nb_paths, nb_dates=nb_dates,\n",
        "        maturity=maturity, dividend=dividend)\n",
        "\n",
        "  def drift_fct(self, x, t):\n",
        "    del t\n",
        "    return self.drift * x\n",
        "\n",
        "  def diffusion_fct(self, x, t, v=0):\n",
        "    del t\n",
        "    return self.sigma * x\n",
        "\n",
        "  def generate_paths(self, nb_paths=None, return_dW=False, dW=None, X0=None):\n",
        "    \"\"\"Returns a nparray (nb_paths * nb_stocks * nb_dates) with prices.\"\"\"\n",
        "    nb_paths = nb_paths or self.nb_paths\n",
        "    spot_paths = np.empty((nb_paths, self.nb_stocks, self.nb_dates+1))\n",
        "    if X0 is None:\n",
        "        spot_paths[:, :, 0] = self.spot\n",
        "    else:\n",
        "        spot_paths[:, :, 0] = X0\n",
        "    if dW is None:\n",
        "        random_numbers = np.random.normal(\n",
        "            0, 1, (nb_paths, self.nb_stocks, self.nb_dates))\n",
        "        dW = random_numbers * np.sqrt(self.dt)\n",
        "    drift = self.drift\n",
        "    r = np.repeat(np.repeat(np.repeat(\n",
        "        np.reshape(drift, (-1, 1, 1)), nb_paths, axis=0),\n",
        "        self.nb_stocks, axis=1), self.nb_dates, axis=2)\n",
        "    sig = np.repeat(np.repeat(np.repeat(\n",
        "        np.reshape(self.sigma, (-1, 1, 1)), nb_paths, axis=0),\n",
        "        self.nb_stocks, axis=1), self.nb_dates, axis=2)\n",
        "    spot_paths[:, :,  1:] = np.repeat(\n",
        "        spot_paths[:, :, 0:1], self.nb_dates, axis=2) * np.exp(np.cumsum(\n",
        "        (r-self.delta) * self.dt - (sig ** 2) * self.dt / 2 + sig * dW, axis=2))\n",
        "    # dimensions: [nb_paths, nb_stocks, nb_dates+1]\n",
        "    \n",
        "    return spot_paths\n",
        "\n",
        "\n",
        "hyperparam_test_stock_models = {\n",
        "    'drift': 0.2, 'sigma': 0.03, 'delta':0.1, 'nb_paths': 1, 'nb_dates': 9, 'maturity': 1.,\n",
        "    'nb_stocks':2, 'spot':90}\n",
        "\n",
        "BS = BlackScholes(**hyperparam_test_stock_models)\n",
        "training_set = BS.generate_paths(20)\n",
        "print(training_set.shape) #(1000, 10, 101)\n",
        "print(training_set[0].shape)\n",
        "\n",
        "validation_set = BS.generate_paths(10000)\n",
        "print(validation_set.shape) #(1000, 10, 101)\n",
        "print(validation_set[0].shape)\n",
        "\n",
        "\n",
        "\n",
        "class MaxPut:\n",
        "  def __init__(self, strike):\n",
        "    self.strike =  strike\n",
        "\n",
        "  def __call__(self, X, strike=None):\n",
        "    assert strike is None or strike == self.strike\n",
        "    return self.eval(X)\n",
        "\n",
        "  def eval(self, X):\n",
        "    # print('payoff.eval ', X, type(X))\n",
        "    payoff = self.strike - np.max(X, axis=1)\n",
        "    return payoff.clip(0, None)\n",
        "\n",
        "\n",
        "class MaxCall:\n",
        "  def __init__(self, strike):\n",
        "    self.strike =  strike\n",
        "\n",
        "  def __call__(self, X, strike=None):\n",
        "    assert strike is None or strike == self.strike\n",
        "    return self.eval(X)\n",
        "\n",
        "  def eval(self, X):\n",
        "    # print('payoff.eval ', X, type(X))\n",
        "    payoff = np.max(X, axis=1) - self.strike\n",
        "    return payoff.clip(0, None)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class NetworkDOS(nn.Module):\n",
        "  def __init__(self, nb_stocks, hidden_size=40):\n",
        "    super(NetworkDOS, self).__init__()\n",
        "    H = hidden_size + nb_stocks\n",
        "    self.bn0 = nn.BatchNorm1d(num_features=nb_stocks)\n",
        "    self.layer1 = nn.Linear(nb_stocks, H)\n",
        "    self.leakyReLU = nn.LeakyReLU(0.5)\n",
        "    self.Softplus = nn.Softplus()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.tanh = nn.Tanh()\n",
        "    self.relu = nn.ReLU()\n",
        "    self.bn1 = nn.BatchNorm1d(num_features=H)\n",
        "    self.layer2 = nn.Linear(H, H)\n",
        "    self.bn2 = nn.BatchNorm1d(num_features=H)\n",
        "    self.layer3 = nn.Linear(H, 1)\n",
        "    self.bn3 = nn.BatchNorm1d(num_features=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.bn0(x)\n",
        "    x = self.layer1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.bn2(x)\n",
        "    x = self.layer3(x)\n",
        "    x = self.sigmoid(x)\n",
        "    return x\n",
        "\n",
        "def init_weights(m):\n",
        "  if isinstance(m, torch.nn.Linear):\n",
        "    torch.manual_seed(42)\n",
        "    # torch.nn.init.zeros_(m.weight)\n",
        "    torch.nn.init.xavier_uniform_(m.weight)\n",
        "    m.bias.data.fill_(0.01)\n",
        "\n",
        "\n",
        "batch_size=2000\n",
        "network = NetworkDOS(BS.nb_stocks).double()\n",
        "network.apply(init_weights)    \n",
        "print(BS.nb_stocks)    \n",
        "print(network)\n",
        "\n",
        "\n",
        "\n",
        "def loss_fn(current, future, one, output):\n",
        "  loss = (current.reshape(-1) * output +\n",
        "              future * (one - output))\n",
        "  \n",
        "  return torch.mean(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxzR_oyu-eNk",
        "outputId": "794afe2b-ae46-4e5e-c472-768f3870a7e4"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20, 2, 10)\n",
            "(2, 10)\n",
            "(10000, 2, 10)\n",
            "(2, 10)\n",
            "2\n",
            "NetworkDOS(\n",
            "  (bn0): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layer1): Linear(in_features=2, out_features=42, bias=True)\n",
            "  (leakyReLU): LeakyReLU(negative_slope=0.5)\n",
            "  (Softplus): Softplus(beta=1, threshold=20)\n",
            "  (sigmoid): Sigmoid()\n",
            "  (tanh): Tanh()\n",
            "  (relu): ReLU()\n",
            "  (bn1): BatchNorm1d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layer2): Linear(in_features=42, out_features=42, bias=True)\n",
            "  (bn2): BatchNorm1d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layer3): Linear(in_features=42, out_features=1, bias=True)\n",
            "  (bn3): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AmericanOptionPricer:\n",
        "  \"\"\"Computes the price of an American Option using backward recusrion.\n",
        "  \"\"\"\n",
        "  def __init__(self, model, payoff):\n",
        "\n",
        "    #class model: The stochastic process model of the stock (e.g. Black Scholes).\n",
        "    self.model = model   \n",
        "\n",
        "    #class payoff: The payoff function of the option (e.g. Max call).\n",
        "    self.payoff = payoff\n",
        "\n",
        "    state_size = model.nb_stocks\n",
        "    self.neural_stopping = OptimalStoppingOptimization(\n",
        "      state_size, model.nb_paths)\n",
        "\n",
        "    \n",
        "\n",
        "  def price(self):\n",
        "    \"\"\"\n",
        "    Compute the price of an American Option using a backward recursion.\n",
        "    \"\"\"\n",
        "    model = self.model\n",
        "    \n",
        "    stock_paths = self.model.generate_paths(100)\n",
        "    mods=[None]*model.nb_dates\n",
        "    loss_functions = [None]*model.nb_dates\n",
        "\n",
        "    payoffs = self.payoff.eval(stock_paths)\n",
        "    stock_paths_with_payoff = np.concatenate(\n",
        "      [stock_paths, np.expand_dims(payoffs, axis=1)], axis=1)\n",
        "    \n",
        "    self.split = int(len(stock_paths)/2)\n",
        "    \n",
        "    disc_factor = np.math.exp((-model.rate) * model.maturity /\n",
        "               (model.nb_dates))\n",
        "    \n",
        "    # maturity\n",
        "    immediate_exercise_value = self.payoff.eval(stock_paths[:, :, -1])\n",
        "    values = immediate_exercise_value\n",
        "\n",
        "    # before maturity\n",
        "    for date in range(stock_paths.shape[2] - 2, 0, -1):\n",
        "      immediate_exercise_value = self.payoff.eval(stock_paths[:, :, date])\n",
        "      paths = stock_paths[:, :, date]\n",
        "      stopping_rule , networks, loss = self.neural_stopping.train_network(\n",
        "          paths, immediate_exercise_value,\n",
        "          values*disc_factor)\n",
        "      \n",
        "\n",
        "      which = stopping_rule > 0.5\n",
        "      values[which] = immediate_exercise_value[which]\n",
        "      values[~which] *= disc_factor\n",
        "      mods[date]=networks\n",
        "      loss_functions[date]=loss\n",
        "      print(\"date\", date, \",\", len([1 for l in stopping_rule if l > 0.5]), \" mean loss \", np.mean(loss))\n",
        "    payoff_0 = self.payoff.eval(stock_paths[:, :, 0])[0]\n",
        "    return mods, loss_functions    #max(payoff_0, np.mean(values[self.split:]) * disc_factor),\n",
        "\n",
        "\n",
        "  def stop(self, stock_values, immediate_exercise_values,\n",
        "           discounted_next_values,\n",
        "           train=True):\n",
        "    \"\"\" see base class \"\"\"\n",
        "    \n",
        "    \n",
        "    if train:\n",
        "      stopping_rule, networks, losses = self.neural_stopping.train_network(\n",
        "        stock_values[:self.split],\n",
        "        immediate_exercise_values.reshape(-1, 1)[:self.split],\n",
        "        discounted_next_values[:self.split])\n",
        "\n",
        "    #inputs = stock_values\n",
        "    #stopping_rule = self.neural_stopping.evaluate_network(inputs)\n",
        "    \n",
        "    return stopping_rule, networks, losses   "
      ],
      "metadata": {
        "id": "TUTtSqqcYAdU"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepOptimalStopping(AmericanOptionPricer):\n",
        "  \"\"\"Computes the American option price using the deep optimal stopping (DOS)\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, model, payoff, nb_epochs=20, nb_batches=None,\n",
        "               hidden_size=10):\n",
        "    super().__init__(model, payoff)\n",
        "    state_size = model.nb_stocks\n",
        "    self.neural_stopping = OptimalStoppingOptimization(\n",
        "      state_size, model.nb_paths, hidden_size=hidden_size,\n",
        "      nb_iters=nb_epochs)\n",
        "\n",
        "  def stop(self, stock_values, immediate_exercise_values,\n",
        "           discounted_next_values,\n",
        "           train=True):\n",
        "    \"\"\" see base class \"\"\"\n",
        "    \n",
        "    \n",
        "    if train:\n",
        "      self.neural_stopping.train_network(\n",
        "        stock_values[:self.split],\n",
        "        immediate_exercise_values.reshape(-1, 1)[:self.split],\n",
        "        discounted_next_values[:self.split])\n",
        "\n",
        "    inputs = stock_values\n",
        "    stopping_rule = self.neural_stopping.evaluate_network(inputs)\n",
        "    \n",
        "    return stopping_rule\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class OptimalStoppingOptimization(object):\n",
        "  \"\"\"Train/evaluation of the neural network used for the stopping decision\"\"\"\n",
        "\n",
        "  def __init__(self, nb_stocks, nb_paths, hidden_size=10, nb_iters=400,\n",
        "               batch_size=2000):\n",
        "    self.nb_stocks = nb_stocks\n",
        "    self.nb_paths = nb_paths\n",
        "    self.nb_iters = nb_iters\n",
        "    self.batch_size = batch_size\n",
        "    self.network = NetworkDOS(\n",
        "      self.nb_stocks, hidden_size=hidden_size).double()\n",
        "    self.network.apply(init_weights)\n",
        "\n",
        "  def _Loss(self, X):\n",
        "    return -torch.mean(X)\n",
        "\n",
        "  def train_network(self, stock_values, immediate_exercise_value,\n",
        "                    discounted_next_values):\n",
        "    optimizer = optim.Adam(self.network.parameters())\n",
        "    discounted_next_values = torch.from_numpy(discounted_next_values).double()\n",
        "    immediate_exercise_value = torch.from_numpy(immediate_exercise_value).double()\n",
        "    inputs = stock_values\n",
        "    X_inputs = torch.from_numpy(inputs).double()\n",
        "\n",
        "    self.network.train(True)\n",
        "    ones = torch.ones(len(discounted_next_values))\n",
        "    losses = []\n",
        "    for iteration in range(self.nb_iters):\n",
        "\n",
        "      running_loss = 0.0\n",
        "\n",
        "      for batch in tdata.BatchSampler(\n",
        "              tdata.RandomSampler(range(len(X_inputs)), replacement=False),\n",
        "              batch_size=self.batch_size, drop_last=False):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(True):\n",
        "          outputs = self.network(X_inputs[batch]).reshape(-1)\n",
        "          values = (immediate_exercise_value[batch].reshape(-1) * outputs +\n",
        "                    discounted_next_values[batch] * (ones[batch] - outputs))\n",
        "          loss = self._Loss(values)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss.item() * self.batch_size\n",
        "      epoch_loss = running_loss /  len(tdata.BatchSampler(\n",
        "              tdata.RandomSampler(range(len(X_inputs)), replacement=False),\n",
        "              batch_size=self.batch_size, drop_last=False).sampler)\n",
        "      losses.append(epoch_loss)\n",
        "\n",
        "    return outputs, self.network, losses      \n",
        "\n",
        "\n",
        "\n",
        "  def evaluate_network(self, X_inputs):\n",
        "    self.network.train(False)\n",
        "    X_inputs = torch.from_numpy(X_inputs).double()\n",
        "    outputs = self.network(X_inputs)\n",
        "    return outputs.view(len(X_inputs)).detach().numpy()"
      ],
      "metadata": {
        "id": "kauqwXS9XQZV"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ALGOS = {\"DOS\": DeepOptimalStopping,\n",
        "}\n",
        "_PAYOFFS = {\n",
        "    \"MaxPut\": MaxPut,\n",
        "    \"MaxCall\": MaxCall,}\n",
        "payoff_ = MaxCall(100)\n",
        "\n",
        "stock_model_  = BlackScholes(**hyperparam_test_stock_models)\n",
        "\n",
        "pricer = AmericanOptionPricer(stock_model_, payoff_)\n",
        "\n",
        "mods, loss_functions = pricer.price()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sn-RvarKZUhO",
        "outputId": "0c777b5b-2bbf-46c6-a998-d26c460e677f"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date 8 , 21  mean loss  -26.848627988982916\n",
            "date 7 , 13  mean loss  -26.18965285329513\n",
            "date 6 , 8  mean loss  -23.672028251532485\n",
            "date 5 , 5  mean loss  -21.164106717436283\n",
            "date 4 , 8  mean loss  -19.991851172196622\n",
            "date 3 , 4  mean loss  -17.245739258339103\n",
            "date 2 , 4  mean loss  -16.268614677669465\n",
            "date 1 , 3  mean loss  -14.38755233844579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_list = list(filter(None, loss_functions))\n",
        "legend = [\"n = 1\", \"n = 2\", \"n = 3\", \"n = 4\", \"n = 5\", \"n = 6\", \"n = 7\", \"n = 8\"]\n",
        "\n",
        "for i in range(len(filtered_list)):\n",
        "  epochs = np.array([i for i in range(len(filtered_list[0]))])\n",
        "  plt.plot(epochs, filtered_list[i], label='loss funciton')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epochs')\n",
        "  plt.legend(legend)\n",
        "  plt.title('Loss curves across time periods')\n",
        "  plt.plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "m-dimcUFdyiy",
        "outputId": "3bc3c3ca-e69f-497b-cc50-e768f80b4162"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(BS.drift, BS.nb_dates, BS.maturity)\n",
        "disc_factor = np.math.exp((-BS.drift) * BS.maturity /(BS.nb_dates))\n",
        "\n",
        "payoff_ = MaxCall(100)\n",
        "immediate_exercise_value = payoff_.eval(training_set[:, :, -1])\n",
        "final = immediate_exercise_value\n",
        "\n",
        "date = 5 #stock_paths.shape[2] - 2\n",
        "\n",
        "immediate_exercise_value = payoff_.eval(training_set[:, :, date])\n",
        "\n",
        "optimizer = optim.Adam(network.parameters())\n",
        "discounted_next_values = disc_factor * (torch.from_numpy(final).double())\n",
        "immediate_exercise_value = torch.from_numpy(immediate_exercise_value).double()\n",
        "\n",
        "X_inputs = torch.from_numpy(training_set[:, :, date]).double()\n",
        "print(X_inputs.shape)\n",
        "print('Training set has {} instances'.format(len(X_inputs)))\n",
        "display_loader = torch.utils.data.DataLoader(X_inputs, batch_size=10)\n",
        "dataiter = iter(display_loader)\n",
        "\n",
        "'''\n",
        "\n",
        "network.train(True)\n",
        "ones = torch.ones(len(discounted_next_values))\n",
        "losses = []\n",
        "for iteration in range(5000):\n",
        "  optimizer.zero_grad()\n",
        "  with torch.set_grad_enabled(True):\n",
        "    outputs = network(X_inputs).reshape(-1)\n",
        "    \n",
        "    loss = -loss_fn(immediate_exercise_value, discounted_next_values, ones, outputs)\n",
        "    losses.append(loss.item())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "'''\n",
        "print(\"training_set shape 5 paths, 2 stocks, 11 dates\", training_set.shape)\n",
        "print(\"X_inputs 5 paths X 2 stocks\", X_inputs)\n",
        "print(tdata.BatchSampler(\n",
        "              tdata.RandomSampler(range(len(X_inputs)), replacement=False),\n",
        "              batch_size=4, drop_last=False))\n",
        "for batch in tdata.BatchSampler(\n",
        "              tdata.RandomSampler(range(len(X_inputs)), replacement=False),\n",
        "              batch_size=4, drop_last=False):\n",
        "  # we sample from X_inputs batches of size 4 paths\n",
        "  print(\"batch\", X_inputs[batch], range(len(X_inputs)), batch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRc6xL34-sLJ",
        "outputId": "dbe5309c-e604-4474-982f-d211b25a4487"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2 9 1.0\n",
            "torch.Size([20, 2])\n",
            "Training set has 20 instances\n",
            "training_set shape 5 paths, 2 stocks, 11 dates (20, 2, 10)\n",
            "X_inputs 5 paths X 2 stocks tensor([[93.0837, 93.6968],\n",
            "        [96.9162, 96.0451],\n",
            "        [95.9635, 96.5389],\n",
            "        [90.9581, 95.0545],\n",
            "        [96.5168, 98.2449],\n",
            "        [96.2604, 91.9720],\n",
            "        [92.4799, 96.2742],\n",
            "        [94.0450, 92.8612],\n",
            "        [95.5543, 98.8281],\n",
            "        [92.1535, 91.8545],\n",
            "        [95.7702, 96.6228],\n",
            "        [95.9526, 95.3796],\n",
            "        [91.6021, 94.9921],\n",
            "        [94.8037, 94.6031],\n",
            "        [93.6590, 93.4806],\n",
            "        [92.0366, 96.6662],\n",
            "        [98.0177, 94.7643],\n",
            "        [98.3155, 96.4981],\n",
            "        [95.7486, 92.1947],\n",
            "        [94.0382, 95.0799]], dtype=torch.float64)\n",
            "<torch.utils.data.sampler.BatchSampler object at 0x7fcd20701d10>\n",
            "batch tensor([[93.0837, 93.6968],\n",
            "        [93.6590, 93.4806],\n",
            "        [92.0366, 96.6662],\n",
            "        [92.1535, 91.8545]], dtype=torch.float64) range(0, 20) [0, 14, 15, 9]\n",
            "batch tensor([[94.0382, 95.0799],\n",
            "        [96.9162, 96.0451],\n",
            "        [95.5543, 98.8281],\n",
            "        [95.7702, 96.6228]], dtype=torch.float64) range(0, 20) [19, 1, 8, 10]\n",
            "batch tensor([[98.3155, 96.4981],\n",
            "        [94.0450, 92.8612],\n",
            "        [95.7486, 92.1947],\n",
            "        [96.2604, 91.9720]], dtype=torch.float64) range(0, 20) [17, 7, 18, 5]\n",
            "batch tensor([[98.0177, 94.7643],\n",
            "        [95.9526, 95.3796],\n",
            "        [96.5168, 98.2449],\n",
            "        [92.4799, 96.2742]], dtype=torch.float64) range(0, 20) [16, 11, 4, 6]\n",
            "batch tensor([[90.9581, 95.0545],\n",
            "        [91.6021, 94.9921],\n",
            "        [95.9635, 96.5389],\n",
            "        [94.8037, 94.6031]], dtype=torch.float64) range(0, 20) [3, 12, 2, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.title(\"Learning rate %f\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "S9miW9O9-uvq",
        "outputId": "504cd1fd-e5c1-4f38-b5f9-bd6d62ebc98c"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-02a6e043a73f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Learning rate %f\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, p in network.named_parameters():\n",
        "    print(name, p.grad)"
      ],
      "metadata": {
        "id": "FlmuGNfQ3lZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders for our datasets; shuffle for training, not for validation\n",
        "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True, num_workers=2)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "iEsMj86FW5Qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Training set has {} instances'.format(len(training_set)))\n",
        "dataiter = iter(training_loader)\n",
        "\n",
        "def train_one_epoch(epoch_index, tb_writer):\n",
        "\n",
        "    final = MaxCall(training_set[:, :, -1])\n",
        "    discounted_next_values = disc_factor * (torch.from_numpy(final).double())\n",
        "\n",
        "    date = 5 #training_set.shape[2] - 2\n",
        "    print(training_set.shape[2], date)\n",
        "    immediate_exercise_value = MaxCall(training_set[:, :, date])\n",
        "    immediate_exercise_value = torch.from_numpy(immediate_exercise_value).double()\n",
        "    X_inputs = torch.from_numpy(training_set[:, :, date]).double()\n",
        "\n",
        "    optimizer = optim.Adam(network.parameters())\n",
        "    \n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "    for i, X_inputs[4] in enumerate(training_loader):\n",
        "\n",
        "        # Zero your gradients for every batch!\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Make predictions for this batch\n",
        "        outputs = network(X_inputs)\n",
        "\n",
        "        # Compute the loss and its gradients\n",
        "        loss = - loss_fn(immediate_exercise_value, discounted_next_values, ones, outputs)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        if i % 1000 == 999:\n",
        "            last_loss = running_loss / 1000 # loss per batch\n",
        "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "            tb_x = epoch_index * len(training_loader) + i + 1\n",
        "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "            running_loss = 0.\n",
        "\n",
        "    return last_loss"
      ],
      "metadata": {
        "id": "waosGoehXkMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
        "epoch_number = 0\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "best_vloss = 1_000_000.\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "    # Make sure gradient tracking is on, and do a pass over the data\n",
        "    network.train(True)\n",
        "    avg_loss = train_one_epoch(epoch_number, writer)\n",
        "\n",
        "    # We don't need gradients on to do reporting\n",
        "    network.train(False)\n",
        "\n",
        "    running_vloss = 0.0\n",
        "    for i, vdata in enumerate(validation_loader):\n",
        "        vinputs, vlabels = vdata\n",
        "        voutputs = network(vinputs)\n",
        "        vloss = loss_fn(voutputs, vlabels)\n",
        "        running_vloss += vloss\n",
        "\n",
        "    avg_vloss = running_vloss / (i + 1)\n",
        "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
        "\n",
        "    # Log the running loss averaged per batch\n",
        "    # for both training and validation\n",
        "    writer.add_scalars('Training vs. Validation Loss',\n",
        "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
        "                    epoch_number + 1)\n",
        "    writer.flush()\n",
        "\n",
        "    # Track best performance, and save the model's state\n",
        "    if avg_vloss < best_vloss:\n",
        "        best_vloss = avg_vloss\n",
        "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
        "        torch.save(network.state_dict(), model_path)\n",
        "\n",
        "    epoch_number += 1"
      ],
      "metadata": {
        "id": "GWbpboJaWhw-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}