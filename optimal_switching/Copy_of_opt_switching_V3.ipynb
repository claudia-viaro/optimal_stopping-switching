{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of opt_switching_V3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNG2sW5S7nEW+woaEXriMql",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudia-viaro/optimal_stopping-switching/blob/main/optimal_switching/Copy_of_opt_switching_V3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we fix the final regime of the process"
      ],
      "metadata": {
        "id": "1sjUHwhxLQUC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem Formulation\n",
        "Let $(\\Omega, \\mathcal{F}, P)$ be a fixed probability space on which an adapted stochastic process is defined $X=(X_t)_{0 \\leq t \\leq T}$ whose natural filtration is $(\\mathcal{F}_t^0 := \\sigma \\{ X_s, s \\leq t \\})_{0 \\leq t \\leq T}$. Let $\\mathbf{F}=(\\mathcal{F}_0)_{0 \\leq t \\leq t}$ be the complete filtration of $(\\mathcal{F}_t^0 := \\sigma \\{ X_s, s \\leq t \\})_{0 \\leq t \\leq T}$. with $P$-null sets of $\\mathcal{F}$.\n",
        "\n",
        "The stochastic process $X$ is $\\mathbb{R}^d$-valued and represents the market price of $d$ financial assets (Bermudan call options) that influence the production of power. Assume $(X^i)_{i=1}^d$ follows a geometric Brownian motion satisfying the SDE:\n",
        "\\begin{equation}\n",
        "dX_t = b_{I_t}X_tdt + \\sigma_{I_t}X_tdW_t\n",
        "\\end{equation}\n",
        "where $W$ is a standard Brownian motion on a filtered probability space $(\\Omega, \\mathcal{F}, \\mathbf{F}=(\\mathcal{F}_t)_{t \\geq 0} P)$ and $I_t$ is the indicator variable of the regimes valued in $\\mathbf{I}_d = \\{1, \\ldots, d \\}$. $b_i \\in \\mathbf{R}$ and $\\sigma_i >0$ are the drift and volatility of the system $X$ once in regime $I_t=i$ at time $t$.\n",
        "\n",
        "We will consider a discrete approximization (Euler schema) with respect to. For $i = 1, \\ldots, d$ we simulate $p$ paths\n",
        "\\begin{equation}\n",
        "X^p_{n,i} = \\exp \\Big\\{ \\sum_{k=0}^n \\big( (b-\\sigma^2_i /2)_{\\mathbf{I}}\\Delta t + \\sigma_{i, \\mathbf{I}} \\sqrt{\\Delta t} \\cdot Z_{k, i}^p \\big)     \\Big\\}\n",
        "\\end{equation}\n",
        "where $\\Delta t = T/N$ and $Z_{k, i}^{p} \\sim \\mathcal{N} (0,1)$.\n",
        "\n"
      ],
      "metadata": {
        "id": "sNSqonJN66zF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "QsjSbJxO9nQV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "np.random.seed(234198)\n",
        "import itertools\n",
        "import random\n",
        "import time\n",
        "import scipy.stats\n",
        "\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as tdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BlackScholes:\n",
        "  def __init__(self, drift, sigma, delta, spot, assets,  paths, periods,\n",
        "         maturity, strike, dividend=0):\n",
        "\n",
        "    self.drift = drift - dividend\n",
        "    self.sigma = sigma\n",
        "    self.delta = delta\n",
        "    self.spot = spot\n",
        "    self.assets = assets\n",
        "    self.paths = paths\n",
        "    self.periods = periods\n",
        "    self.maturity = maturity\n",
        "    self.strike = strike\n",
        "    self.dt = self.maturity / self.periods\n",
        "    self.df = math.exp(-self.drift * self.dt)\n",
        "\n",
        "  def drift_fct(self, x, t):\n",
        "    del t\n",
        "    return self.drift * x\n",
        "\n",
        "  def diffusion_fct(self, x, t, v=0):\n",
        "    del t\n",
        "    return self.sigma * x\n",
        "\n",
        "\n",
        "\n",
        "  def simulate_process(self):\n",
        "    \"\"\"Returns a nparray (nb_paths * assets * nb_dates) with prices.\"\"\"\n",
        "    paths = self.paths\n",
        "    spot_paths = np.empty((self.periods+1, paths, self.assets ))\n",
        "\n",
        "    spot_paths[0, :, :] = self.spot\n",
        "    random_numbers = np.random.normal(0, 1, (self.periods, paths, self.assets ))\n",
        "    dW = random_numbers * np.sqrt(self.dt)\n",
        "    drift = self.drift\n",
        "    r = np.repeat(np.repeat(np.repeat(\n",
        "        np.reshape(drift, (-1, 1, 1)), self.periods, axis=0),\n",
        "        paths, axis=1), self.assets, axis=2)\n",
        "    sig = np.ones((self.periods, paths, self.assets))*self.sigma\n",
        "    \n",
        "    spot_paths[1:, :,  :] = np.repeat(\n",
        "        spot_paths[0:1, :, :], self.periods, axis=0)* np.exp(np.cumsum((r-self.delta) * self.dt - (sig ** 2) * self.dt / 2 + sig * dW, axis=0))\n",
        "\n",
        "    return spot_paths #.reshape(spot_paths.shape[2], spot_paths.shape[0], spot_paths.shape[1])\n",
        "\n",
        "\n",
        "'''\n",
        "PLOT\n",
        "'''\n",
        "\n",
        "def draw_stock_model(stockmodel):\n",
        "    stock_paths = stockmodel\n",
        "\n",
        "    # draw a path\n",
        "    one_path = stock_paths[:, 0, 0]\n",
        "    dates = np.array([i for i in range(len(one_path))])\n",
        "    plt.plot(dates, one_path, label='stock path')\n",
        "    plt.ylabel('Stock price')\n",
        "    plt.ylabel('Time')\n",
        "    plt.legend()\n",
        "    return plt.show()"
      ],
      "metadata": {
        "id": "kRvtM2Fl9qTd"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We employ a neural network to approximate the stopping decision functions $\\{f_n\\}_{n=0}^N$ by constructing a sequence of neural networks of the form $f^{\\theta_n}:\\mathbb{R}^d → \\{0,1\\}$ with parameters $\\theta_n \\in \\mathbb{R}^q$ to approximate $f_n$.\n",
        "\n",
        "\n",
        "In its basic form, a neural network is composed of several layers, and layers are made of nodes. From the picture below, we can observe that a node combines input from the data, $x_{1:n}$, with a set of weights, $w_{1:n}$, that either amplify or dampen that input, thereby assigning significance to inputs with regard to the task the algorithm is trying to learn. $x_{1:n}$ are either the inputs of the overall network if this node is in the first layer or the outputs from the previous layer. Then, the input-weight products are summed, usually with a bias term, and the sum is passed through a node’s so-called activation function $f$, to determine whether and to what extent that signal should progress further through the network to affect the ultimate outcome (depending on the magnitude of each associated weight $w_i$). If the signals passes through, we can say that the neuron has been “activated” and returns an output.\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1rButBJka1QjKsLSrAWgJKrxNCGdntf-K)\n",
        "\n",
        "Generally, NNs comprise multiple node layers through which data is passed, giving rise to what can be referred to as the depth of a neural network. In such networks, each layer of nodes trains on a distinct set of features based on the previous layer’s output. The further we move into the neural net, the more complex the features can be recognized by the nodes, since they aggregate and recombine features from the previous layer.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1b8Hbzn5xahE9jHf5jgIslG3dedkLAHst)\n",
        "\n",
        "The neural network used here takes the form $F^{\\theta}: \\mathbb{R}^d → (0,1)$ for $\\theta \\in \\{\\theta_0, \\ldots, \\theta_N  \\}$, that is the parameters are trained via a neural network that outputs probabilities in the interval $(0,1)$. This is due to the fact that the G-B optimization algorithm is to be applied to a continuous function with respect to $\\theta_n$, which $f^{\\theta_n}$ is not. Hence, the multi-layer, feed-forward neural network takes the form:\n",
        "\n",
        "\\begin{equation}\n",
        "F^{\\theta}= \\psi \\circ a_3^{\\theta} \\circ \\phi_{q_2} \\circ a_2^{\\theta} \\circ \\phi_{q_1} \\circ a_1^{\\theta}\n",
        "\\end{equation}\n",
        "where \n",
        "\n",
        "-  $q_1, q_2$ are the number of nodes in the hidden layers\n",
        "- $a_1^{\\theta} : \\mathbb{R}^d → \\mathbb{R}^{q_1}, a_2^{\\theta}: \\mathbb{R}^{q_1} → \\mathbb{R}^{q_2}$ are linear transformation functions: $a_i^{\\theta}(x)=W_i x + b_i$ with matrices $W_1 \\in \\mathbb{R}^{q_1 \\times d}, W_2 \\in \\mathbb{R}^{q_2 \\times q_1}, W_3 \\in \\mathbb{R}^{q_2 \\times 1}$ and vectors $b_1 \\in \\mathbb{R}^{q_1}, b_2 \\in \\mathbb{R}^{q_2}, b_3 \\in \\mathbb{R}^{1}$.\n",
        "- $\\phi_{q_i}: \\mathbb{R}^{q_i}$ is the ReLU activation function: $\\phi_{q_1}(x_i, \\ldots, x_{q_i})=(x_i^{+}, \\ldots, x_{q_i}^{+})$\n",
        "- $\\psi = \\mathbb{R} → \\mathbb{R}$ is the logistic sigmoid function: $\\psi(x)=1/(1+ e^{-x})$.\n",
        "Between the layers a batch normalization is also added, it takes the output from the previous layer and normalizes it before sending it to the next layer. This has the effect of stabilizing the neural network. \n",
        "\n",
        "The parameters will comprise $\\theta = \\{W_1, W_2,, W_3, b_1, b_2, b_3\\}\\in \\mathbb{R}^q$, where $q=q_1(d+q_2+1)+2q_2+1$. The value of $d$ stands for the dimension, that is the number of assets and will be varied among $d=\\{2,4, 5, 10, 20\\}. \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MbI9kTmBCcAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' NEURAL NETWORK'''\n",
        "\n",
        "class Ftheta_NN(nn.Module):\n",
        "  def __init__(self, assets):\n",
        "    super(Ftheta_NN, self).__init__()\n",
        "    H = assets + 40\n",
        "    self.bn0 = nn.BatchNorm1d(num_features=assets)\n",
        "    self.a1 = nn.Linear(assets, H)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.bn1 = nn.BatchNorm1d(num_features=H)\n",
        "    self.a2 = nn.Linear(H, H)\n",
        "    self.bn2 = nn.BatchNorm1d(num_features=H)\n",
        "    self.a3 = nn.Linear(H, 1)\n",
        "    self.bn3 = nn.BatchNorm1d(num_features=1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, input):\n",
        "    out = self.bn0(input)\n",
        "    out = self.a1(out)\n",
        "\n",
        "    out = self.relu(out)\n",
        "    out = self.bn1(out)\n",
        "\n",
        "    #out = self.a2(out)\n",
        "    \n",
        "    #out = self.relu(out)\n",
        "    #out = self.bn2(out)\n",
        "    out = self.a3(out)\n",
        "    \n",
        "    out = self.sigmoid(out)\n",
        "    return out\n",
        "\n",
        "# set initial weights of a linear layer of the NN with uniform values and bias=0.01 (or choose zero initial weights)\n",
        "def init_weights(m):\n",
        "  if isinstance(m, torch.nn.Linear):\n",
        "    torch.manual_seed(42)\n",
        "    #torch.nn.init.zeros_(m.weight)\n",
        "    torch.nn.init.xavier_uniform_(m.weight)\n",
        "    m.bias.data.fill_(0.01)"
      ],
      "metadata": {
        "id": "p94V2uI-CC86"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class \"Training_network\"\n",
        "\n",
        "The NN is used to approximate the optimal stopping decisions $f_n: \\mathbb{R}^d \\rightarrow \\{0, 1 \\}$, $n = \\{ 1, \\ldots, N-1 \\}$, at each date by a neural network $f^{\\theta}: \\mathbb{R}^d \\rightarrow \\{0, 1 \\}$ with parameter $\\theta \\in \\mathbb{R}^q$. \n",
        "\n",
        "We choose $\\theta_N \\in \\mathbb{R}^q$ such that $f^{\\theta}_N \\equiv 1$ and determine $\\theta_n \\in \\mathbb{R}^q$ for $n \\leq N-1$ by recursion of the form:\n",
        "\n",
        "\\begin{equation}\n",
        "\\tau_{n+1} = \\sum_{m=n+1}^N m f^{\\theta_m}(X_m) \\prod _{j=n+1}^{m-1} (1-f^{\\theta_j}(X_j))\n",
        "\\end{equation}\n",
        "\n",
        "Since $f^{\\theta}$ takes values in $\\{ 0,1 \\}$, hence not appropriate for a gradient-descent optimization method, the neural network includes a layer performing a logistic transformation such that we have a continuous output function $F^{\\theta}: \\mathbb{R}^d \\rightarrow (0,1)$.\n",
        "\n",
        "At each time step, for each epoch we compute $F^{\\theta_n}$ using the $\\theta_n$ from the previous epoch. Then, the parameter $\\theta_n$ is update via backpropagation by the gradient of the loss function (Adam optimization algorithm \\citep{kingma2014adam}), which is specified as:\n",
        "\n",
        "\\begin{equation}\n",
        "    Loss = - \\mathbb{E}[g(n, X_n)F^{\\theta_n}(X_n) + g(\\tau_{n+1}, X_{\\tau_{n+1}})(1-F^{\\theta_n}(X_n))]\n",
        "\\end{equation}\n",
        "\n",
        "The aim is to determine $\\theta_n \\in \\mathbb{R}^q$ so that the negative of the loss function is close to the supremum $\\sup_{\\theta \\in \\mathbb{R}^q}\\mathbb{E}[g(n, X_n)F^{\\theta}(X_n) + g(\\tau_{n+1}, X_{\\tau_{n+1}})(1-F^{\\theta}(X_n))   ]$. \n",
        "\n",
        "Looking at the formula for the loss function, it takes as inputs:\n",
        "- _current payoff:_ payoff of the option computed at time $n$ for all paths if it is exercised at time $n$. this is the value of the option at time $n$ when it is exercised\n",
        "- _future payoff:_ expected value of the future payoff, computed at a stopping time observed in the future ($\\tau+1$), this is the value of the option at time $n$ when it is not exercised (continuation value)\n",
        "\n",
        "The NN takes as inputs:\n",
        "- _stock prices:_ the prices of the stock at time $n$ across multiple paths\n",
        "\n",
        "and as outputs:\n",
        "- values in $\\{ 0,1 \\}$, representing the probability of stopping the process (exercising the option). "
      ],
      "metadata": {
        "id": "1zHY7snQpmQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Training_network(object):\n",
        "\n",
        "  def __init__(self, assets, epochs = 400, batch_size=2000):\n",
        "    self.assets = assets\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = batch_size\n",
        "    self.network = Ftheta_NN(self.assets).double()\n",
        "    self.network.apply(init_weights)\n",
        "\n",
        "\n",
        "  def train_network(self,  stock_values, current_payoff,\n",
        "                    future_payoff):\n",
        "    optimizer = optim.Adam(self.network.parameters())\n",
        "    \n",
        "    # transform data into tensors \n",
        "    future_payoff = torch.from_numpy(future_payoff).double()\n",
        "    current_payoff = torch.from_numpy(current_payoff).double()\n",
        "    X_inputs = torch.from_numpy(stock_values).double()\n",
        "\n",
        "    self.network.train(True)\n",
        "    ones = torch.ones(len(future_payoff))\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(self.epochs):\n",
        "      running_loss = 0.0\n",
        "\n",
        "      for batch in tdata.BatchSampler(\n",
        "              tdata.RandomSampler(range(len(X_inputs)), replacement=False),\n",
        "              batch_size=self.batch_size, drop_last=False):\n",
        "        optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(True):\n",
        "          outputs = self.network.forward(X_inputs[batch]).reshape(-1) \n",
        "          reward = (current_payoff[batch].reshape(-1) * outputs + future_payoff[batch] * (ones[batch] - outputs) )\n",
        "\n",
        "          # compute loss function\n",
        "          loss = -torch.mean(reward)\n",
        "\n",
        "          # compute gradients and backpropagate\n",
        "          loss.backward() \n",
        "\n",
        "          # take a step, updating the parameters  \n",
        "          optimizer.step() \n",
        "\n",
        "          running_loss += loss.item() * self.batch_size\n",
        "      epoch_loss = running_loss /  len(tdata.BatchSampler(\n",
        "              tdata.RandomSampler(range(len(X_inputs)), replacement=False),\n",
        "              batch_size=self.batch_size, drop_last=False).sampler)\n",
        "      losses.append(epoch_loss)\n",
        "\n",
        "\n",
        "    torch.save(self.network.state_dict(), 'checkpoint.pth')# (?)\n",
        "    return outputs, self.network, losses  \n",
        "\n",
        "  def evaluate_network(self, X_inputs):\n",
        "\n",
        "    # load saved optimized parameters\n",
        "    state_dict = torch.load('checkpoint.pth')\n",
        "    \n",
        "    # impose \"evaluation\" mode\n",
        "    self.network.eval()\n",
        "    X_inputs = torch.from_numpy(X_inputs).double()\n",
        "    outputs = self.network(X_inputs)\n",
        "    return outputs.view(X_inputs.size()).detach().numpy(), self.network"
      ],
      "metadata": {
        "id": "ts0bQ6QyCHOk"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class \"Payoff\"\n",
        "This class contains various payoff and costs elements that define the reward. The final profit value is computed for each date and path.\n",
        "\n",
        "### terminal reward\n",
        "The terminal function $\\Gamma_T$ is set to an option payoff function of choice regardless of the regime in which the process is at, in this case we have a Max Call. (other choices can be made as well). The terminal payoff is received at maturity, with no other costs nor payoffs.\n",
        "\\begin{equation}\n",
        "\\Gamma(T) = \\Big(\\max_{d \\in \\{1, \\ldots, D \\}} x^d - K   \\Big)^{+} \\tag{1}\n",
        "\\end{equation}\n",
        "\n",
        "### running reward\n",
        "The function $\\Psi_i = (\\Psi_i(t))_{n \\in \\mathbb{N}}$ represents the running reward received while in mode $q \\in \\mathbb{I}$. \n",
        "\\begin{equation}\n",
        "\\Psi_i(t) = \\Big[\\Big(\\max_{d \\in \\{1, \\ldots, D\\}} x^d - K   \\Big)^{+} \\Big] \\tag{2}\n",
        "\\end{equation}\n",
        "\n",
        "### switching cost\n",
        "The function $\\gamma_{i, j} = (\\gamma_{i, j}(t))_{t \\in \\mathbb{T}}$ with $i,j \\in \\mathbb{I} = \\{0, 1 \\}$ represents the cost for switching from mode $i \\in \\mathbb{I}$ to mode $j \\in \\mathbb{I}$.\n",
        "\\begin{equation} \\tag{3}\n",
        "\\gamma_{0,0} \\equiv \\gamma_{1,1} \\equiv 0 \\\\\n",
        "\\gamma_{0,1}(t) = \\Big(\\max_{d \\in \\{1, \\ldots, D \\}} x^d - K   \\Big)^{0.3} + \\delta  \\;\\;\\;\\;\\; \\delta \\sim \\mathcal{N}(0,1)   \\\\ \n",
        "\\gamma_{1, 0}(t) = - \\Big(\\max_{d \\in \\{1, \\ldots, D \\}} x^d - K   \\Big)^{+} \n",
        "\\end{equation}\n",
        "\n",
        "### the full expression for the profit\n",
        "The entire expression for the value of the process at each time $n$, $g(t, X_t, i)$ can be decomposed in: \n",
        "- $g(T, X_T, i)$ the terminal payoff: \n",
        "  \\begin{equation} \\tag{4}\n",
        "  \\tilde{Y}_{T}^i = \\Gamma \\mathbf{1}_{\\{\\tau = T\\}} \n",
        "  \\end{equation}\n",
        "- $g(t, X_t, i)$ the payoff at any date before maturity, distinguishing wether the process switched ($i \\neq j$) or not ($i = j$):\n",
        "  \\begin{equation}\n",
        "  \\begin{aligned}\n",
        "  \\tilde{Y}_{t}^i &= - \\gamma_{i, j}(\\tau) + \\Psi_i(\\tau) + \\mathbb{E}[\\tilde{Y}_{t+1}^i | \\mathcal{F}_t] \\;\\;\\;\\;\\; \\text{if } i \\neq j \\text{,   for    } t=T-1, \\ldots, 0 \\\\\n",
        "  &= \\Psi_i(\\tau) + \\mathbb{E}[\\tilde{Y}_{t+1}^i | \\mathcal{F}_t]  \\;\\;\\;\\;\\; \\text{if } i = j \\text{,   for    } t=T-1, \\ldots, 0\n",
        "  \\end{aligned}\n",
        "  \\end{equation}\n",
        "\n",
        "\n",
        "# The functions\n",
        "There are two main functions:\n",
        "1. _current payoff:_ it computes the value of the current payoff when there is a switch, considering both alternatives 0-1 and 1-0\n",
        "2. _current payoff trained:_  it computes the value of the current payoff with trained params. it considers the three options: no switch, yes switch 0-1 or 1-0. for a single date, there are multiple paths which can experience these 3 options."
      ],
      "metadata": {
        "id": "foENTegs2ZLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Payoff:\n",
        "  def __init__(self, model):\n",
        "    self.strike = model.strike\n",
        "    self.model = model\n",
        "\n",
        "  def terminal(self, X):\n",
        "    payoff = np.max(X, axis=1) - self.strike\n",
        "    return payoff.clip(0, None)  \n",
        "\n",
        "  def g(self, date,path,X):\n",
        "    max1=np.max(X[date , path , : ]-self.strike)\n",
        "    return np.max(max1,0) \n",
        "\n",
        "\n",
        "  # switch is F_theta_train \n",
        "  def running(self, Y, date, path, S, X, switch, gamma):\n",
        "    val=Y[date+1, path]- gamma  \n",
        "    k = np.array([0.4, 0.7])\n",
        "    r_benefit = self.g(date, path, X)\n",
        "    return val*int(switch[date, path])+r_benefit.numpy()\n",
        "\n",
        "\n",
        "  def current_payoff(self, data, Y, date, regimes, regimepath):\n",
        "    current_p = np.zeros((self.model.paths))\n",
        "    \n",
        "    for m in range(0, self.model.paths - 1):\n",
        "        value = Y[date+1, m]\n",
        "        running_benefit = self.g(date, m, data)      \n",
        "        old_regime = int(regimepath[date +1, m])     \n",
        "        current_regime = int(regimes[~old_regime])\n",
        "\n",
        "        \n",
        "        if (old_regime - current_regime)>0:          #gamma 0-1\n",
        "          gamma = self.g(date, m, data)**0.3+np.random.normal(0,1,1) + value\n",
        "        \n",
        "        else: gamma = - self.g(date, m, data) + value #gamma 1-0  \n",
        "        current_p[m] = gamma + running_benefit\n",
        "        #current_p[m] = gamma + running_benefit\n",
        "            \n",
        "    return current_p\n",
        "\n",
        "  def current_payoff_trained(self, data, Y, date, regimes, regime_path, which): \n",
        "    current_p = np.zeros((self.model.paths))\n",
        "    for m in range(0, self.model.paths - 1):\n",
        "      running_benefit = self.g(date, m, data) \n",
        "      old_regime = int(regime_path[date +1, m])\n",
        "      value = Y[date+1, m]\n",
        "      \n",
        "      if int(which[m])==1:\n",
        "        regime_path[date, m] = regimes[~old_regime]\n",
        "        current_regime = regime_path[date, m]\n",
        "        if (old_regime - current_regime)>0:          \n",
        "          gamma = self.g(date, m, data)**0.3+np.random.normal(0,1,1) + value #gamma 0-1\n",
        "        else: \n",
        "          gamma = - self.g(date, m, data) + value #gamma 1-0  \n",
        "      else:\n",
        "        regime_path[date, m]=int(regimes[~old_regime])\n",
        "        gamma = 0\n",
        "      current_p[m] = gamma + running_benefit\n",
        "    return current_p\n",
        "    "
      ],
      "metadata": {
        "id": "4vdhJjVlyJbF"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class \"Training\"\n",
        "\n",
        "This is a recursion. It starts by initializing:\n",
        "- model, that is the simulated stock prices\n",
        "- payoff class\n",
        "- NN **too early? should it go at every time step instead**\n",
        "\n",
        "Create some matrices to store values that will be accessed to at different time steps\n",
        "- _mods_ records the models at each time step, basically it appends, by date, the networks (not sure if it is enough to say that it stores the optimized parameters)\n",
        "- _loss functions:_ just to plot\n",
        "- _regime path:_ matrix $(n \\times m)$, it records in which regime the process is at (for the same date, there are multiple paths which can be in different regimes)\n",
        "- _Y train:_ vector $(n \\times 1)$, it records the profit, as it is needed at each iteration\n",
        "- _tau dates:_ matrix $(n \\times m)$, it records for each date when there is a stopping time (can only be at such date or higher)\n",
        "- _F theta:_ matrix $(n \\times m)$, it records the outputs of the NN (0,1 values), only used to obtain the tau_dates\n",
        "\n",
        "The backward induction goes as:\n",
        "1. at maturity\n",
        "  - compute the terminal payoff and record it in _Y train_\n",
        "  - no need to compute the stopping times/stopping decision functions because by construction $f_N \\equiv 1$. Hence, set F_theta[N,:]=1 and tau_dates[N,:]=N\n",
        "  - sample a regime for the process at that time (same for all paths)\n",
        "\n",
        "2. before maturity\n",
        "- compute values for training the NN, hence current payoff $g(n, X_n, i)$ and future payoff $g(\\tau_{n+1}, X_{\\tau_{n+1}})$. \n",
        "\n",
        "  To compute the payoff we need: stock values, date, one step ahead Y and the regime of the $n+1$ date because we need to identify which running cost function to use. It is a payoff when there is a switching so old regime and current regime are different.\n",
        "\n",
        "  The future payoff needs as time variable $\\tau_{n+1}$, we get it from the matrix tau_dates, taking value at $n+1$. The future payoff, as it refers to a value at time $\\tau_{n+1}$, needs to be discounted to $n$ **this last two sentences are not done yet**. Instead we are using the payoff at maturity (ok for now)\n",
        "- feed in the values in the function neural_stopping.train_network()\n",
        "- using the outputs returned by this function, we record values in matrices F_theta\n",
        "- compute the \"actual\" current payoff. As we now now if there was or not a switch, we can compute the current payoff accordingly. we need to do this to record the value in Y_train\n",
        "\n",
        "This is repeated until $n=1$"
      ],
      "metadata": {
        "id": "hKLrUKZbyFg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Training:\n",
        "  def __init__(self, model, payoff_function):\n",
        "\n",
        "    self.model = model    \n",
        "    self.neural_stopping = Training_network(self.model.assets, 400) \n",
        "    self.payoff = payoff_function(self.model)\n",
        "    self.disc_factor = np.exp(-self.model.drift*self.model.maturity/self.model.periods) \n",
        "\n",
        "  def value(self):\n",
        "    model = self.model\n",
        "    stock_paths = self.model.simulate_process()   \n",
        "    \n",
        "    # create empty objects to store values\n",
        "    regimes = [0, 1]\n",
        "    regime_path=np.zeros((model.periods+1, model.paths)) # record at which regime we're at at each n\n",
        "    Y_train=np.zeros((model.periods+1, model.paths))\n",
        "    F_theta_train=np.zeros((model.periods+1,model.paths)) # record switching events for each n\n",
        "    tau_dates=np.zeros((model.periods+1,model.paths)) # switching times\n",
        " \n",
        "\n",
        "    mods=[None]*model.periods # record the models of the NN for testing\n",
        "    loss_functions = [None]*model.periods\n",
        "\n",
        "    # AT MATURITY\n",
        "\n",
        "    # payoff of the last date for each path\n",
        "    final_payoff = self.payoff.terminal(stock_paths[-1, :, :])   \n",
        "\n",
        "    # record values\n",
        "    Y_train[model.periods, :]= final_payoff\n",
        "    F_theta_train[model.periods,:]=1 # by def\n",
        "    tau_dates[model.periods,:]=model.periods # by def  \n",
        "    regime_path[model.periods, :] = random.sample(regimes, 1)[0] # sample a regime at maturity\n",
        "\n",
        "    values = final_payoff\n",
        "    print(\"date\", model.periods, \",\", model.paths)\n",
        "\n",
        "    # BEFORE MATURITY\n",
        "    for date in range(stock_paths.shape[0] - 2, 0, -1): \n",
        "      current_payoff = self.payoff.current_payoff(data = stock_paths, \n",
        "                                               Y = Y_train, date = date, \n",
        "                                               regimes = regimes,\n",
        "                                               regimepath = regime_path)\n",
        "\n",
        "      stopping_probability, networks, loss = self.neural_stopping.train_network(stock_paths[date, : , :], \n",
        "                                                    current_payoff, \n",
        "                                                    final_payoff*self.disc_factor)\n",
        "\n",
        "      \n",
        "      \n",
        "      print(\"date\", date, \",\", len([1 for l in stopping_probability if l > 0.5]), \" mean loss \", np.mean(loss))\n",
        "      \n",
        "      F_theta_train[date,:]=(stopping_probability > 0.5)*1.0   # transform stopping probabilities in 0-1 decision\n",
        "      which = stopping_probability > 0.5\n",
        "\n",
        "      # compute the payoff taking into account if the regime switches (because the payoff funciton is different depending on that)\n",
        "      immediate_exercise_value = self.payoff.current_payoff_trained(stock_paths, Y_train, date, regimes, regime_path, which)\n",
        "\n",
        "      # the final value is this \"values\", where we take for each path, either the final payoff if there was no switch, or the current payoff if there was a switch\n",
        "      values[which] = immediate_exercise_value[which]        # when we switch we take the current profit\n",
        "      values[~which] *= self.disc_factor                     # when we don't switch we take final profit discounted \n",
        "      Y_train[date, :] = values # update Y_train\n",
        "\n",
        "      mods[date]=networks \n",
        "      loss_functions[date]=loss    \n",
        "    \n",
        "    return mods, loss_functions\n",
        "\n",
        "\n",
        "    \n",
        "  # not used here   \n",
        "  def stop(self, stock_values, current_payoff,\n",
        "           future_payoff):\n",
        "    \n",
        "    self.neural_stopping.train_network(\n",
        "      stock_values,\n",
        "      current_payoff ,\n",
        "      future_payoff)\n",
        "\n",
        "    inputs = stock_values\n",
        "    stopping_probability , networks   = self.neural_stopping.evaluate_network(inputs)\n",
        "    return stopping_probability , networks  \n"
      ],
      "metadata": {
        "id": "7UvqI1aXKWAL"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simulate paths Y\n",
        "# goal of this phase is to be able to get stopping decisions f_theta_n\n",
        "hyperparam_training = {'drift': 0.2, 'sigma': 0.05, 'delta': 0.1,  'paths':200, 'periods': 11, 'maturity': 3., 'strike' : 100,'assets':2,  'spot':90,}\n",
        "S_train=BlackScholes(**hyperparam_training)\n",
        "\n",
        "\n",
        "pricing = Training(S_train, Payoff)\n",
        "'''\n",
        "arguments are:\n",
        "- path process\n",
        "- Profit testing classes\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "mods, functions = pricing.value()\n"
      ],
      "metadata": {
        "id": "sp-KE2XpKsKY",
        "outputId": "f4b78be4-cec3-4f16-b0c6-22f6b4115f36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date 11 , 200\n",
            "date 10 , 183  mean loss  -269.1956015536632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: RuntimeWarning: invalid value encountered in double_scalars\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date 9 , 0  mean loss  nan\n",
            "date 8 , 0  mean loss  nan\n",
            "date 7 , 0  mean loss  nan\n",
            "date 6 , 0  mean loss  nan\n",
            "date 5 , 0  mean loss  nan\n",
            "date 4 , 0  mean loss  nan\n",
            "date 3 , 0  mean loss  nan\n",
            "date 2 , 0  mean loss  nan\n",
            "date 1 , 0  mean loss  nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_list = list(filter(None, functions))\n",
        "legend = [\"n = 1\", \"n = 2\", \"n = 3\", \"n = 4\", \"n = 5\", \"n = 6\", \"n = 7\", \"n = 8\", \"n=9\", \"n=10\"]\n",
        "\n",
        "for i in range(len(filtered_list)):\n",
        "  epochs = np.array([i for i in range(len(filtered_list[0]))])\n",
        "  plt.plot(epochs, filtered_list[i], label='loss funciton')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epochs')\n",
        "  plt.legend(legend)\n",
        "  plt.title('Loss curves across time periods')\n",
        "  plt.plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "yVXeiEjqrfRf",
        "outputId": "6f36dfd9-e7f2-448b-b293-d4f61bb348c5"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lower bound\n",
        "\n",
        "the stopping time $\\tau^{\\Theta}$ gives a lower bound $L=\\mathbb{E}g(\\tau^{\\Theta}, X_{\\tau^{\\Theta}})$ for the optimal value $V_0= \\sup_{\\tau \\in \\mathcal{T}}\\mathbb{E}g(\\tau, X_{\\tau})$.\n",
        "\n",
        "Simulate \n",
        "- $K_L = 1024$ paths $(y_n^k)_{n=0}^N$, $k=1, \\ldots, K_L$, of $(X_n)_{n=0}^N$ and assume these are drawn independently from the realizations $(x_n^k)_{n=0}^N$, $k=1, \\ldots, K$.\n",
        "\n",
        "The unbiased estimate of the lower bound $L$ is given by\n",
        "\\begin{equation}\n",
        "\\hat{L}=\\frac{1}{K_L} \\sum_{k=1}^{K_L} g(l^k, y_{l^k}^k)\n",
        "\\end{equation}\n",
        "where $l^k = l(y_0^k, \\ldots, y_{N-1}^k)$"
      ],
      "metadata": {
        "id": "k8JSTFLsNMD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing phase - Lower bound\n",
        "\n",
        "# sample y from the process (Y)\n",
        "hyperparam_testing_L = {'drift': 0.2, 'sigma': 0.05, 'delta': 0.1,  'paths':5000, 'periods': 9, 'maturity': 3., 'strike' : 100,'assets':20,  'spot':110,}\n",
        "S_test_L=BlackScholes(**hyperparam_testing_L)\n",
        "\n",
        "# now we can compute all the stopping times recursively"
      ],
      "metadata": {
        "id": "eBdQnHLJNOMj"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "\n",
        "class Testing_Lower:\n",
        "  def __init__(self, model, training, testing, mods):   \n",
        "    self.model = model # argument is S   \n",
        "    self.neural_stopping = Training_network(model.assets, 400) \n",
        "    self.profit_testing = Profit_testing(self.model)\n",
        "    self.mods = mods\n",
        "\n",
        "  def price(self):\n",
        "    model = self.model\n",
        "    disc_factor = np.math.exp((-model.drift) * model.maturity/(model.periods))\n",
        "    stock_paths = self.model.simulate_process()\n",
        "    k = np.array([0.4, 0.7])\n",
        "    regimes = [0, 1]\n",
        "    regime_path=np.zeros((model.periods+1, model.paths)) # record at which regime we're at at each n\n",
        "    Y_train=np.zeros((model.periods+1, model.paths))\n",
        "    F_theta_train=np.zeros((model.periods+1,model.paths)) # record switching events for each n\n",
        " \n",
        "    # at maturity N\n",
        "    final_payoff = np.array([self.profit_training.terminal(stock_paths[-1, :, :]), self.profit_training.terminal(stock_paths[-1, :, :])])   # payoff of the last date for each path.\n",
        "    future_payoff = torch.from_numpy(final_payoff*(np.math.exp((-model.drift) * model.periods))).double() \n",
        "    Y_train[model.periods, :]= final_payoff[0]\n",
        "    F_theta_train[model.periods,:]=1 # at maturity we switch (does it matter?)\n",
        "    regime_path[model.periods, :] = random.sample(regimes, 1)[0] # sample a regime at maturity\n",
        "    values = Y_train[model.periods, :]\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    # recursive calc. before maturity\n",
        "         \n",
        "    for date in range(stock_paths.shape[0] - 2, 0, -1):\n",
        "      current_payoff = self.profit_training.running(Y_train[date+1, :], stock_paths[date, :, :])\n",
        "      mod_curr=self.mods[date]\n",
        "      probs=mod_curr(torch.from_numpy(stock_paths[date])) \n",
        "      np_probs=probs.detach().numpy().reshape(self.model.paths)\n",
        "      \n",
        "      F_theta_train[date,:]=(np_probs > 0.5)*1.0   # transform stopping probabilities in 0-1 decision\n",
        "      which = np_probs > 0.5\n",
        "\n",
        "      for m in range(0,model.paths-1):\n",
        "        old_regime = regime_path[date +1, m]\n",
        "        regime_path[date, m] = int(which[m])\n",
        "        if which[m] == True:\n",
        "          if int(old_regime) - int(which[m])>0:  #gamma 0-1\n",
        "            gamma = self.profit_testing.g(date, m, stock_paths)+0.7\n",
        "          else: gamma = -self.profit_testing.g(date, m, stock_paths) #gamma 1-0  \n",
        "        else:\n",
        "          gamma = 0 \n",
        "        Y_train[date, m] = Y_train[date+1, m]- gamma\n",
        "\n",
        "\n",
        "      immediate_exercise_value = Y_train[date, :]       \n",
        "      values[which] = immediate_exercise_value[which] # when we switch we take the current profit\n",
        "      values[~which] *= ((model.periods-date)/model.periods)           # when we don't switch we take final profit discounted \n",
        "\n",
        "      #Y_train[date, :] = values\n",
        "      print(\"date\", date, round(np.mean(values), 3), len([1 for l in np_probs if l > 0.5]))\n",
        "\n",
        "    \n",
        "    return round(np.mean(values), 3), Y_train\n",
        "\n"
      ],
      "metadata": {
        "id": "PpoI_K_MNO6b"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "dict ={}\n",
        " \n",
        "# Insert data into dictionary\n",
        "dict1 = {\n",
        "     1: [\"2\", 90, 97.339, 0.009],\n",
        "     2: [\"2\", 100, 205.426, 0.006],\n",
        "     3: [\"2\", 110, 315.878, 0.007],\n",
        "     7: [\"4\", 90, 130.082, 0.008],\n",
        "     8: [\"4\", 100, 235.951, 0.008],\n",
        "     9: [\"4\", 110, 334.079, 0.005],\n",
        "     10: [\"5\", 90, 134.486, 0.008],\n",
        "     11: [\"5\", 100, 224.051, 0.006],\n",
        "     12: [\"5\", 110, 282.737, 0.006],\n",
        "     13: [\"10\", 90, 158.875, 0.005],\n",
        "     14: [\"10\", 100, 273.452, 0.008],\n",
        "     15: [\"10\", 110, 391.043, 0.015],\n",
        "     16: [\"20\", 90, 100.447, 0.008],\n",
        "     17: [\"20\", 100, 192.448, 0.01],\n",
        "     18: [\"20\", 110, 301.107, 0.009],\n",
        "     }\n",
        " \n",
        "# Print the names of the columns.\n",
        "print (\"{:<10} {:<10} {:<10} {:<10}\".format('assets', 'spot', 'L', 'timeL'))\n",
        " \n",
        "# print each data item.\n",
        "for key, value in dict1.items():\n",
        "    assets, spot, L, timeL = value\n",
        "    print (\"{:<10} {:<10} {:<10} {:<10}\".format(assets, spot, L, timeL))"
      ],
      "metadata": {
        "id": "qsGOwzwDvUCX",
        "outputId": "11c4deb5-6263-461f-8e3b-87c84e58f9c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assets     spot       L          timeL     \n",
            "2          90         97.339     0.009     \n",
            "2          100        205.426    0.006     \n",
            "2          110        315.878    0.007     \n",
            "4          90         130.082    0.008     \n",
            "4          100        235.951    0.008     \n",
            "4          110        334.079    0.005     \n",
            "5          90         134.486    0.008     \n",
            "5          100        224.051    0.006     \n",
            "5          110        282.737    0.006     \n",
            "10         90         158.875    0.005     \n",
            "10         100        273.452    0.008     \n",
            "10         110        391.043    0.015     \n",
            "20         90         100.447    0.008     \n",
            "20         100        192.448    0.01      \n",
            "20         110        301.107    0.009     \n"
          ]
        }
      ]
    }
  ]
}