{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "opt_switching_V3",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNOkDBausyHGveFjoQkXitY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudia-viaro/optimal_stopping-switching/blob/main/optimal_switching/opt_switching_V3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we fix the final regime of the process"
      ],
      "metadata": {
        "id": "1sjUHwhxLQUC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem Formulation\n",
        "Let $(\\Omega, \\mathcal{F}, P)$ be a fixed probability space on which an adapted stochastic process is defined $X=(X_t)_{0 \\leq t \\leq T}$ whose natural filtration is $(\\mathcal{F}_t^0 := \\sigma \\{ X_s, s \\leq t \\})_{0 \\leq t \\leq T}$. Let $\\mathbf{F}=(\\mathcal{F}_0)_{0 \\leq t \\leq t}$ be the complete filtration of $(\\mathcal{F}_t^0 := \\sigma \\{ X_s, s \\leq t \\})_{0 \\leq t \\leq T}$. with $P$-null sets of $\\mathcal{F}$.\n",
        "\n",
        "The stochastic process $X$ is $\\mathbb{R}^d$-valued and represents the market price of $d$ financial assets (Bermudan call options) that influence the production of power. Assume $(X^i)_{i=1}^d$ follows a geometric Brownian motion satisfying the SDE:\n",
        "\\begin{equation}\n",
        "dX_t = b_{I_t}X_tdt + \\sigma_{I_t}X_tdW_t\n",
        "\\end{equation}\n",
        "where $W$ is a standard Brownian motion on a filtered probability space $(\\Omega, \\mathcal{F}, \\mathbf{F}=(\\mathcal{F}_t)_{t \\geq 0} P)$ and $I_t$ is the indicator variable of the regimes valued in $\\mathbf{I}_d = \\{1, \\ldots, d \\}$. $b_i \\in \\mathbf{R}$ and $\\sigma_i >0$ are the drift and volatility of the system $X$ once in regime $I_t=i$ at time $t$.\n",
        "\n",
        "We will consider a discrete approximization (Euler schema) with respect to. For $i = 1, \\ldots, d$ we simulate $p$ paths\n",
        "\\begin{equation}\n",
        "X^p_{n,i} = \\exp \\Big\\{ \\sum_{k=0}^n \\big( (b-\\sigma^2_i /2)_{\\mathbf{I}}\\Delta t + \\sigma_{i, \\mathbf{I}} \\sqrt{\\Delta t} \\cdot Z_{k, i}^p \\big)     \\Big\\}\n",
        "\\end{equation}\n",
        "where $\\Delta t = T/N$ and $Z_{k, i}^{p} \\sim \\mathcal{N} (0,1)$.\n",
        "\n"
      ],
      "metadata": {
        "id": "sNSqonJN66zF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "QsjSbJxO9nQV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "np.random.seed(234198)\n",
        "import itertools\n",
        "import random\n",
        "import time\n",
        "import scipy.stats\n",
        "\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as tdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BlackScholes:\n",
        "  def __init__(self, drift, sigma, delta, spot, assets,  paths, periods,\n",
        "         maturity, strike, dividend=0):\n",
        "\n",
        "    self.drift = drift - dividend\n",
        "    self.sigma = sigma\n",
        "    self.delta = delta\n",
        "    self.spot = spot\n",
        "    self.assets = assets\n",
        "    self.paths = paths\n",
        "    self.periods = periods\n",
        "    self.maturity = maturity\n",
        "    self.strike = strike\n",
        "    self.dt = self.maturity / self.periods\n",
        "    self.df = math.exp(-self.drift * self.dt)\n",
        "\n",
        "  def drift_fct(self, x, t):\n",
        "    del t\n",
        "    return self.drift * x\n",
        "\n",
        "  def diffusion_fct(self, x, t, v=0):\n",
        "    del t\n",
        "    return self.sigma * x\n",
        "\n",
        "\n",
        "\n",
        "  def simulate_process(self):\n",
        "    \"\"\"Returns a nparray (nb_paths * assets * nb_dates) with prices.\"\"\"\n",
        "    paths = self.paths\n",
        "    spot_paths = np.empty((self.periods+1, paths, self.assets ))\n",
        "\n",
        "    spot_paths[0, :, :] = self.spot\n",
        "    random_numbers = np.random.normal(0, 1, (self.periods, paths, self.assets ))\n",
        "    dW = random_numbers * np.sqrt(self.dt)\n",
        "    drift = self.drift\n",
        "    r = np.repeat(np.repeat(np.repeat(\n",
        "        np.reshape(drift, (-1, 1, 1)), self.periods, axis=0),\n",
        "        paths, axis=1), self.assets, axis=2)\n",
        "    sig = np.ones((self.periods, paths, self.assets))*self.sigma\n",
        "    \n",
        "    spot_paths[1:, :,  :] = np.repeat(\n",
        "        spot_paths[0:1, :, :], self.periods, axis=0)* np.exp(np.cumsum((r-self.delta) * self.dt - (sig ** 2) * self.dt / 2 + sig * dW, axis=0))\n",
        "\n",
        "    return spot_paths #.reshape(spot_paths.shape[2], spot_paths.shape[0], spot_paths.shape[1])\n",
        "\n",
        "\n",
        "'''\n",
        "PLOT\n",
        "'''\n",
        "\n",
        "def draw_stock_model(stockmodel):\n",
        "    stock_paths = stockmodel\n",
        "\n",
        "    # draw a path\n",
        "    one_path = stock_paths[:, 0, 0]\n",
        "    dates = np.array([i for i in range(len(one_path))])\n",
        "    plt.plot(dates, one_path, label='stock path')\n",
        "    plt.ylabel('Stock price')\n",
        "    plt.ylabel('Time')\n",
        "    plt.legend()\n",
        "    return plt.show()"
      ],
      "metadata": {
        "id": "kRvtM2Fl9qTd"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We employ a neural network to approximate the stopping decision functions $\\{f_n\\}_{n=0}^N$ by constructing a sequence of neural networks of the form $f^{\\theta_n}:\\mathbb{R}^d → \\{0,1\\}$ with parameters $\\theta_n \\in \\mathbb{R}^q$ to approximate $f_n$.\n",
        "\n",
        "\n",
        "In its basic form, a neural network is composed of several layers, and layers are made of nodes. From the picture below, we can observe that a node combines input from the data, $x_{1:n}$, with a set of weights, $w_{1:n}$, that either amplify or dampen that input, thereby assigning significance to inputs with regard to the task the algorithm is trying to learn. $x_{1:n}$ are either the inputs of the overall network if this node is in the first layer or the outputs from the previous layer. Then, the input-weight products are summed, usually with a bias term, and the sum is passed through a node’s so-called activation function $f$, to determine whether and to what extent that signal should progress further through the network to affect the ultimate outcome (depending on the magnitude of each associated weight $w_i$). If the signals passes through, we can say that the neuron has been “activated” and returns an output.\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1rButBJka1QjKsLSrAWgJKrxNCGdntf-K)\n",
        "\n",
        "Generally, NNs comprise multiple node layers through which data is passed, giving rise to what can be referred to as the depth of a neural network. In such networks, each layer of nodes trains on a distinct set of features based on the previous layer’s output. The further we move into the neural net, the more complex the features can be recognized by the nodes, since they aggregate and recombine features from the previous layer.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1b8Hbzn5xahE9jHf5jgIslG3dedkLAHst)\n",
        "\n",
        "The neural network used here takes the form $F^{\\theta}: \\mathbb{R}^d → (0,1)$ for $\\theta \\in \\{\\theta_0, \\ldots, \\theta_N  \\}$, that is the parameters are trained via a neural network that outputs probabilities in the interval $(0,1)$. This is due to the fact that the G-B optimization algorithm is to be applied to a continuous function with respect to $\\theta_n$, which $f^{\\theta_n}$ is not. Hence, the multi-layer, feed-forward neural network takes the form:\n",
        "\n",
        "\\begin{equation}\n",
        "F^{\\theta}= \\psi \\circ a_3^{\\theta} \\circ \\phi_{q_2} \\circ a_2^{\\theta} \\circ \\phi_{q_1} \\circ a_1^{\\theta}\n",
        "\\end{equation}\n",
        "where \n",
        "\n",
        "-  $q_1, q_2$ are the number of nodes in the hidden layers\n",
        "- $a_1^{\\theta} : \\mathbb{R}^d → \\mathbb{R}^{q_1}, a_2^{\\theta}: \\mathbb{R}^{q_1} → \\mathbb{R}^{q_2}$ are linear transformation functions: $a_i^{\\theta}(x)=W_i x + b_i$ with matrices $W_1 \\in \\mathbb{R}^{q_1 \\times d}, W_2 \\in \\mathbb{R}^{q_2 \\times q_1}, W_3 \\in \\mathbb{R}^{q_2 \\times 1}$ and vectors $b_1 \\in \\mathbb{R}^{q_1}, b_2 \\in \\mathbb{R}^{q_2}, b_3 \\in \\mathbb{R}^{1}$.\n",
        "- $\\phi_{q_i}: \\mathbb{R}^{q_i}$ is the ReLU activation function: $\\phi_{q_1}(x_i, \\ldots, x_{q_i})=(x_i^{+}, \\ldots, x_{q_i}^{+})$\n",
        "- $\\psi = \\mathbb{R} → \\mathbb{R}$ is the logistic sigmoid function: $\\psi(x)=1/(1+ e^{-x})$.\n",
        "Between the layers a batch normalization is also added, it takes the output from the previous layer and normalizes it before sending it to the next layer. This has the effect of stabilizing the neural network. \n",
        "\n",
        "The parameters will comprise $\\theta = \\{W_1, W_2,, W_3, b_1, b_2, b_3\\}\\in \\mathbb{R}^q$, where $q=q_1(d+q_2+1)+2q_2+1$. The value of $d$ stands for the dimension, that is the number of assets and will be varied among $d=\\{2,4, 5, 10, 20\\}. \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MbI9kTmBCcAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' NEURAL NETWORK'''\n",
        "\n",
        "class Ftheta_NN(nn.Module):\n",
        "  def __init__(self, assets):\n",
        "    super(Ftheta_NN, self).__init__()\n",
        "    H = assets + 40\n",
        "    self.bn0 = nn.BatchNorm1d(num_features=assets)\n",
        "    self.a1 = nn.Linear(assets, H)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.bn1 = nn.BatchNorm1d(num_features=H)\n",
        "    self.a2 = nn.Linear(H, H)\n",
        "    self.bn2 = nn.BatchNorm1d(num_features=H)\n",
        "    self.a3 = nn.Linear(H, 1)\n",
        "    self.bn3 = nn.BatchNorm1d(num_features=1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, input):\n",
        "    out = self.bn0(input)\n",
        "    out = self.a1(out)\n",
        "\n",
        "    out = self.relu(out)\n",
        "    out = self.bn1(out)\n",
        "\n",
        "    #out = self.a2(out)\n",
        "    \n",
        "    #out = self.relu(out)\n",
        "    #out = self.bn2(out)\n",
        "    out = self.a3(out)\n",
        "    \n",
        "    out = self.sigmoid(out)\n",
        "    return out\n",
        "\n",
        "# set initial weights of a linear layer of the NN with uniform values and bias=0.01 (or choose zero initial weights)\n",
        "def init_weights(m):\n",
        "  if isinstance(m, torch.nn.Linear):\n",
        "    torch.manual_seed(42)\n",
        "    #torch.nn.init.zeros_(m.weight)\n",
        "    torch.nn.init.xavier_uniform_(m.weight)\n",
        "    m.bias.data.fill_(0.01)"
      ],
      "metadata": {
        "id": "p94V2uI-CC86"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class \"Training_network\"\n",
        "\n",
        "The NN is used to approximate the optimal stopping decisions $f_n: \\mathbb{R}^d \\rightarrow \\{0, 1 \\}$, $n = \\{ 1, \\ldots, N-1 \\}$, at each date by a neural network $f^{\\theta}: \\mathbb{R}^d \\rightarrow \\{0, 1 \\}$ with parameter $\\theta \\in \\mathbb{R}^q$. \n",
        "\n",
        "We choose $\\theta_N \\in \\mathbb{R}^q$ such that $f^{\\theta}_N \\equiv 1$ and determine $\\theta_n \\in \\mathbb{R}^q$ for $n \\leq N-1$ by recursion of the form:\n",
        "\n",
        "\\begin{equation}\n",
        "\\tau_{n+1} = \\sum_{m=n+1}^N m f^{\\theta_m}(X_m) \\prod _{j=n+1}^{m-1} (1-f^{\\theta_j}(X_j))\n",
        "\\end{equation}\n",
        "\n",
        "Since $f^{\\theta}$ takes values in $\\{ 0,1 \\}$, hence not appropriate for a gradient-descent optimization method, the neural network includes a layer performing a logistic transformation such that we have a continuous output function $F^{\\theta}: \\mathbb{R}^d \\rightarrow (0,1)$.\n",
        "\n",
        "At each time step, for each epoch we compute $F^{\\theta_n}$ using the $\\theta_n$ from the previous epoch. Then, the parameter $\\theta_n$ is update via backpropagation by the gradient of the loss function (Adam optimization algorithm \\citep{kingma2014adam}), which is specified as:\n",
        "\n",
        "\\begin{equation}\n",
        "    Loss = - \\mathbb{E}[g(n, X_n)F^{\\theta_n}(X_n) + g(\\tau_{n+1}, X_{\\tau_{n+1}})(1-F^{\\theta_n}(X_n))]\n",
        "\\end{equation}\n",
        "\n",
        "The aim is to determine $\\theta_n \\in \\mathbb{R}^q$ so that the negative of the loss function is close to the supremum $\\sup_{\\theta \\in \\mathbb{R}^q}\\mathbb{E}[g(n, X_n)F^{\\theta}(X_n) + g(\\tau_{n+1}, X_{\\tau_{n+1}})(1-F^{\\theta}(X_n))   ]$. \n",
        "\n",
        "Looking at the formula for the loss function, it takes as inputs:\n",
        "- _current payoff:_ payoff of the option computed at time $n$ for all paths if it is exercised at time $n$. this is the value of the option at time $n$ when it is exercised\n",
        "- _future payoff:_ expected value of the future payoff, computed at a stopping time observed in the future ($\\tau+1$), this is the value of the option at time $n$ when it is not exercised (continuation value)\n",
        "\n",
        "The NN takes as inputs:\n",
        "- _stock prices:_ the prices of the stock at time $n$ across multiple paths\n",
        "\n",
        "and as outputs:\n",
        "- values in $\\{ 0,1 \\}$, representing the probability of stopping the process (exercising the option). "
      ],
      "metadata": {
        "id": "1zHY7snQpmQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Training_network(object):\n",
        "\n",
        "  def __init__(self, assets, epochs = 400, batch_size=2000):\n",
        "    self.assets = assets\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = batch_size\n",
        "    self.network = Ftheta_NN(self.assets).double()\n",
        "    self.network.apply(init_weights) # move later (?), not sure where it makes more sense to init params\n",
        "\n",
        "\n",
        "  def train_network(self,  stock_values, current_payoff,\n",
        "                    future_payoff):\n",
        "\n",
        "    #self.network.apply(init_weights) # not sure where it makes more sense to init params\n",
        "\n",
        "    # set values for the loss\n",
        "    # current_payoff, future_payoff are already tensors\n",
        "    ones = torch.ones(len(future_payoff)) # we need a vector of 1's in the loss function\n",
        "\n",
        "    # set values for the NN inputs (stock_values) \n",
        "    X_inputs = torch.from_numpy(stock_values).double() \n",
        "\n",
        "\n",
        "    # several optimization methods are available (here Adam algorithm). as argument input the parameters to be optimized    \n",
        "    # try diff learning rates\n",
        "    optimizer = optim.Adam(self.network.parameters())\n",
        "    self.network.train(True) # set training mode ON    \n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(self.epochs):\n",
        "      running_loss = 0.0\n",
        "\n",
        "      for batch in tdata.BatchSampler(\n",
        "              tdata.RandomSampler(range(len(X_inputs)), replacement=False),\n",
        "              batch_size=self.batch_size, drop_last=False):\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(True):\n",
        "          outputs = self.network.forward(X_inputs[batch]).reshape(-1) \n",
        "          reward = (current_payoff[batch].reshape(-1) * outputs + future_payoff[batch] * (ones[batch] - outputs) )\n",
        "\n",
        "          # compute loss function\n",
        "          loss = -torch.mean(reward)\n",
        "\n",
        "          # compute gradients and backpropagate\n",
        "          loss.backward() \n",
        "\n",
        "          # take a step, updating the parameters  \n",
        "          optimizer.step() \n",
        "\n",
        "          running_loss += loss.item() * self.batch_size\n",
        "      epoch_loss = running_loss /  len(tdata.BatchSampler(\n",
        "              tdata.RandomSampler(range(len(X_inputs)), replacement=False),\n",
        "              batch_size=self.batch_size, drop_last=False).sampler)\n",
        "      losses.append(epoch_loss*(-1))\n",
        "\n",
        "\n",
        "    torch.save(self.network.state_dict(), 'checkpoint.pth')# (?)\n",
        "    return outputs, self.network, losses  "
      ],
      "metadata": {
        "id": "ts0bQ6QyCHOk"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class \"Payoff\"\n",
        "This class contains various payoff and costs elements that define the reward. The final profit value is computed for each date and path.\n",
        "\n",
        "### terminal reward\n",
        "The terminal function $\\Gamma_T$ is set to an option payoff function of choice regardless of the regime in which the process is at, in this case we have a Max Call. (other choices can be made as well). The terminal payoff is received at maturity, with no other costs nor payoffs.\n",
        "\\begin{equation}\n",
        "\\Gamma(T) = \\Big(\\max_{d \\in \\{1, \\ldots, D \\}} x^d - K   \\Big)^{+} \\tag{1}\n",
        "\\end{equation}\n",
        "\n",
        "### running reward\n",
        "The function $\\Psi_i = (\\Psi_i(t))_{n \\in \\mathbb{N}}$ represents the running reward received while in mode $i \\in \\mathbb{I}$. \n",
        "\\begin{equation}\n",
        "\\Psi_i(t) = \\Big[\\Big(\\max_{d \\in \\{1, \\ldots, D\\}} x^d - K   \\Big)^{+} \\Big] \\tag{2}\n",
        "\\end{equation}\n",
        "\n",
        "### switching cost\n",
        "The function $\\gamma_{i, j} = (\\gamma_{i, j}(t))_{t \\in \\mathbb{T}}$ with $i,j \\in \\mathbb{I} = \\{0, 1 \\}$ represents the cost for switching from mode $i \\in \\mathbb{I}$ to mode $j \\in \\mathbb{I}$.\n",
        "\\begin{equation} \\tag{3}\n",
        "\\begin{aligned}\n",
        "&\\gamma_{0,0} \\equiv \\gamma_{1,1} \\equiv 0 \\\\\n",
        "&\\gamma_{0,1}(t) = \\Big(\\Big(\\max_{d \\in \\{1, \\ldots, D \\}} x^d - K   \\Big)^{+}\\Big)^{0.3} + \\delta  \\;\\;\\;\\;\\; \\delta \\sim \\mathcal{N}(0,1)   \\\\ \n",
        "&\\gamma_{1, 0}(t) = - \\Big(\\max_{d \\in \\{1, \\ldots, D \\}} x^d - K   \\Big)^{+} \n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "### the full expression for the profit\n",
        "The entire expression for the value of the process at each time $n$, $g(t, X_t, i)$ can be decomposed in: \n",
        "- $g(T, X_T, i)$ the terminal payoff: \n",
        "  \\begin{equation} \\tag{4}\n",
        "  \\tilde{Y}_{T}^i = \\Gamma \\mathbf{1}_{\\{\\tau = T\\}} \n",
        "  \\end{equation}\n",
        "- $g(t, X_t, i)$ the payoff at any date before maturity, distinguishing whether the process switched ($i \\neq j$) or not ($i = j$):\n",
        "  \\begin{equation}\n",
        "  \\begin{aligned}\n",
        "  \\tilde{Y}_{t}^i &= - \\gamma_{i, j}(\\tau) + \\Psi_i(\\tau) + \\mathbb{E}[\\tilde{Y}_{t+1}^i | \\mathcal{F}_t] \\;\\;\\;\\;\\; \\text{if } i \\neq j \\text{,   for    } t \\leq T-1 \\\\\n",
        "  &= \\Psi_i(\\tau) + \\mathbb{E}[\\tilde{Y}_{t+1}^i | \\mathcal{F}_t]  \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\text{if } i = j \\text{,   for    } t \\leq T-1\n",
        "  \\end{aligned}\n",
        "  \\end{equation}\n",
        "\n",
        "\n",
        "# The functions\n",
        "There are two main functions:\n",
        "1. _current value:_ it computes the value of the current payoff when there is a switch, considering both alternatives 0-1 and 1-0\n",
        "2. _continuation value:_  it computes the value of the current payoff when there is not a switch, hence the discounted expected value of the process when switched at a future switching time"
      ],
      "metadata": {
        "id": "foENTegs2ZLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Payoff:\n",
        "  def __init__(self, model):\n",
        "    self.strike = model.strike\n",
        "    self.model = model \n",
        "  \n",
        "  def terminal_(self, n, X): # already get tensor\n",
        "    a = torch.tensor(())\n",
        "    for m in range(0, self.model.paths):\n",
        "      date = n[m]\n",
        "      tensorX= torch.from_numpy(X[int(date),m,:])\n",
        "      max1=torch.max(tensorX-100)  \n",
        "      max2 = torch.max(max1, torch.tensor([0.0])).float() \n",
        "      a = torch.cat((a, max2), 0)\n",
        "    return a\n",
        "\n",
        "  def running(self, X):\n",
        "    # X is X[int(date),m,:]\n",
        "    tensorX= torch.from_numpy(X)\n",
        "    max1=torch.max(tensorX-100) \n",
        "    max2 = torch.max(max1, torch.tensor([0.0])).float() \n",
        "    #i = torch.tensor([max1]).float()\n",
        "    return max2 \n",
        "  \n",
        "  def continuation_payoff(self, data, Y, t):\n",
        "    # t is tau plus one\n",
        "    # data is stock_paths\n",
        "    a = torch.tensor(())\n",
        "    for m in range(0, self.model.paths):\n",
        "      date1 = t[m]\n",
        "      value = Y[int(date1), m] # no need of plus one as it is already like that\n",
        "      running_benefit = self.running(data[int(date1), m, :])      \n",
        "      i = torch.tensor([value + running_benefit]).float()\n",
        "      a = torch.cat((a, i), 0)         \n",
        "    return a\n",
        "  \n",
        "  def payoff_Nswitch(self, X, tau, Y, date):\n",
        "    a = np.array([])\n",
        "    for m in range(0, self.model.paths):\n",
        "      date1 = tau[m]\n",
        "      value = Y[int(date1), m] # no need of plus one as it is already like that\n",
        "      runn1 = self.running(X[int(date1), m, :])      \n",
        "      i = (value + runn1.numpy())*np.exp(-self.model.drift*self.model.dt*(date1-date))\n",
        "      a = np.concatenate((a, i), 0)\n",
        "    return a  \n",
        "\n",
        "  def payoff_Yswitch(self, X, date, Y, regimepath, regimes):\n",
        "    a = np.array([])\n",
        "    for m in range(0, self.model.paths):\n",
        "      old_regime = int(regimepath[date +1, m])\n",
        "      regimepath[date, m] = regimes[~old_regime]\n",
        "      current_regime = regimepath[date, m]\n",
        "      Yvalue = Y[date+1, m]\n",
        "      runn = (self.running(X[int(date), m, :])).numpy()\n",
        "      if (old_regime - current_regime)>0:  \n",
        "        gamma = self.running(X[date, m, :])**0.3+np.random.normal(0,1,1) #gamma 0-1\n",
        "        i = gamma.numpy() + Yvalue + runn\n",
        "        a = np.concatenate((a, i), 0)\n",
        "      else: \n",
        "        gamma = - self.running(X[date, m, :])  #gamma 1-0  \n",
        "        i = gamma.numpy() + Yvalue + runn\n",
        "        a = np.concatenate((a, i), 0)\n",
        "    return a   \n",
        "\n",
        "  def current_payoff(self, data, Y, date, regimes, regimepath):\n",
        "    a = torch.tensor(())     \n",
        "    for m in range(0, self.model.paths):\n",
        "        value = Y[date+1, m]\n",
        "        running_benefit = self.running(data[date, m, :])    \n",
        "        old_regime = int(regimepath[date +1, m])     \n",
        "        current_regime = regimes[~old_regime]        \n",
        "        if (old_regime - current_regime)>0:          #gamma 0-1\n",
        "          gamma = self.running(data[date, m, :])**0.3 +np.random.normal(0,1,1)         \n",
        "        else:\n",
        "          gamma = - self.running(data[date, m, :]) #gamma 1-0          \n",
        "        i = torch.tensor([value + running_benefit+gamma]).float()\n",
        "        a = torch.cat((a, i), 0)            \n",
        "    return a"
      ],
      "metadata": {
        "id": "4vdhJjVlyJbF"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class \"Training\"\n",
        "\n",
        "This is a recursion. It starts by initializing:\n",
        "- model, that is the simulated stock prices\n",
        "- payoff class\n",
        "- NN \n",
        "\n",
        "Create some matrices to store values that will be accessed to at different time steps\n",
        "- _mods_ records the models at each time step, basically it appends, by date, the networks (not sure if it is enough to say that it stores the optimized parameters)\n",
        "- _loss functions:_ just to plot\n",
        "- _regime path:_ matrix $(n \\times m)$, it records in which regime the process is at (for the same date, there are multiple paths which can be in different regimes)\n",
        "- _Y train:_ vector $(n \\times 1)$, it records the profit, as it is needed at each iteration\n",
        "- _tau dates:_ matrix $(n \\times m)$, it records for each date when there is a stopping time (can only be at such date or higher)\n",
        "- _F theta:_ matrix $(n \\times m)$, it records the outputs of the NN (0,1 values), only used to obtain the tau_dates\n",
        "\n",
        "The backward induction goes as:\n",
        "1. at maturity\n",
        "  - compute the terminal payoff and record it in _Y train_\n",
        "  - no need to compute the stopping times/stopping decision functions because by construction $f_N \\equiv 1$. Hence, set F_theta[N,:]=1 and tau_dates[N,:]=N\n",
        "  - sample a regime for the process at that time (same for all paths)\n",
        "\n",
        "2. before maturity\n",
        "- compute values for training the NN, hence current payoff $g(n, X_n, i)$ and future payoff $g(\\tau_{n+1}, X_{\\tau_{n+1}}, i)$. \n",
        "\n",
        "  To compute the payoff we need: stock values, date, one step ahead Y and the regime of the $n+1$ date because we need to identify which running cost function to use. It is a payoff when there is a switching so old regime and current regime are different.\n",
        "\n",
        "  The future payoff needs as time variable $\\tau_{n+1}$, we get it from the matrix tau_dates, taking value at $n+1$. The future payoff, as it refers to a value at time $\\tau_{n+1}$, needs to be discounted to $n$.\n",
        "- feed in the values in the function neural_stopping.train_network()\n",
        "- using the outputs returned by this function, we record switching decisions in matrix _F theta_ and optimal switching time in matrix _tau dates_\n",
        "- compute the \"actual\" current payoff. As we now know if there was or not a switch, we can compute the current payoff accordingly. we need to do this to record the value in _Y train_\n",
        "\n",
        "This is repeated until $n=1$"
      ],
      "metadata": {
        "id": "hKLrUKZbyFg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Training:\n",
        "  def __init__(self, model, payoff_function):\n",
        "\n",
        "    self.model = model    \n",
        "    self.neural_stopping = Training_network(self.model.assets, 400) \n",
        "    self.payoff = payoff_function(self.model)\n",
        "\n",
        "  def value(self):\n",
        "    model = self.model\n",
        "    stock_paths = self.model.simulate_process()   \n",
        "    \n",
        "    # create empty objects to store values\n",
        "    regimes = [0, 1]\n",
        "    regime_path=np.zeros((model.periods+1, model.paths)) # record at which regime we're at at each n\n",
        "    Y_train=np.zeros((model.periods+1, model.paths))\n",
        "    F_theta_train=np.zeros((model.periods+1,model.paths)) # record switching events for each n\n",
        "    tau_dates=np.zeros((model.periods+1,model.paths)) # switching times\n",
        " \n",
        "    mods=[None]*model.periods # record the models of the NN for testing\n",
        "    loss_functions = [None]*model.periods\n",
        "\n",
        "    # AT MATURITY\n",
        "    tau_dates[model.periods,:]=model.periods # by def \n",
        "    final_dates =  tau_dates[model.periods,:]\n",
        "    final_payoff = self.payoff.terminal_(final_dates, stock_paths) \n",
        "\n",
        "    # record values\n",
        "    Y_train[model.periods, :]= final_payoff.numpy()\n",
        "    F_theta_train[model.periods,:]=1 # by def\n",
        "     \n",
        "    regime_path[model.periods, :] = random.sample(regimes, 1)[0] # sample a regime at maturity\n",
        "    print(\"date\", model.periods, \", # switches\", model.paths)\n",
        "\n",
        "    # BEFORE MATURITY\n",
        "    for date in range(stock_paths.shape[0] - 2, 0, -1): \n",
        "      tau_date_plus_one = tau_dates[date+1, :]\n",
        "      discount_factor = np.exp(-model.drift*model.dt*(tau_date_plus_one-date))\n",
        "\n",
        "      continuation_value =  self.payoff.continuation_payoff(stock_paths, Y_train, tau_date_plus_one)\n",
        "      current_payoff = self.payoff.current_payoff(data = stock_paths, \n",
        "                                               Y = Y_train, date = date, \n",
        "                                               regimes = regimes,\n",
        "                                               regimepath = regime_path)\n",
        "\n",
        "      stopping_probability, networks, loss = self.neural_stopping.train_network(stock_paths[date, : , :], \n",
        "                                                    current_payoff, \n",
        "                                                    continuation_value*discount_factor)\n",
        "\n",
        "      print(\"date\", date, \", # switches\", len([1 for l in stopping_probability if l > 0.5]), \" mean reward \", np.mean(loss))\n",
        "      \n",
        "      # record values\n",
        "      F_theta_train[date,:]=(stopping_probability > 0.5)*1.0   # transform stopping probabilities in 0-1 decision\n",
        "      mods[date]=networks\n",
        "      loss_functions[date]=loss\n",
        "      tau_dates[date,:]=np.argmax(F_theta_train, axis=0)\n",
        "\n",
        "      which = stopping_probability > 0.5\n",
        "      \n",
        "      # compute the payoff taking into account if the regime switches (because the payoff funciton is different depending on that)    \n",
        "      if_switch = self.payoff.payoff_Yswitch(stock_paths, date, Y_train, regime_path, regimes) \n",
        "      if_no_switch = self.payoff.payoff_Nswitch(stock_paths, tau_date_plus_one, Y_train, date) \n",
        "\n",
        "      current_payoff = current_payoff.numpy()\n",
        "      # the final value is this \"current_payoff\", where we take for each path, either the expected future payoff or current\n",
        "      current_payoff[which] = if_switch[which]       # when we switch we take the current profit and distinguish if 0-1, 1-0\n",
        "      current_payoff[~which] = if_no_switch[~which]  # when we don't switch we take the discounted expected profit (when switched at a future stopping time)\n",
        "      Y_train[date, :] = current_payoff # update Y_train\n",
        "\n",
        "      mods[date]=networks \n",
        "      loss_functions[date]=loss    \n",
        "    \n",
        "    return mods, loss_functions"
      ],
      "metadata": {
        "id": "7UvqI1aXKWAL"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simulate paths Y\n",
        "# goal of this phase is to be able to get stopping decisions f_theta_n\n",
        "hyperparam_training = {'drift': 0.2, 'sigma': 0.05, 'delta': 0.1,  'paths':1000, 'periods': 9, 'maturity': 1., 'strike' : 100,'assets':2,  'spot':90,}\n",
        "S_train=BlackScholes(**hyperparam_training)\n",
        "\n",
        "pricing = Training(S_train, Payoff)\n",
        "'''\n",
        "arguments are:\n",
        "- path process\n",
        "- Payoff class\n",
        "'''\n",
        "\n",
        "mods, functions = pricing.value()"
      ],
      "metadata": {
        "id": "sp-KE2XpKsKY",
        "outputId": "313748b6-349d-4d0b-9589-4749b1eb7eff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date 9 , # switches 1000\n",
            "date 8 , # switches 201  mean reward  10.988794331511421\n",
            "date 7 , # switches 733  mean reward  13.302238662779796\n",
            "date 6 , # switches 176  mean reward  14.097711043892655\n",
            "date 5 , # switches 710  mean reward  14.537537330815104\n",
            "date 4 , # switches 855  mean reward  14.726832332063022\n",
            "date 3 , # switches 903  mean reward  14.748531022794591\n",
            "date 2 , # switches 988  mean reward  14.559396195190548\n",
            "date 1 , # switches 979  mean reward  14.676671475691021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_list = list(filter(None, functions))\n",
        "legend = [f't={i}' for i in range(len(functions))]\n",
        "\n",
        "for i in range(len(filtered_list)):\n",
        "  epochs = np.array([i for i in range(len(filtered_list[0]))])\n",
        "  plt.plot(epochs, filtered_list[i], label='reward function')\n",
        "  plt.ylabel('reward')\n",
        "  plt.xlabel('epochs')\n",
        "  plt.legend(legend)\n",
        "  plt.title('Reward curves across time periods')\n",
        "  plt.plot()"
      ],
      "metadata": {
        "id": "yVXeiEjqrfRf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "7023c318-7f7b-4476-c813-4ec8b7deaeab"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''CHECK'''\n",
        "'''\n",
        "model = S_train\n",
        "neural_stopping = Training_network(model.assets, 400) \n",
        "payoff = Payoff(model)\n",
        "\n",
        "\n",
        "stock_paths = model.simulate_process()   \n",
        "print(\"shape\", stock_paths.shape) # periods \n",
        "    \n",
        "# create empty objects to store values\n",
        "regimes = [0, 1]\n",
        "regime_path=np.zeros((model.periods+1, model.paths)) # record at which regime we're at at each n\n",
        "Y_train=np.zeros((model.periods+1, model.paths))\n",
        "F_theta_train=np.zeros((model.periods+1,model.paths)) # record switching events for each n\n",
        "tau_dates=np.zeros((model.periods+1,model.paths)) # switching times\n",
        "mods=[None]*model.periods # record the models of the NN for testing\n",
        "loss_functions = [None]*model.periods\n",
        "\n",
        "# AT MATURITY N\n",
        "tau_dates[model.periods,:]=model.periods \n",
        "final_dates =  tau_dates[model.periods,:] \n",
        "\n",
        "final_payoff = payoff.terminal_(final_dates, stock_paths)\n",
        "\n",
        "Y_train[model.periods, :]= final_payoff\n",
        "F_theta_train[model.periods,:]=1 # by def\n",
        "regime_path[model.periods, :] = random.sample(regimes, 1)[0] # sample a regime at maturity\n",
        "print(\"final_payoff\", final_payoff) \n",
        "#print(\"final_dates\", final_dates)\n",
        "#print(\"stock_paths\", stock_paths[-1, :, :])\n",
        "\n",
        "# from n=N-1 to 0 with steps of -1\n",
        "\n",
        "date = stock_paths.shape[0] - 2      \n",
        "tau_date_plus_one = tau_dates[date+1, :]\n",
        "discount_factor = np.exp(-model.drift*model.dt*(tau_date_plus_one-date))\n",
        "\n",
        "current_payoff = payoff.current_payoff(data = stock_paths, \n",
        "                                               Y = Y_train, date = date, \n",
        "                                               regimes = regimes,\n",
        "                                               regimepath = regime_path)\n",
        "                                               \n",
        "print(\"type current\", type(current_payoff), current_payoff)\n",
        "\n",
        "\n",
        "continuation_value =  payoff.continuation_payoff(stock_paths, Y_train, tau_date_plus_one)\n",
        "print(\"continuation_value\", continuation_value*discount_factor)\n",
        "stopping_probability, networks, loss = neural_stopping.train_network(stock_paths[date, : , :], \n",
        "                                              current_payoff, \n",
        "                                              continuation_value*discount_factor)\n",
        "\n",
        "# record values\n",
        "F_theta_train[date,:]=(stopping_probability > 0.5)*1.0   # transform stopping probabilities in 0-1 decision\n",
        "tau_dates[date,:]=np.argmax(F_theta_train, axis=0)\n",
        "which = stopping_probability > 0.5\n",
        "print(\"which\", which)\n",
        "\n",
        "if_switch = payoff.payoff_Yswitch(stock_paths, date, Y_train, regime_path, regimes) \n",
        "if_no_switch = payoff.payoff_Nswitch(stock_paths, tau_date_plus_one, Y_train, date) \n",
        "print(\"yes\", if_switch)\n",
        "print(\"no\", if_no_switch)\n",
        "'''"
      ],
      "metadata": {
        "id": "ewbzySRO-rpY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "476df370-27e0-40a1-d06e-a4653b124cac"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nmodel = S_train\\nneural_stopping = Training_network(model.assets, 400) \\npayoff = Payoff(model)\\n\\n\\nstock_paths = model.simulate_process()   \\nprint(\"shape\", stock_paths.shape) # periods \\n    \\n# create empty objects to store values\\nregimes = [0, 1]\\nregime_path=np.zeros((model.periods+1, model.paths)) # record at which regime we\\'re at at each n\\nY_train=np.zeros((model.periods+1, model.paths))\\nF_theta_train=np.zeros((model.periods+1,model.paths)) # record switching events for each n\\ntau_dates=np.zeros((model.periods+1,model.paths)) # switching times\\nmods=[None]*model.periods # record the models of the NN for testing\\nloss_functions = [None]*model.periods\\n\\n# AT MATURITY N\\ntau_dates[model.periods,:]=model.periods \\nfinal_dates =  tau_dates[model.periods,:] \\n\\nfinal_payoff = payoff.terminal_(final_dates, stock_paths)\\n\\nY_train[model.periods, :]= final_payoff\\nF_theta_train[model.periods,:]=1 # by def\\nregime_path[model.periods, :] = random.sample(regimes, 1)[0] # sample a regime at maturity\\nprint(\"final_payoff\", final_payoff) \\n#print(\"final_dates\", final_dates)\\n#print(\"stock_paths\", stock_paths[-1, :, :])\\n\\n# from n=N-1 to 0 with steps of -1\\n\\ndate = stock_paths.shape[0] - 2      \\ntau_date_plus_one = tau_dates[date+1, :]\\ndiscount_factor = np.exp(-model.drift*model.dt*(tau_date_plus_one-date))\\n\\ncurrent_payoff = payoff.current_payoff(data = stock_paths, \\n                                               Y = Y_train, date = date, \\n                                               regimes = regimes,\\n                                               regimepath = regime_path)\\n                                               \\nprint(\"type current\", type(current_payoff), current_payoff)\\n\\n\\ncontinuation_value =  payoff.continuation_payoff(stock_paths, Y_train, tau_date_plus_one)\\nprint(\"continuation_value\", continuation_value*discount_factor)\\nstopping_probability, networks, loss = neural_stopping.train_network(stock_paths[date, : , :], \\n                                              current_payoff, \\n                                              continuation_value*discount_factor)\\n\\n# record values\\nF_theta_train[date,:]=(stopping_probability > 0.5)*1.0   # transform stopping probabilities in 0-1 decision\\ntau_dates[date,:]=np.argmax(F_theta_train, axis=0)\\nwhich = stopping_probability > 0.5\\nprint(\"which\", which)\\n\\nif_switch = payoff.payoff_Yswitch(stock_paths, date, Y_train, regime_path, regimes) \\nif_no_switch = payoff.payoff_Nswitch(stock_paths, tau_date_plus_one, Y_train, date) \\nprint(\"yes\", if_switch)\\nprint(\"no\", if_no_switch)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lower bound\n",
        "\n",
        "the stopping time $\\tau^{\\Theta}$ gives a lower bound $L=\\mathbb{E}g(\\tau^{\\Theta}, X_{\\tau^{\\Theta}})$ for the optimal value $V_0= \\sup_{\\tau \\in \\mathcal{T}}\\mathbb{E}g(\\tau, X_{\\tau})$.\n",
        "\n",
        "Simulate \n",
        "- $K_L = 1024$ paths $(y_n^k)_{n=0}^N$, $k=1, \\ldots, K_L$, of $(X_n)_{n=0}^N$ and assume these are drawn independently from the realizations $(x_n^k)_{n=0}^N$, $k=1, \\ldots, K$.\n",
        "\n",
        "The unbiased estimate of the lower bound $L$ is given by\n",
        "\\begin{equation}\n",
        "\\hat{L}=\\frac{1}{K_L} \\sum_{k=1}^{K_L} g(l^k, y_{l^k}^k)\n",
        "\\end{equation}\n",
        "where $l^k = l(y_0^k, \\ldots, y_{N-1}^k)$"
      ],
      "metadata": {
        "id": "k8JSTFLsNMD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "\n",
        "class Testing_Lower:\n",
        "  def __init__(self, model, payoff_function, mods):   \n",
        "    self.model = model # argument is S   \n",
        "    self.neural_stopping = Training_network(model.assets, 400) \n",
        "    self.payoff = payoff_function(self.model)\n",
        "    self.mods = mods\n",
        "\n",
        "  def value(self):\n",
        "    model = self.model\n",
        "    disc_factor = np.math.exp((-model.drift) * model.maturity/(model.periods))\n",
        "    stock_paths = self.model.simulate_process()\n",
        "    k = np.array([0.4, 0.7])\n",
        "    regimes = [0, 1]\n",
        "    regime_path=np.zeros((model.periods+1, model.paths)) # record at which regime we're at at each n\n",
        "    Y_train=np.zeros((model.periods+1, model.paths))\n",
        "    F_theta_train=np.zeros((model.periods+1,model.paths)) # record switching events for each n\n",
        " \n",
        "    # at maturity N\n",
        "    final_payoff = np.array([self.profit_training.terminal(stock_paths[-1, :, :]), self.profit_training.terminal(stock_paths[-1, :, :])])   # payoff of the last date for each path.\n",
        "    future_payoff = torch.from_numpy(final_payoff*(np.math.exp((-model.drift) * model.periods))).double() \n",
        "    Y_train[model.periods, :]= final_payoff[0]\n",
        "    F_theta_train[model.periods,:]=1 # at maturity we switch (does it matter?)\n",
        "    regime_path[model.periods, :] = random.sample(regimes, 1)[0] # sample a regime at maturity\n",
        "    values = Y_train[model.periods, :]   \n",
        "\n",
        "\n",
        "    # recursive calc. before maturity\n",
        "         \n",
        "    for date in range(stock_paths.shape[0] - 2, 0, -1):\n",
        "      current_payoff = self.profit_training.running(Y_train[date+1, :], stock_paths[date, :, :])\n",
        "      mod_curr=self.mods[date]\n",
        "      probs=mod_curr(torch.from_numpy(stock_paths[date])) \n",
        "      np_probs=probs.detach().numpy().reshape(self.model.paths)\n",
        "      \n",
        "      F_theta_train[date,:]=(np_probs > 0.5)*1.0   # transform stopping probabilities in 0-1 decision\n",
        "      which = np_probs > 0.5\n",
        "\n",
        "      for m in range(0,model.paths-1):\n",
        "        old_regime = regime_path[date +1, m]\n",
        "        regime_path[date, m] = int(which[m])\n",
        "        if which[m] == True:\n",
        "          if int(old_regime) - int(which[m])>0:  #gamma 0-1\n",
        "            gamma = self.profit_testing.g(date, m, stock_paths)+0.7\n",
        "          else: gamma = -self.profit_testing.g(date, m, stock_paths) #gamma 1-0  \n",
        "        else:\n",
        "          gamma = 0 \n",
        "        Y_train[date, m] = Y_train[date+1, m]- gamma\n",
        "\n",
        "\n",
        "      immediate_exercise_value = Y_train[date, :]       \n",
        "      values[which] = immediate_exercise_value[which] # when we switch we take the current profit\n",
        "      values[~which] *= ((model.periods-date)/model.periods)           # when we don't switch we take final profit discounted \n",
        "\n",
        "      #Y_train[date, :] = values\n",
        "      print(\"date\", date, round(np.mean(values), 3), len([1 for l in np_probs if l > 0.5]))\n",
        "\n",
        "    \n",
        "    return round(np.mean(values), 3), Y_train"
      ],
      "metadata": {
        "id": "PpoI_K_MNO6b"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing phase - Lower bound\n",
        "\n",
        "# sample y from the process (Y)\n",
        "hyperparam_testing_L = {'drift': 0.2, 'sigma': 0.05, 'delta': 0.1,  'paths':5000, 'periods': 9, 'maturity': 3., 'strike' : 100,'assets':20,  'spot':90,}\n",
        "S_test_L=BlackScholes(**hyperparam_testing_L)\n",
        "\n",
        "# now we can compute all the stopping times recursively\n",
        "testing_pricing = Testing_Lower(S_test_L, Payoff, mods)\n",
        "'''\n",
        "arguments are:\n",
        "- path process\n",
        "- Payoff class\n",
        "'''\n",
        "value, Ytrain = testing_pricing.value()"
      ],
      "metadata": {
        "id": "eBdQnHLJNOMj"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "dict ={}\n",
        " \n",
        "# Insert data into dictionary\n",
        "dict1 = {\n",
        "     1: [\"2\", 90, 97.339, 0.009],\n",
        "     2: [\"2\", 100, 205.426, 0.006],\n",
        "     3: [\"2\", 110, 315.878, 0.007],\n",
        "     7: [\"4\", 90, 130.082, 0.008],\n",
        "     8: [\"4\", 100, 235.951, 0.008],\n",
        "     9: [\"4\", 110, 334.079, 0.005],\n",
        "     10: [\"5\", 90, 134.486, 0.008],\n",
        "     11: [\"5\", 100, 224.051, 0.006],\n",
        "     12: [\"5\", 110, 282.737, 0.006],\n",
        "     13: [\"10\", 90, 158.875, 0.005],\n",
        "     14: [\"10\", 100, 273.452, 0.008],\n",
        "     15: [\"10\", 110, 391.043, 0.015],\n",
        "     16: [\"20\", 90, 100.447, 0.008],\n",
        "     17: [\"20\", 100, 192.448, 0.01],\n",
        "     18: [\"20\", 110, 301.107, 0.009],\n",
        "     }\n",
        " \n",
        "# Print the names of the columns.\n",
        "print (\"{:<10} {:<10} {:<10} {:<10}\".format('assets', 'spot', 'L', 'timeL'))\n",
        " \n",
        "# print each data item.\n",
        "for key, value in dict1.items():\n",
        "    assets, spot, L, timeL = value\n",
        "    print (\"{:<10} {:<10} {:<10} {:<10}\".format(assets, spot, L, timeL))"
      ],
      "metadata": {
        "id": "qsGOwzwDvUCX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0e6991d-2c4e-423d-b8f0-e73093d1905d"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assets     spot       L          timeL     \n",
            "2          90         97.339     0.009     \n",
            "2          100        205.426    0.006     \n",
            "2          110        315.878    0.007     \n",
            "4          90         130.082    0.008     \n",
            "4          100        235.951    0.008     \n",
            "4          110        334.079    0.005     \n",
            "5          90         134.486    0.008     \n",
            "5          100        224.051    0.006     \n",
            "5          110        282.737    0.006     \n",
            "10         90         158.875    0.005     \n",
            "10         100        273.452    0.008     \n",
            "10         110        391.043    0.015     \n",
            "20         90         100.447    0.008     \n",
            "20         100        192.448    0.01      \n",
            "20         110        301.107    0.009     \n"
          ]
        }
      ]
    }
  ]
}