{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "optimal_switching_V1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN2Y+EwZwqNMFhnod9gJOol",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudia-viaro/optimal_switching/blob/main/optimal_switching_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LbFaWc55v5HL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "np.random.seed(234198)\n",
        "import itertools\n",
        "import random\n",
        "import time\n",
        "import scipy.stats\n",
        "\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as tdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class stock:\n",
        "    def __init__(self, drift, sigma, delta, spot, assets,  paths, periods,\n",
        "         maturity, strike = 100,dividend=0):\n",
        "        self.maturity = maturity\n",
        "        self.strike = strike\n",
        "        self.assets = assets\n",
        "        self.sigma=sigma *np.ones(self.assets)\n",
        "        self.delta=delta\n",
        "        self.spot = spot*np.ones(self.assets)\n",
        "        self.drift = drift - dividend\n",
        "        self.paths = paths\n",
        "        self.periods = periods\n",
        "        self.dt = self.maturity / self.periods\n",
        "    \n",
        "    def GBM(self):\n",
        "        \n",
        "        dt = self.maturity / self.periods\n",
        "        So_vec=self.spot*np.ones((1,S.paths, S.assets))\n",
        "        \n",
        "        Z=np.random.standard_normal((self.periods,self.paths, self.assets))\n",
        "        s=self.spot*np.exp(np.cumsum((self.drift-0.5*self.sigma**2)*self.dt+self.sigma*np.sqrt(self.dt)*Z, axis=0))\n",
        "        \n",
        "        s=np.append(So_vec, s, axis=0)\n",
        "        return s"
      ],
      "metadata": {
        "id": "Eba9zKxNwIv4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GBM:\n",
        "  def __init__(self, drift, sigma, delta, spot, assets,  paths, periods,\n",
        "         maturity, strike = 100,dividend=0):\n",
        "    self.drift = drift - dividend\n",
        "    self.sigma = sigma\n",
        "    self.delta = delta\n",
        "    self.spot = spot\n",
        "    self.assets = assets\n",
        "    self.paths = paths\n",
        "    self.periods = periods\n",
        "    self.maturity = maturity\n",
        "    self.dt = self.maturity / self.periods\n",
        "    self.df = math.exp(-drift * self.dt)\n",
        "    self.strike = strike\n",
        "\n",
        "  def simulate_process(self):\n",
        "    paths = self.paths\n",
        "    path = np.array([self.simulate_one_path() for i in range(paths)]) \n",
        "    return path.reshape(path.shape[2], path.shape[0], path.shape[1])\n",
        "\n",
        "  def drift_fct(self,x):\n",
        "    return  (self.drift-self.delta-0.5*self.sigma**2)* x\n",
        "\n",
        "  def diffusion_fct(self,x):\n",
        "    return self.sigma * x\n",
        "\n",
        "\n",
        "  def simulate_one_path(self):\n",
        "    path = np.empty((self.assets, self.periods+1))\n",
        "    path[:, 0] = self.spot\n",
        "    for k in range(1, self.periods+1):\n",
        "      random_numbers = np.random.normal(0, 1, self.assets)\n",
        "      dW =(random_numbers*np.sqrt(self.dt))\n",
        "      previous_spots = path[:, k - 1]\n",
        "      diffusion = (self.diffusion_fct(previous_spots))\n",
        "      path[:, k] = (\n",
        "          previous_spots\n",
        "          + self.drift_fct(previous_spots)* self.dt\n",
        "          + diffusion*dW) \n",
        "    return path  "
      ],
      "metadata": {
        "id": "QzgW1KFUwTXW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_stock_model(stockmodel):\n",
        "    stock_paths = stockmodel\n",
        "\n",
        "    # draw a path\n",
        "    one_path = stock_paths[:, 0, 0]\n",
        "    dates = np.array([i for i in range(len(one_path))])\n",
        "    plt.plot(dates, one_path, label='stock path')\n",
        "    plt.ylabel('Stock price')\n",
        "    plt.ylabel('Time')\n",
        "    plt.legend()\n",
        "    return plt.show()   "
      ],
      "metadata": {
        "id": "bjcflzhmwbOR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparam_test_stock_models = {'drift': 0.2, 'sigma': 0.05, 'delta': 0.1,  'paths':5000, 'periods': 49, 'maturity': 3., 'strike' : 100,'assets':10,  'spot':90,}\n",
        "#S = GBM(**hyperparam_test_stock_models)\n",
        "#X=S.simulate_process()\n",
        "\n",
        "S=stock(**hyperparam_test_stock_models)\n",
        "X=S.GBM()\n",
        "\n",
        "print(X.shape) # (date, path, asset)\n",
        "draw_stock_model(X) "
      ],
      "metadata": {
        "id": "5GOfCwrFwbqc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "9c36124f-ae3e-466d-8b1c-290dafa6c772"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50, 5000, 10)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD4CAYAAAAD6PrjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hUZf7//+c7IYUQCIFQUwhIEaQJkSoKuiIiir0XbLjWddfuFt1d3Z+6rrqurruoLOJXUVDsiiLLCh+REhAInVBTIAklgRAIKffvj4wYIdRkcpKZ1+O6cmXOfe6ZeR8c85pz7nPuY845REREAEK8LkBEROoOhYKIiBygUBARkQMUCiIicoBCQUREDmjgdQHVERcX55KTk70uQ0SkXlm4cOE251yLqtb5LRTMbDwwCsh1znX3tT0B3Abk+bo95pz7wrfuUeAWoAy41zn31dHeIzk5mdTUVD9ULyISuMxs0+HW+fPw0QRgRBXtLzjnevt+fgyEbsBVwCm+5/zTzEL9WJuIiFTBb6HgnJsF7DjG7qOBd51zxc65DUA60M9ftYmISNW8GGi+28yWmtl4M4v1tcUDGZX6ZPraRESkFtX2QPOrwJ8B5/v9N+Dm43kBMxsLjAVISko6ZH1JSQmZmZns27ev2sXKz0VGRpKQkEBYWJjXpYiIn9RqKDjncn58bGavAZ/5FrOAxEpdE3xtVb3GOGAcQEpKyiETN2VmZtK4cWOSk5Mxs5oqPeg559i+fTuZmZm0b9/e63JExE9q9fCRmbWptHgxsMz3+BPgKjOLMLP2QCdg/om8x759+2jevLkCoYaZGc2bN9cemEiA8+cpqZOAoUCcmWUCjwNDzaw3FYePNgK3AzjnlpvZZGAFUArc5Zwrq8Z7V694qZL+XUUCn99CwTl3dRXNbxyh/1PAU/6qR0SkvthfWs6izTtJyywgJTmW3olNa+1LWb2+ork+efHFFxk7dixRUVHH/dwnnniC6OhoHnjggRqrZ+PGjcyZM4drrrkGgAkTJpCamsrLL79cY+8hIsfGOcf6bXuYvSaP2Wu38f367RTt/+lgSYe4RlzSJ56L+yQQ37ShX2tRKNSSF198keuuu+6EQsEfNm7cyDvvvHMgFETEG/+3dhsPf7CUrPy9ACQ3j+KSPvEM6dSCngkxzFqTxweLsnju6zU89/UaBnZoziV94jmvRxuiI2r+T7hCoYbt2bOHK664gszMTMrKyvj9739PTk4O2dnZDBs2jLi4OGbOnMmkSZP4y1/+gnOO888/n2eeeQaAadOm8dhjj1FWVkZcXBwzZsz42eu/9tprTJ06lalTp9Kw4U/fGMaMGUNkZCSpqans2rWL559/nlGjRrFx40auv/569uzZA8DLL7/MoEGDeOSRR1i5ciW9e/fmxhtvJDY2luzsbEaMGMG6deu4+OKLefbZZ2vvH04kCC3LKuD2t1Jp07QhT13cnSEdW5DU/OdfHK88LYkrT0siY0cRUxdlMfWHTB58fylLMwv480Xda7ymgA6FP366nBXZu2r0Nbu1bcLjF5xy2PXTpk2jbdu2fP755wAUFBQQExPD888/z8yZM4mLiyM7O5uHH36YhQsXEhsby/Dhw/noo48YPHgwt912G7NmzaJ9+/bs2PHzC8Jffvllpk+fzkcffURERMQh771x40bmz5/PunXrGDZsGOnp6bRs2ZLp06cTGRnJ2rVrufrqq0lNTeXpp5/mueee47PPKs4KnjBhAosXL+aHH34gIiKCLl26cM8995CYmHjI+4hI9WXuLOKmCQuIaRjG27f2p1WTyCP2T2wWxa9+0Yl7z+7Ios07iY0K90tdAR0KXujRowf3338/Dz/8MKNGjWLIkCGH9FmwYAFDhw6lRYuKSQqvvfZaZs2aRWhoKGecccaB6wCaNWt24DkTJ04kMTGRjz766LAXj11xxRWEhITQqVMnOnTowKpVq2jfvj133303ixcvJjQ0lDVr1hy29rPPPpuYmBgAunXrxqZNmxQKIn5QUFTCmP8sYF9JGW/fMeiogVCZmdG3XbOjdzxBAR0KR/pG7y+dO3dm0aJFfPHFF/zud7/j7LPP5g9/+EO1X7dHjx4sXrz4iBePHXx2gpnxwgsv0KpVK5YsWUJ5eTmRkYf/8FXe+wgNDaW0tLTadYvIzxWXljH2rVQ2by/izZv70blVY69L+hndZKeGZWdnExUVxXXXXceDDz7IokWLAGjcuDG7d+8GoF+/fnz77bds27aNsrIyJk2axJlnnsmAAQOYNWsWGzZsAPjZ4aNTTz2Vf//731x44YVkZ2dX+d5TpkyhvLycdevWsX79erp06UJBQQFt2rQhJCSEt956i7KyskPqEZHaUV7ueGDKUuZt2MFfL+/JwJOae13SIQJ6T8ELaWlpPPjgg4SEhBAWFsarr74KwNixYxkxYgRt27Zl5syZPP300wwbNuzAQPPo0aMBGDduHJdccgnl5eUHxgN+dPrpp/Pcc89x/vnnM336dOLi4n723klJSfTr149du3bxr3/9i8jISO68804uvfRSJk6cyIgRI2jUqBEAPXv2JDQ0lF69ejFmzBhiY2MREf96ZtoqPl2SzSPnnczo3nVzzk9z7pDpg+qNlJQUd/BNdlauXEnXrl09qsg7Y8aMYdSoUVx22WV+fZ9g/fcVqa73Fmzm4Q/SuH5AO/40+hRPZwgws4XOuZSq1unwkYiIn+UX7ecvX6xiYIfmPHGht4FwNDp8FCAmTJjgdQkichh/n7GW3ftKePzCboSG1N1AgADdU6jPh8TqMv27ihy/9XmFvPX9Jq48LZGTWzfxupyjCrhQiIyMZPv27foDVsN+vJ/CkU5pFZFDPf3lKiIahPDrczp7XcoxCbjDRwkJCWRmZpKXl+d1KQHnxzuvicixmbt+O1+vyOHBc7vQsnH9+EIVcKEQFhamO4OJSI1xzpGzq5j1eYWs37aHDdv2sD6vkOz8fVzWN4Fbh7SvcuC4vNzx5OcraBsTyS2n15+/SQEXCiIiNWFLwV5en72ByakZ7N7309X9kWEhtI+LJjI8lKe+WMnqnN08dXF3IhqE/uz5H/6QxbKsXbx4ZW8iw0IPfvk6S6EgIlJJeu5u/vXtej5enEW5g5E92tAvOZb2cdF0aNGI1k0iCQkxnHP8fcZaXvxmLZu27+Ff1/WleXTFVDFF+0v561er6ZUQw4W92nq8RcdHoSAiAizctJN/fbuO6StyiAwL4dr+7bjl9PYkNqv6Hihmxn2/6MxJLaJ5YMoSRr/yHW/ceBpdWjfmtVkb2LprH/+45lRC6vgpqAdTKIhIUCvaX8rvPlrG1EVZxDQM496zO3HjwHYHvvUfzQW92pLULIrbJqZyyT+/4/ELTuHfs9ZxXvfWnJbsv9lM/UWhICJBKz23kDvfXsja3ELuOasjvzzzJBqdwN3MeiU25eO7B3Prm6k89MFSwkKNR8472Q8V+59CQUSC0seLs3h0ahqRYaFMvLkfQzq1qNbrtYlpyJRfDuTJz1fSqWU07Zo3qqFKa5dCQUSCSnFpGX/+bAX/b+5mUtrF8vI1fWgdUzPXEESFN+AvF/eokdfyikJBRIJGxo4i7npnEUszCxh7RgcePLcLYaEBN7FDtSgURCQoLNy0g9smLqSkrJx/X9+Xc09p7XVJdZJCQUQC3idLsnlgyhLaxkQyfsxpdGgR7XVJdZbf9pvMbLyZ5ZrZsirW3W9mzszifMtmZi+ZWbqZLTWzPv6qS0SCh3OOf8xYy72TfqB3QlOm3jlYgXAU/jyYNgEYcXCjmSUCw4HNlZrPAzr5fsYCr/qxLhEJAvtLy3lgylL+Nn0NF58az1u39qNZo3Cvy6rz/BYKzrlZwI4qVr0APARUntt6NDDRVZgLNDWzNv6qTUQCW37Rfq5/Yx4fLMrk17/ozPNX9DpkbiKpWq2OKZjZaCDLObfkoFkF44GMSsuZvrYtVbzGWCr2JkhKSvJfsSJSLznnuGH8fFZt2c3fr+rN6N7xXpdUr9TauVhmFgU8BvyhOq/jnBvnnEtxzqW0aFG9i01EJPB8l76dpZkF/PmiUxQIJ6A29xROAtoDP+4lJACLzKwfkAUkVuqb4GsTETkuE+ZsIC46nItOVSCciFrbU3DOpTnnWjrnkp1zyVQcIurjnNsKfALc4DsLaQBQ4Jw75NCRiMiRbNq+hxmrcrmmfzuNIZwgf56SOgn4HuhiZplmdssRun8BrAfSgdeAO/1Vl4gErglzNtIgxLiuv8YbT5TfDh85564+yvrkSo8dcJe/ahGRwFdYXMqU1EzO79GGlk3qx/2Q6yJN+iEiAeH91AwKi0sZM7j+3A+5LlIoiEi9V17uePP7TZya1JTeiU29LqdeUyiISL337Zo8Nmzbw03aS6g2hYKI1Hvjv9tAqyYRnNddM59Wl0JBROq19NzdzF67jesHtNO9EWqA/gVFpF6bMGcj4Q1CuLqfTkOtCQoFEam3CopK+GBhFqN7taV5dITX5QQEhYKI1FvvpW5mb0kZYwYne11KwFAoiEi9VFbueHPOJvq1b8YpbWO8LidgKBREpF6aviKHrPy93DQo2etSAopCQUTqpf98t4H4pg05p1srr0sJKAoFEal3VmTvYt6GHdwwsB0NdBpqjdK/pojUOxPmbCAyLIQrT0s8emc5LgoFEalXthcW89HibC7pk0DTqHCvywk4CgURqVfeXZDB/tJyDTD7iUJBROqNkrJy3vp+E0M6xdGpVWOvywlICgURqTemLdvK1l37GKO9BL9RKIhIvfGf7zaQ3DyKYV1ael1KwFIoiEi9sCQjn0Wb87lxUDIhIeZ1OQFLoSAi9cKEORuJjmjAZX0TvC4loCkURKTOy921j8+WZnNZ3wQaR4Z5XU5AUyiISJ339rzNlJY7DTDXAoWCiNRpxaVlvD1vE8O6tCQ5rpHX5QQ8hYKI1FlbCvYyZvwCthXu55bT23tdTlBo4HUBIiJV+TJtC49MTaOkrJxnL+3J4I5xXpcUFPy2p2Bm480s18yWVWr7s5ktNbPFZva1mbX1tZuZvWRm6b71ffxVl4jUbXuKS3no/SXc8fYikptH8fm9Q7hCE9/VGn8ePpoAjDio7a/OuZ7Oud7AZ8AffO3nAZ18P2OBV/1Yl4jUUUsy8jn/pdlMWZjJXcNO4v07BtFe4wi1ym+Hj5xzs8ws+aC2XZUWGwHO93g0MNE554C5ZtbUzNo457b4qz4R8d62wmLSsgpYmlFAWlY+/1udR8vGEUy6bQADOjT3urygVOtjCmb2FHADUAAM8zXHAxmVumX62g4JBTMbS8XeBElJSX6tVURq3tfLtzJ1URZpWQVk5e8FwAxOahHNNf2TuP+cLsRE6VoEr9R6KDjnfgv81sweBe4GHj/O548DxgGkpKS4o3QXkTrCOcer367j2WmriW/akD7tYhkzKJkeCTF0j48hOkLnvdQFXv5XeBv4gopQyAIqjyQl+NpEJACUlpXz+4+XM2n+Zkb3bsuzl/UkokGo12VJFWr1OgUz61RpcTSwyvf4E+AG31lIA4ACjSeIBIbC4lJunZjKpPmbuXtYR168srcCoQ7z256CmU0ChgJxZpZJxR7BSDPrApQDm4Bf+rp/AYwE0oEi4CZ/1SUitSdn1z5unrCAVVt385eLe3BNf40D1nX+PPvo6iqa3zhMXwfc5a9aRKT2rcnZzZjx88nfW8LrN6boHgj1hEZ2RKTG7d5XwjWvzcXMmHz7QLrHx3hdkhwjhYKI1LjXZ29gW+F+Pr5rsAKhntGEeCJSo7YVFvP67PWM7NGaXolNvS5HjpNCQURq1Csz09lXWs79w7t4XYqcAIWCiNSYjB1FvD13M5f3TeCkFtFelyMnQKEgIjXmxW/WgsGvftHp6J2lTlIoiEiNWL11N1N/yGTMoGTaxDT0uhw5QQoFEakRz329mujwBtxx5klelyLVoFAQkWpbuGkn01fkcPuZHYhtFO51OVINCgURqRbnHM9OW0VcdDg3DdZ9lOs7hYKIVMustduYt2EH95zViUaa/rreUyiIyAnL3bWPZ75cRUJsQ67up8nuAoFiXUSOy979ZXy9ouLuabPX5lHu4JVr+hDeQN8xA4FCQUSOyfwNO3h/YQZfpG2lsLiU+KYNuXNoRy7uE68L1QKIQkFEjuqdeZt57MM0GoWHcl6PNlzSJ54B7ZsTEmJelyY1TKEgIkdUUFTCX79aRf/2zfjPTacRFa4/G4FMBwFF5IhenLGGgr0lPH7BKQqEIKBQEJHDSs8t5K3vN3HlaUl0a9vE63KkFigURIJQdv5ecnfvO2q/pz5fQcOwUO4f3rkWqpK6QKEgEkRKysp5ZWY6Q5/7H+e9OJuVW3Ydtu//Vucyc3Ue95zdkbjoiFqsUrykUBAJEksy8rngH//HX79azbAuLQhvEMJV4+ayNDP/kL4lZeU8+flKkptHMWaQpq4IJkcNBTNrZWZvmNmXvuVuZnaL/0sTkZpQtL+UP3+2gov/+R07i/Yz7vq+/Pv6FCbfPpDGkQ249rV5pG7c8bPnvD13E+m5hTw2sqsuSgsyx/JfewLwFdDWt7wGuM9fBYlIzfkufRvDX5jFG/+3gWv6JzH9N2cy/JTWACQ2i2LKLwfSonEE178xnznp2wDYuWc/L3yzlsEdm3NOt1Zeli8eOJZQiHPOTQbKAZxzpUCZX6sSkWpbmpnPDePnE94ghMm3D+TJi3rQJDLsZ33axDTk3dsHkNQsipsmLGDm6lz+PmMtu/eV8PtR3TDTxWnB5lhOOt5jZs0BB2BmA4ACv1YlItWyr6SMX7+3mBbREXx4x2BiosIO27dl40gmjR3ADePnMXZiKuUOrumfxMmtdQpqMDqWPYXfAJ8AJ5nZd8BE4J6jPcnMxptZrpktq9T2VzNbZWZLzexDM2taad2jZpZuZqvN7NwT2BYR8Xlm2irW5e3hr5f3PGIg/KhZo3DevnUAPeJjaBLZgF//QqegBitzzh29k1kDoAtgwGrnXMkxPOcMoBCY6Jzr7msbDvzXOVdqZs8AOOceNrNuwCSgHxVjF98AnZ1zRzxMlZKS4lJTU49av0gwmZO+jWten8eNA9vxx9Hdj+u5pWXl7CkuO6YgkfrLzBY651KqWnfUw0dmFgqMBJJ9/YebGc6554/0POfcLDNLPqjt60qLc4HLfI9HA+8654qBDWaWTkVAfH+0+kTkJwV7S3hgyhI6xDXikfO6HvfzG4SGEBOls42C2bGMKXwK7APS8A0215Cbgfd8j+OpCIkfZfraDmFmY4GxAElJuqmHSGV//HQ5ObuL+eCOQTQMD/W6HKmHjiUUEpxzPWvyTc3st0Ap8PbxPtc5Nw4YBxWHj2qyLpH6bNqyLUxdlMW9Z3Wkd2LToz9BpArHsp/4pW8soEaY2RhgFHCt+2lAIwtIrNQtwdcmIscgd/c+HvtwGd3jm3DP2Z28LkfqsWMJhbnAh2a218x2mdluMzv8hClHYGYjgIeAC51zRZVWfQJcZWYRZtYe6ATMP5H3EAk25eWOx6amUVhcygtX9CYsVGMCcuKO5fDR88BAIM0dy6lKPmY2CRgKxJlZJvA48CgQAUz3XRQz1zn3S+fccjObDKyg4rDSXUc780hEKuYzevyT5SzOyOf3o7rRqVVjr0uSeu5YQiEDWHY8gQDgnLu6iuY3jtD/KeCp43kPkWC1rbCYZ6etYnJqJnHREfzt8l5c0qfKczNEjsuxhMJ64H++CfGKf2w82impIlLzSsrKeev7TbzwzRr27i9j7BkduOesjjSO1HUFUjOOJRQ2+H7CfT8iUstydu1j+oocJn6/kTU5hQzpFMfjF5xCx5bRXpcmAeaooeCc+2NtFCISTIr2l/LZki0U7C0hsVkUSc2iSGoeRXTET/9LpucW8vWKrXy9PIfFGRX3PDipRSPGXd+Xc7q10mR14heHDQUze9k5d7eZfYpvMrzKnHMX+rUykQCUlb+Xid9v5N35GRTsPXS2mGaNwklsFsXuvSWs37YHgF4JMTx4bheGd2tFx5bRCgPxqyPtKdwA3A08V0u1iAQk5xypm3byn+828NXyHABGnNKamwYn07FlNBk79rJ5R9GBn4wdRcQ0DGPM4GTO6daKNjENPd4CCSZHCoV1AM65b2upFpGA4ZxjTU4h01ds5Yu0razYsouYhmHcOqQ9NwxMJr7pT3/om0aF0yMhxsNqRX5ypFBoYWa/OdxKnX0k8nNl5Y7UjTuYviKH6Stz2LS94vrM3olNefKi7lzSJ56o8GM5t0PEO0f6hIYC0VRMly0ih7Eur5BJ8zYz9YcsduzZT3hoCIM6Nuf2M07iF11b0rJJpNclihyzI4XCFufcn2qtEpF6pLi0jK+W5/DOvE3MXb+DBiHG8FNacX6PtpzZpcXPziISqU+O9MnVHoLIQdJzC5myMIP3UzPZvmc/CbENefDcLlyekkDLxtojkPrvSKFwdq1VIVKH5e7axydLsvl4cTZpWQWEhhhnn9ySawe0Y0jHOEJC9P1JAsdhQ8E5t6M2CxGpS4r2l/JF2lY+XpzFd+nbKHfQIz6G353flQt7tdU4gQQsHfgUOcj8DTv49XuLycrfS2Kzhtw1rCOje8drSgkJCgoFEZ+SsnJemrGWV2amk9gsindu7c/Ak5rrCmIJKgoFEWDDtj3c995ilmTkc0VKAn+44BSdQSRBSZ96CWrOOaakZvLEp8sJCw3hn9f2YWSPNl6XJeIZhYIEraL9pTz4/lI+X7qFgR2a8/yVvTTPkAQ9hYIEpZxd+7jlzQWsyN7FwyNO5vYzOujUUhEUChKElmUVcOubqezeV8LrN6Zw1smtvC5JpM5QKEhQmb4ih3sn/UBsVBjv3zGIrm2aeF2SSJ2iUJCg4Jzj9dkb+MuXK+kZH8NrN6ToAjSRKigUJOAV7S/lT5+u4N0FGYzs0Zq/Xd6bhuGhXpclUieFeF2A1E+vzEzn/Jdm89nSbMrLD7lb6yGcc6TnFh5T35r031U5nPP8LN5dkMFdw07i5av7KBBEjkB7CnLcUjfu4G9fryYqvAF3v/MD3dqs48FzuzC0S4tDrv4tKCrhg0WZvD1vE+vy9jCyR2uev6I3kWH+/cO8pWAvf/xkBdOWb6VTy2gm3z6Qfu2b+fU9RQKBQkGOS2FxKb+ZvIT42IZ8fu8QZqzM4YXpa7lpwgL6tovlwXO70L99M5ZkFvD23E18ujSbfSXlnJrUlDGDknnz+41sLZjLazek0Dw6osbrKy0rZ+L3m/jb16spLXc8eG4XbhvSgfAG2ikWORbmnH92581sPDAKyHXOdfe1XQ48AXQF+jnnUiv1fxS4BSgD7nXOfXW090hJSXGpqalH6yY16NGpaby7YDPvjf3pm3dJWTmTUzN4acZacnYV0zYmkuyCfUSFh3LRqfFc0y+J7vEV9yD+Mm0L9723mDYxkfznpn60j2tUY7Vl5e/l9rdSWZa1i6FdWvCnC7uT1Dyqxl5fJFCY2ULnXEqV6/wYCmcAhcDESqHQFSgH/g088GMomFk3YBLQD2gLfAN0ds6VHek9FAq167+rcrh5Qiq3n9GBR0d2PWT9vpIy3vp+E7PW5jH8lNZc1LstjSPDDum3cNNObpuYWnFG0I0p9G1X/cM6e4pLufTVOWTt3MvTl/ZkZI/WmshO5DCOFAp+26d2zs0CdhzUttI5t7qK7qOBd51zxc65DUA6FQEhdcSOPft56P00Tm7dmN8M71xln8iwUG47owNv3dKf6we0qzIQAPq2i2XqHYNoGhXO1a/N4/OlW6pVW3m54zeTF7MmZzcvX9uH83u2USCInKC6cqA1HsiotJzpazuEmY01s1QzS83Ly6uV4oKdc47ffphGwd79PH9FbyIaVH+QODmuER/cMYie8THc9c4i/r8vVlJYXHpCr/XiN2v4ankOj43sypmdW1S7NpFgVldC4Zg558Y551KccyktWugPQG34aHEWXy7bym/O6UK3tjV3BXCzRuH8v1v7c2VKIv+etZ6znvsfHyzMPK7TVj9bms1L/03n8r4J3HJ6+xqrTSRY1ZVQyAISKy0n+NrEY9n5e/nDx8tJaRfL2DM61PjrR4aF8sxlPZl65yDaNG3I/VOWcMmrc/hh886jPndZVgEPTFlC33axPHlxdx0yEqkBdSUUPgGuMrMIM2sPdALme1yTAL/9MI2ycsffruhFqB9nEe2TFMuHdwzib5f3Iit/Lxf/cw6/mbyYdXlVX/CWu3sft01MpVlUOP+6rm+NHNISET9ep2Bmk4ChQJyZZQKPUzHw/A+gBfC5mS12zp3rnFtuZpOBFUApcNfRzjwS/1u4aSczV+fxyHkn0655zZ06ejghIcalfRM4t3trXpmZzhuzNzB1URZR4aF0btWYrm0ac3LrJnRp3Zhnp60iv6iE9+8YSIvGNX+9g0iw8tspqbVBp6T615j/zGdpZgGzHxpGIw9uTZmxo4g567axcstuVm3dxaqtu8kvKjmwXndJEzkxRzolVVc0S5WWZOTzv9V5PDSiiyeBAJDYLIormyUdWHbOkbu7mJVbdhEd0YCUZE1bIVLTFApSpX/8dy1No8K4YWCy16UcYGa0ahJJK015LeI3dWWgWeqQZVkFfLMyl1sGtyfao70EEfGGQkEO8dKMtTSJbMCNg5O9LkVEaplCQX5mRfYuvl6Rw82nt6fJYaapEJHApVCQn3l55loaRzTgpkG6OlgkGCkU5IDVW3fzRdpWxgxOJiZKewkiwUihIAe8PDOdRuGh3DxYewkiwUqhIACk5+7ms6XZ3DAomdhG4V6XIyIeUSgI5eWOF79ZS2SDUG7VTKMiQU0noQeokrJywkKPnPm79pUwJTWTid9vZNP2Iu4YepJf7pssIvWHQiEAvTlnI098upzOLRvTp10sfZKa0rddLO3jGmFmpOcW8uacjXywKJOi/WX0bRfLA8O7aB4hEVEoBJrc3fv461erOaVtE+KiI/h8aTaT5m8GIDYqjMRmUSzNLCA8NIQLerVlzKBkeiTEeFy1iNQVCoUA8+y01RSXlvHSVafSoUU05eWOdXmFLNy0k4WbdpKeV8j953Tm6v5JxOlQkYgcRKEQQJZCwQUAAAr1SURBVBZt3sn7CzP55Zkn0aFFNFBxj4JOrRrTqVVjruqXdJRXEJFgp7OPAkR5ueOJT5bTsnEEd5/V0etyRKSeUigEiCkLM1iaWcBjI7tqZlMROWEKhQBQsLeEZ6etJqVdLKN7t/W6HBGpx/SVMgC8MH0NO4r28+aF/TAzr8sRkXpMewr13Oqtu3lr7iau6ZdE93idWioi1aNQqMecqxhcjo5owAPDu3hdjogEAIVCPfbZ0i18v347DwzvrEnsRKRGKBTqqbnrt/Pg+0vomRDDNf3beV2OiAQIhUI9tGDjDm6esIDE2CjGjzmN0BANLotIzVAo1DMLN+1kzPj5tI6J5O3b+muqChGpUX4LBTMbb2a5ZrasUlszM5tuZmt9v2N97WZmL5lZupktNbM+/qqrPluckc+Y8fNp2SSSSbcNoGXjSK9LEpEA4889hQnAiIPaHgFmOOc6ATN8ywDnAZ18P2OBV/1YV72UllnADW/MI7ZROO/c1p9WTRQIIlLz/BYKzrlZwI6DmkcDb/oevwlcVKl9oqswF2hqZprc3ycts4Dr3phH48gw3rmtP21iGnpdkogEqNq+ormVc26L7/FWoJXvcTyQUalfpq9tCwcxs7FU7E2QlBSYs36WlJWzaNNOZq7O43+rc1m1dTdtYyJ5d+wAEmKjvC5PRAKYZ9NcOOecmbkTeN44YBxASkrKcT+/LvsybQufLs1m9tpt7N5XSoMQIyU5lkfPO5mLT42npQ4ZiYif1XYo5JhZG+fcFt/hoVxfexaQWKlfgq8taPxvdS53vL2IVk0iGNm9DcNObsHgjnE0jgzzujQRCSK1HQqfADcCT/t+f1yp/W4zexfoDxRUOswU8IpLy3jik+V0iGvEl/cNIaJBqNcliUiQ8lsomNkkYCgQZ2aZwONUhMFkM7sF2ARc4ev+BTASSAeKgJv8VVdd9PrsDWzcXsTEm/spEETEU34LBefc1YdZdXYVfR1wl79qqcsydxbxj/+uZcQprTmjcwuvyxGRIKcrmj325GcrAfj9Bd08rkRERKHgqW/X5DFt+VbuOasT8U117YGIeE+h4JEfB5fbxzXi1iHtvS5HRARQKHjm9dkb2LBtD49f0E2DyyJSZygU/GTHnv18vnQLG7ftoWIc/SdZ+Xt5+b/pDO/WiqFdWnpUoYjIoTy7ojmQOee4773FzFqTB0CzRuGcmtiUU5OacmpSLBO/30i5c/x+lAaXRaRuUSj4wUeLs5i1Jo97z+5E6yaR/LB5Jz9k5DNjVe6BPvef05nEZprHSETqFoVCDdteWMyfPl1Bn6Sm/OrsToSGGNf0r5i4r6CohMWZ+WTuLOLyvolHeSURkdqnUKhhT36+ksLiUp6+tOcht8mMiQrjTF2gJiJ1mAaaa9C3a/L48Ics7hjakc6tGntdjojIcVMo1JA9xaU8NjWNk1o04q5hJ3ldjojICdHhoxry/PQ1ZOXvZcovB+q6AxGpt7SnUAOWZOTzn+82cN2AJE5LbuZ1OSIiJ0yhUE0lZeU8/MFSWjSO4KERJ3tdjohItejwUTU453h++hpWbd3NuOv70kR3SRORek6hcIL2lZTx2IdpTF2UxWV9Exh+SmuvSxIRqTaFwgnI2bWPsW8tZElGPvf9ohP3ntXJ65JERGqEQuE4/bB5J7e/tZDC4lL+dV0fRnRv43VJIiI1RqFwHN5fmMljU9NoFRPBxFsGcXLrJl6XJCJSoxQKx2Btzm7Gf7eBSfMzGNihOa9c24dmjcK9LktEpMYpFA5j5579fLIkmw8WZbI0s4DQEOPmwe15dOTJhIXqTF4RCUwKhYPMXJ3Le/MzmLEqh5IyR9c2Tfj9qG6M7t2WuOgIr8sTEfErhYJPebnjyc9XMv67DcRFh3PDwGQu7ZNAt7YaNxCR4KFQoOKag1+/t5gvl23lpsHJPDayqw4RiUhQCvpQ2LlnP7dOTGXR5p387vyu3Dqkg9cliYh4xpOvw2b2KzNbZmbLzew+X1szM5tuZmt9v2P9Xcfm7UVc+uoc0rIKePnqPgoEEQl6tR4KZtYduA3oB/QCRplZR+ARYIZzrhMww7fsN0sy8rnk1e/Yvmc/b9/an/N76iI0EREv9hS6AvOcc0XOuVLgW+ASYDTwpq/Pm8BF/ipg9to8rho3l8iwUD64Y5CmuxYR8fEiFJYBQ8ysuZlFASOBRKCVc26Lr89WoFVVTzazsWaWamapeXl5J1RAfNOGpCTHMvXOQXRsGX1CryEiEojMOVf7b2p2C3AnsAdYDhQDY5xzTSv12emcO+K4QkpKiktNTfVrrSIigcbMFjrnUqpa58lAs3PuDedcX+fcGcBOYA2QY2ZtAHy/c72oTUQkmHl19lFL3+8kKsYT3gE+AW70dbkR+NiL2kREgplX1yl8YGbNgRLgLudcvpk9DUz2HVraBFzhUW0iIkHLk1Bwzg2pom07cLYH5YiIiI/mchARkQMUCiIicoBCQUREDlAoiIjIAZ5cvFZTzCyPijOVTkQcsK0Gy6lPgnXbtd3BRdt9eO2ccy2qWlGvQ6E6zCz1cFf0Bbpg3XZtd3DRdp8YHT4SEZEDFAoiInJAMIfCOK8L8FCwbru2O7hou09A0I4piIjIoYJ5T0FERA6iUBARkQOCMhTMbISZrTazdDPz672gvWRm480s18yWVWprZmbTzWyt7/cRb2RUH5lZopnNNLMVZrbczH7law/obTezSDObb2ZLfNv9R197ezOb5/u8v2dm4V7X6g9mFmpmP5jZZ77lgN9uM9toZmlmttjMUn1t1fqcB10omFko8ApwHtANuNrMunlbld9MAEYc1PYIMMM51wmY4VsONKXA/c65bsAA4C7ff+NA3/Zi4CznXC+gNzDCzAYAzwAvOOc6UnFTq1s8rNGffgWsrLQcLNs9zDnXu9K1CdX6nAddKAD9gHTn3Hrn3H7gXWC0xzX5hXNuFrDjoObRwJu+x28CF9VqUbXAObfFObfI93g3FX8o4gnwbXcVCn2LYb4fB5wFvO9rD7jtBjCzBOB84HXfshEE230Y1fqcB2MoxAMZlZYzfW3BopVzbovv8VaglZfF+JuZJQOnAvMIgm33HUJZTMXtbKcD64B851ypr0ugft5fBB4Cyn3LzQmO7XbA12a20MzG+tqq9Tn36s5rUgc455yZBew5yWYWDXwA3Oec21Xx5bFCoG67c64M6G1mTYEPgZM9LsnvzGwUkOucW2hmQ72up5ad7pzL8t3ieLqZraq88kQ+58G4p5AFJFZaTvC1BYscM2sD4Pud63E9fmFmYVQEwtvOuam+5qDYdgDnXD4wExgINDWzH78ABuLnfTBwoZltpOJw8FnA3wn87cY5l+X7nUvFl4B+VPNzHoyhsADo5DszIRy4CvjE45pq0yfAjb7HNwIfe1iLX/iOJ78BrHTOPV9pVUBvu5m18O0hYGYNgXOoGE+ZCVzm6xZw2+2ce9Q5l+CcS6bi/+f/OueuJcC328wamVnjHx8Dw4FlVPNzHpRXNJvZSCqOQYYC451zT3lckl+Y2SRgKBVT6eYAjwMfAZOBJCqmHb/COXfwYHS9ZmanA7OBNH46xvwYFeMKAbvtZtaTioHFUCq+8E12zv3JzDpQ8Q26GfADcJ1zrti7Sv3Hd/joAefcqEDfbt/2fehbbAC845x7ysyaU43PeVCGgoiIVC0YDx+JiMhhKBREROQAhYKIiBygUBARkQMUCiIicoBCQUREDlAoiIjIAf8/5wQj1sKCiywAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Ftheta_NN(nn.Module):\n",
        "  def __init__(self, assets):\n",
        "    super(Ftheta_NN, self).__init__()\n",
        "    H = assets + 40\n",
        "    self.bn0 = nn.BatchNorm1d(num_features=assets)\n",
        "    self.layer1 = nn.Linear(assets, H)\n",
        "    self.leakyReLU = nn.LeakyReLU(0.5)\n",
        "    self.Softplus = nn.Softplus()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.tanh = nn.Tanh()\n",
        "    self.relu = nn.ReLU()\n",
        "    self.bn1 = nn.BatchNorm1d(num_features=H)\n",
        "    self.layer2 = nn.Linear(H, H)\n",
        "    self.bn2 = nn.BatchNorm1d(num_features=H)\n",
        "    self.layer3 = nn.Linear(H, 1)\n",
        "    self.bn3 = nn.BatchNorm1d(num_features=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.bn0(x)\n",
        "    x = self.layer1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.bn2(x)\n",
        "    x = self.layer3(x)\n",
        "    x = self.sigmoid(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "class Ftheta_NN(nn.Module):\n",
        "  def __init__(self, assets, hidden_size):\n",
        "    super(Ftheta_NN, self).__init__()\n",
        "    self.l1 = nn.Linear(assets, hidden_size) \n",
        "    self.relu = nn.ReLU()\n",
        "    self.l2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.l3 = nn.Linear(hidden_size, 1)  \n",
        "    self.sigmoid=nn.Sigmoid()\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = self.l1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.l2(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.l3(out)\n",
        "    out = self.sigmoid(out)\n",
        "    return out\n",
        "'''\n",
        "# set initial weights of a linear layer of the NN with uniform values and bias=0.01 (or choose zero initial weights)\n",
        "def init_weights(m):\n",
        "  if isinstance(m, torch.nn.Linear):\n",
        "    torch.manual_seed(42)\n",
        "    # torch.nn.init.zeros_(m.weight)\n",
        "    torch.nn.init.xavier_uniform_(m.weight)\n",
        "    m.bias.data.fill_(0.01)"
      ],
      "metadata": {
        "id": "cmfIHVbswjlt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class \"Profit\"\n",
        "This class contains various payoff and costs elements that define the reward. The final profit value is computed for each date and path.\n",
        "\n",
        "### terminal reward\n",
        "The terminal function $\\Gamma$ is set to an option payoff function of choice regardless of the regime in which the process is at, in this case we have a Max Call. (other choices can be made as well). The terminal payoff is received at maturity, with no other costs nor payoffs.\n",
        "\\begin{equation}\n",
        "\\Gamma(n) = \\Big(\\max_{i \\in \\{1, \\ldots, d \\}} x^i - K   \\Big)^{+} \\tag{1}\n",
        "\\end{equation}\n",
        "\n",
        "### running reward\n",
        "The function $\\Psi_q = (\\Psi_q(n))_{n \\in \\mathbb{N}}$ represents the running reward received while in mode $q \\in \\mathbb{I}$. \n",
        "\\begin{equation}\n",
        "\\Psi_q(n) = \\Big[\\Big(\\max_{i \\in \\{1, \\ldots, d\\}} x^i_n - K   \\Big)^{+} \\Big]^{k_q} \\;\\;\\;\\;\\; k \\in \\{.4, .7 \\} \\tag{2}\n",
        "\\end{equation}\n",
        "\n",
        "### switching cost\n",
        "The function $\\gamma_{i, j} = (\\gamma_{i, j}(n))_{n \\in \\mathbb{N}}$ with $i,j \\in \\mathbb{I} = \\{0, 1 \\}$ represents the cost for switching from mode $i \\in \\mathbb{I}$ to mode $j \\in \\mathbb{I}$.\n",
        "\\begin{equation} \\tag{3}\n",
        "\\gamma_{0,0} \\equiv \\gamma_{1,1} \\equiv 0 \\\\\n",
        "\\gamma_{0,1}(n) = \\Big(\\max_{i \\in \\{1, \\ldots, d \\}} x^i - K   \\Big)^{+} + \\delta  \\;\\;\\;\\;\\; \\delta = .7   \\\\ \n",
        "\\gamma_{1, 0}(n) = \\Big(\\max_{i \\in \\{1, \\ldots, d \\}} x^i - K   \\Big)^{+} \n",
        "\\end{equation}\n",
        "\n",
        "### the full expression for the profit\n",
        "The entire expression for the value of the process at each time $n$ can be represented as: \n",
        "\\begin{equation} \\tag{4}\n",
        "\\check{Y}_{N}^i = \\Gamma \\\\\n",
        "\\check{Y}_{n}^i = \\Psi_i(n) + \\max_{j \\in \\{0, 1 \\}} \n",
        "\\{- \\gamma_{i, j}(n) + e^{-\\rho h} \\check{Y}_{n+1}^j   \\} \\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\text{for } n=N-1, \\ldots, 0\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "### Note\n",
        "There are actually two profit classes:\n",
        "- Profit_training (actually only the function \"running\") is used to compute $(\\check{Y}_{n}^i, \\check{Y}_{n}^j)$ at each date which is needed to in the loss function for the optimization of the parameters. As the optimization of the parameters will tell about switching/not I have to assume the profit can occur under both regimes [which is not fully correct then]\n",
        "- Profit_testing (function \"running\") is used to compute $(\\check{Y}_{n}^i), i \\in \\mathbb{I}$, once the new regime of the system is known  \n"
      ],
      "metadata": {
        "id": "bBR2f-4mYFZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Profit_training:\n",
        "  def __init__(self, model):\n",
        "    self.strike = model.strike\n",
        "\n",
        "  def terminal(self, X):\n",
        "    payoff = np.max(X, axis=1) - self.strike\n",
        "    return payoff.clip(0, None)\n",
        "\n",
        "  def g(self, date,path,X):\n",
        "    X=torch.from_numpy(X).float()\n",
        "    max1=torch.max(X[int(date) , path , : ].float()-S.strike)\n",
        "    return np.exp(-S.drift*S.dt*date)*torch.max(max1,torch.tensor([0.0])) \n",
        " \n",
        "\n",
        "  def running(self, Y, X):\n",
        "    gamma = np.array([-self.terminal(X), self.terminal(X) + 0.7]) # there are two rows, the first for \\gamma_{0,1}, the second for \\gamma_{1,0}\n",
        "    r_benefit = self.terminal(X)\n",
        "    return torch.from_numpy(r_benefit+Y-gamma)  \n",
        " \n",
        "class Profit_testing:\n",
        "  def __init__(self, model):\n",
        "    self.strike = model.strike\n",
        "\n",
        "  def terminal(self, X):\n",
        "    terminal = np.max(X, axis=1) - self.strike\n",
        "    return terminal.clip(0, None)\n",
        "\n",
        "  def g(self, date,path,X):\n",
        "    X=torch.from_numpy(X).float()\n",
        "    max1=torch.max(X[int(date) , path , : ].float()-S.strike)\n",
        "    return np.exp(-S.drift*S.dt*date)*torch.max(max1,torch.tensor([0.0])) \n",
        "\n",
        "\n",
        "  # switch is F_theta_train \n",
        "  def running(self, Y, date, path, S, X, switch, gamma):\n",
        "    val=Y[date, path]- gamma  \n",
        "    k = np.array([0.4, 0.7])\n",
        "    r_benefit = self.g(date, path, X)\n",
        "    return val*int(switch[date, path])+r_benefit.numpy()"
      ],
      "metadata": {
        "id": "OK5Zq9ly3PQs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class \"OptimizationPart\"\n",
        "This class contains the building blocks necessary to train the neural network which allows to perform a gradient ascent optimization algorithm on the parameters $\\theta_n$, thus approximating the stopping times $\\tau_{n+1}$.\n",
        "The main components of this class are:\n",
        "\n",
        "### 1) train_network (the loss function) \n",
        "\n",
        "In order to use the Back Propagation algorithm from Pytorch, the maximization task is transformed into minimizing the negative of the objective function.\n",
        "\n",
        "\\begin{equation}\n",
        "L = -\\frac{1}{M} \\sum_{m=1}^M r_n^m (\\theta_n) \\tag{5}\n",
        "\\end{equation}\n",
        "\n",
        "where we specify the reward function as being\n",
        "\\begin{equation}\n",
        "r_n^m (\\theta_n) = g(n, x_n^m)F^{\\theta}(x_n^m)+ g(\\bar{\\tau}_{n+1}^m, x_{\\bar{\\tau}_{n+1}^m}^m)(1-F^{\\theta}(x_n^m) ) \\tag{6}\n",
        "\\end{equation}\n",
        "where the $g()$ function is $(3)$.\n",
        "\n",
        "### *To recap*\n",
        "At time $n$ along path $m$, the reward is equal to $(2)$ taking into account the possible events of switching or not regime with probability given by $F^{\\theta_n}$.\n",
        "\n",
        "For a large number of paths $M$, we can say that $(1)$ approximates $\\mathbb{E}[g(n, X_n)f^{\\theta}(X_n)+ g(\\tau_{n+1}, X_{\\tau_{n+1}})(1-f^{\\theta}(X_n) ) ]$. This is the function we optimize using gradient descent algorithm with respect to $\\theta_n$. The algorithm outputs the probability values of switching $F^{\\theta_n}$ for each date and for all paths considered, which we then translate into $0-1$ switching decisions $f^{\\theta_n}$.\n",
        "\n",
        "For each epoch, we use the set $\\theta_n$ from the previous epoch (where the very first set is initialized according to a uniform distribution) to compute $F^{\\theta_n}$. Then the updated (\"loss.step()\") $\\theta_n$ are obtained via backpropagation by the gradient of the loss function (\"loss.backward()\"). \n",
        "\n",
        "\n",
        "### 2) evaluate_network\n",
        "After training, the algorithm proceeds with the evaluation phase where it outputs the probability values of switching $F^{\\theta_n}$. These corresponds to the object \"stopping_probability\"."
      ],
      "metadata": {
        "id": "24zW3TAlMNxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OptimizationPart(object):\n",
        "\n",
        "  def __init__(self, assets, paths, epochs=50, batch_size=2000):\n",
        "    self.assets = assets\n",
        "    self.paths = paths\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = batch_size\n",
        "    self.network = Ftheta_NN(self.assets).double()\n",
        "    self.network.apply(init_weights)\n",
        "\n",
        "\n",
        "  def train_network(self,  stock_values, current_payoff,\n",
        "                    future_payoff):\n",
        "    optimizer = optim.Adam(self.network.parameters())\n",
        "    #future_payoff = torch.from_numpy(future_payoff).double()\n",
        "    #current_payoff = torch.from_numpy(current_payoff).double()\n",
        "    X_inputs = torch.from_numpy(stock_values).double()\n",
        "\n",
        "    self.network.train(True)\n",
        "    ones = torch.ones(self.paths)\n",
        "    for epoch in range(self.epochs):\n",
        "      optimizer.zero_grad()\n",
        "      outputs = self.network(X_inputs).reshape(-1) # probabilities\n",
        "      reward = (current_payoff * outputs ) +future_payoff * (ones - outputs) # reward function\n",
        "      loss = -torch.mean(reward) # loss function\n",
        "      loss.backward() # gradient calculation of the loss function\n",
        "      optimizer.step() # gradient descent update\n",
        "\n",
        "  def evaluate_network(self, X_inputs):\n",
        "    self.network.train(False)\n",
        "    X_inputs = torch.from_numpy(X_inputs).double()\n",
        "    outputs = self.network(X_inputs)\n",
        "    return outputs.view(X_inputs.size()[0]).detach().numpy()\n",
        "   "
      ],
      "metadata": {
        "id": "IA1la3-vzY85"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class \"Recursion\"\n",
        "This class contains the main calculations of the algorithm, hence the recursion.\n",
        "\n",
        "### price\n",
        "First we start with some elements:\n",
        "1. we simulate $d$ asset prices $\\{X^i \\}^d_{i=1}$ along $m$ paths according to a geometric Brownian motion process. We consider the iys discretized version on an equidistant time grid, $t_n = n \\cdot T/N$ for $n=0, \\ldots, N$:\n",
        "\\begin{equation} \\tag{7}\n",
        "x_{n,i}^m = x_{0,i} \\cdot \\exp \\Big\\{\\sum_{k=0}^n \\Big((r-\\delta_i - \\sigma_i^2 /2) \\Delta t + \\sigma_i \\sqrt{\\Delta t} \\cdot Z_{k, i}^m  \\Big) \\Big\\}\n",
        "\\end{equation}\n",
        "where $(r-\\delta_i) \\in \\mathbb{R}$ and $\\sigma_i >0$ are the drift and volatility of the system $X$, $\\Delta t =T/N$ and $Z_{k, i}^m \\sim \\mathcal{N}(0,1)$\n",
        "\n",
        "2. we define the discount factor $\\exp \\{(r-\\delta_i )\\Delta t \\}$\n",
        "3. we create the \"regime\" object $\\mathbb{I} = \\{0, 1 \\}$\n",
        "4. we create the empty objects: \"regime_path\" that records the regime in which the process is at for every time step and path; \"Y_train\" that records the profit value for each time step and path; \"F_theta_train\" that records the presence of a stopping time\n",
        "\n",
        "\n",
        "Then we can start the recursion:\n",
        "\n",
        "*At maturity $N$*\n",
        "1. we sample a regime to specify in which regime the process is and record it in \"regime_path\". The sampled regime is the same across all paths\n",
        "2. we compute the profit at $N$ \"final_payoff\" $\\check{Y}_{N}^i$ according to $(4)$ and record it in \"Y_train\"\n",
        "\n",
        "*Before maturity, for each date $ n=N-1, \\ldots, 0$*\n",
        "1. we compute \"current_payoff\" using \"Profit_training.running()\"\n",
        "2. we obtain a stopping rule for each path, using as arguments the current payoff, the discounted final payoff (now called \"values\") and the entire process. Record then the stopping rules in \"F_theta_train\" **[I am not sure about the discounting]**\n",
        "3. we determine the current regime on the basis of the regime from the previous time step and if there has been or not a stopping time. Hence if at $n$ we have $F_n^{\\theta}=1$ and $q_{n+1} = i$, we choose $q_n=j$. We then record this in \"regime_path\"\n",
        "4. we compute $\\check{Y}_{n}^i$ according to $(4)$, using \"Profit_training.running()\" appropriate for its current regime and record it under \"Y_train\"\n",
        "\n",
        "Hence we have a matrix with dimension periods x paths, recording $\\check{Y}_{n}^i$.\n",
        "\n",
        "The final price will be $max \\big\\{ \\check{Y}_{0}, \\frac{1}{M}\\sum_{n=1}^N \\check{Y}_{n} \\big\\} $\n",
        "\n",
        "### stop\n",
        "This function combines the function sin \"OptimizationPart\". It is called at every time step and it produces stopping probabilities for each date across all paths. The arguments it takes are those we see also in the loss function, hence:\n",
        "- current profit $g(n, x_n^m)$\n",
        "- future profit $g(\\bar{\\tau}_{n+1}^m, x_{\\bar{\\tau}_{n+1}^m}^m)$\n",
        "- the entire process\n"
      ],
      "metadata": {
        "id": "5anu8qXoXXOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Recursion:\n",
        "  def __init__(self, model, training, testing, nb_epochs=50):\n",
        "\n",
        "    self.model = model # argument is S    \n",
        "    self.neural_stopping = OptimizationPart(model.assets, model.paths) \n",
        "    self.profit_training = Profit_training(self.model)\n",
        "    self.profit_testing = Profit_testing(self.model)\n",
        "\n",
        "  def price(self):\n",
        "    model = self.model\n",
        "    stock_paths = self.model.GBM()    \n",
        "    disc_factor = np.math.exp((-model.drift) * model.maturity/(model.periods))\n",
        "    \n",
        "    # create empty objects to store values\n",
        "    k = np.array([0.4, 0.7])\n",
        "    regimes = [0, 1]\n",
        "    regime_path=np.zeros((model.periods+1, model.paths)) # record at which regime we're at at each n\n",
        "    Y_train=np.zeros((model.periods+1, model.paths))\n",
        "    F_theta_train=np.zeros((model.periods+1,model.paths)) # record switching events for each n\n",
        "\n",
        "    # at maturity N\n",
        "    final_payoff = np.array([self.profit_training.terminal(stock_paths[-1, :, :]), self.profit_training.terminal(stock_paths[-1, :, :])])   # payoff of the last date for each path.\n",
        "    future_payoff = torch.from_numpy(final_payoff*disc_factor).double() \n",
        "    Y_train[model.periods, :]= final_payoff[0]\n",
        "    F_theta_train[model.periods,:]=1 # at maturity we switch (does it matter?)\n",
        "    regime_path[model.periods, :] = random.sample(regimes, 1)[0] # sample a regime at maturity\n",
        "    values = Y_train[model.periods, :]\n",
        "\n",
        "    # recursive calc. before maturity\n",
        "    for date in range(stock_paths.shape[0] - 2, 0, -1): \n",
        "      current_payoff = self.profit_training.running(Y_train[date+1, :], stock_paths[date, :, :])\n",
        "\n",
        "      stopping_probability = self.stop(stock_paths[date, : , :], \n",
        "                                current_payoff,\n",
        "                                future_payoff)\n",
        "      \n",
        "      print(date, \":\", np.min(stopping_probability),\" , \", np.max(stopping_probability), \" , \", len([1 for l in stopping_probability if l > 0.5])) # print the min/max stopping probabilities and the count of stopping times for each date\n",
        "      F_theta_train[date,:]=(stopping_probability > 0.5)*1.0   # transform stopping probabilities in 0-1 decision\n",
        "      which = stopping_probability > 0.5\n",
        "\n",
        "      for m in range(0,model.paths-1):\n",
        "        old_regime = regime_path[date +1, m]\n",
        "        if  which[m] == True :\n",
        "          regime_path[date , m] = regimes[~int(old_regime)] \n",
        "          gamma = self.profit_testing.g(date, m, X) + 0.7\n",
        "        else:\n",
        "          regime_path[date , m] = old_regime\n",
        "          gamma = -self.profit_testing.g(date, m, X)\n",
        "        Y_train[date, m] = self.profit_testing.running(Y = Y_train, date = date, \n",
        "                                                path = m, S=model, \n",
        "                                                X = stock_paths,  switch = F_theta_train, \n",
        "                                                gamma = gamma)\n",
        "      immediate_exercise_value = Y_train[date, :]       \n",
        "      values[which] = immediate_exercise_value[which] # when we switch we take the current profit\n",
        "      values[~which] *= disc_factor           # when we don't switch we take final profit discounted \n",
        "\n",
        "\n",
        "    payoff_0 = Y_train[0, :]\n",
        "    return max(payoff_0[0], np.mean(values) * disc_factor), payoff_0, values \n",
        "\n",
        "\n",
        "\n",
        "  def stop(self, stock_values, current_payoff,\n",
        "           future_payoff):\n",
        "    \n",
        "    self.neural_stopping.train_network(\n",
        "      stock_values,\n",
        "      current_payoff ,\n",
        "      future_payoff)\n",
        "\n",
        "    inputs = stock_values\n",
        "    stopping_probability = self.neural_stopping.evaluate_network(inputs)\n",
        "    return stopping_probability    "
      ],
      "metadata": {
        "id": "N2Tje8ll4AUa"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pricing = Recursion(S, Profit_training, Profit_testing, nb_epochs=50)\n",
        "\n",
        "'''\n",
        "arguments are:\n",
        "- path process\n",
        "- Profit training and profit testing classes\n",
        "- number of epochs to be used for the gradient descent algorithm\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "3UmjnYsLWHLM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cced5b2b-39e0-44d1-e5b2-ea2982d6e10f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\narguments are:\\n- path process\\n- Profit training and profit testing classes\\n- number of epochs to be used for the gradient descent algorithm\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output, payoff_0, values = pricing.price()\n",
        "\n",
        "'''\n",
        "here we are printing:\n",
        "- date\n",
        "- min and max probability of switching across the 5000 paths\n",
        "- number paths that will switch for each date\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xf_lbl45l91-",
        "outputId": "b6f2b703-e4a7-472e-fe6d-685d1a777590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48 : 7.180927217196726e-07  ,  0.9827728708565249  ,  3769\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7PJT61dgfvY",
        "outputId": "f81bba62-fcb9-43fd-d909-0ae62fba67bb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "in case I need to check steps\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "model = S # argument is S    \n",
        "profit_training = Profit_training(model) #class profit. the argument is Profit, then you can call profit.terminal, etc\n",
        "epochs = 50\n",
        "stock_paths = model.GBM()    \n",
        "disc_factor = np.math.exp((-model.drift) * model.maturity/(model.periods))\n",
        "\n",
        "k = np.array([0.4, 0.7])\n",
        "regimes = [0, 1]\n",
        "regime_path=np.zeros((S.periods+1, S.paths)) # record at which regime we're at at each n\n",
        "Y_train=np.zeros((S.periods+1, S.paths))\n",
        "F_theta_train=np.zeros((S.periods+1,S.paths)) # record switching events for each n\n",
        "\n",
        "# at maturity N\n",
        "final_payoff = np.array([profit_training.terminal(stock_paths[-1, :, :]), profit_training.terminal(stock_paths[-1, :, :])])   # payoff of the last date for each path. \n",
        "Y_train[S.periods, :]= final_payoff[0]\n",
        "\n",
        "F_theta_train[S.periods,:]=1 # at maturity we switch (does it matter?)\n",
        "regime_path[S.periods, :] = random.sample(regimes, 1)[0] # sample a regime at maturity\n",
        "neural_stopping = OptimizationPart(model.assets, model.paths) \n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "date = stock_paths.shape[0]-2\n",
        "current_payoff =  profit_training.running(Y_train[date+1, :], stock_paths[date, :, :])\n",
        "\n",
        "network = Ftheta_NN(model.assets).double()\n",
        "network.apply(init_weights)\n",
        "optimizer = optim.Adam(network.parameters())\n",
        "print(type(final_payoff))\n",
        "future_payoff = torch.from_numpy(final_payoff*disc_factor).double()\n",
        "X_inputs = torch.from_numpy(stock_paths[date, :, :]).double()\n",
        "print(\"final\", torch.from_numpy(final_payoff).double())\n",
        "print(\"current\", current_payoff.size(),current_payoff)\n",
        "print(\"X_inopts\", X_inputs)\n",
        "print(\"values\", final_payoff)\n",
        "print(\"future\", future_payoff.size(), future_payoff)\n",
        "print(network(X_inputs))\n",
        "network.train(True)\n",
        "ones = torch.ones(S.paths)\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  optimizer.zero_grad()\n",
        "  outputs = network(X_inputs).reshape(-1) # probabilities\n",
        "  reward = (current_payoff * outputs +\n",
        "              future_payoff * (ones - outputs)) # reward function\n",
        "  print(len(outputs) ,type(outputs), len(current_payoff), len(future_payoff))            \n",
        "  loss = -torch.mean(reward) # loss function\n",
        "  loss.backward() # gradient calculation of the loss function\n",
        "  optimizer.step() # gradient descent update\n",
        "\n",
        "\n",
        "network.train(False)\n",
        "X_inputs = torch.from_numpy(stock_paths[date, :, :]).double()\n",
        "outputs = network(X_inputs)\n",
        "stopping_probablities =outputs.view(X_inputs.size()[0]).detach().numpy()\n",
        "print(stopping_probablities)\n",
        "\n",
        "print(date, \":\", np.min(stopping_probablities),\" , \", np.max(stopping_probablities), \" , \", len([1 for l in stopping_probablities if l > 0.5]))\n",
        "F_theta_train[date,:]=(stopping_probablities > 0.5)*1.0   \n",
        "which = stopping_probablities > 0.5\n",
        "print(len(stopping_probablities), model.paths)\n",
        "profit_testing = Profit_testing(model)\n",
        "print(which)\n",
        "for m in range(0, model.paths):\n",
        "         old_regime = regime_path[date +1, m]\n",
        "         if  which[m] == True :\n",
        "           regime_path[date , m] = regimes[~int(old_regime)] \n",
        "           gamma = profit_testing.g(date, m, stock_paths) + 0.7\n",
        "         else:\n",
        "           regime_path[date , m] = old_regime\n",
        "           gamma = -profit_testing.g(date, m, X)\n",
        "         Y_train[date, m] = profit_testing.running(Y = Y_train, date = date, \n",
        "                                                path = m, S=model, \n",
        "                                                X = stock_paths,  switch = F_theta_train, \n",
        "                                                gamma = gamma)\n",
        "values = Y_train[S.periods, :]\n",
        "immediate_exercise_value = Y_train[date, :]       \n",
        "values[which] = immediate_exercise_value[which] # when we switch we take the current profit\n",
        "values[~which] *= disc_factor           # when we don't switch we take final profit discounted (?)\n",
        "\n",
        "print(\"immediate\", immediate_exercise_value)\n",
        "print(\"values\", values)\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "jugaRK4rXBDR",
        "outputId": "f1a4b615-c2d3-43ac-c613-96dc779d318b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nmodel = S # argument is S    \\nprofit_training = Profit_training(model) #class profit. the argument is Profit, then you can call profit.terminal, etc\\nepochs = 50\\nstock_paths = model.GBM()    \\ndisc_factor = np.math.exp((-model.drift) * model.maturity/(model.periods))\\n\\nk = np.array([0.4, 0.7])\\nregimes = [0, 1]\\nregime_path=np.zeros((S.periods+1, S.paths)) # record at which regime we\\'re at at each n\\nY_train=np.zeros((S.periods+1, S.paths))\\nF_theta_train=np.zeros((S.periods+1,S.paths)) # record switching events for each n\\n\\n# at maturity N\\nfinal_payoff = np.array([profit_training.terminal(stock_paths[-1, :, :]), profit_training.terminal(stock_paths[-1, :, :])])   # payoff of the last date for each path. \\nY_train[S.periods, :]= final_payoff[0]\\n\\nF_theta_train[S.periods,:]=1 # at maturity we switch (does it matter?)\\nregime_path[S.periods, :] = random.sample(regimes, 1)[0] # sample a regime at maturity\\nneural_stopping = OptimizationPart(model.assets, model.paths) \\n\\n    \\n\\n\\ndate = stock_paths.shape[0]-2\\ncurrent_payoff =  profit_training.running(Y_train[date+1, :], stock_paths[date, :, :])\\n\\nnetwork = Ftheta_NN(model.assets).double()\\nnetwork.apply(init_weights)\\noptimizer = optim.Adam(network.parameters())\\nprint(type(final_payoff))\\nfuture_payoff = torch.from_numpy(final_payoff*disc_factor).double()\\nX_inputs = torch.from_numpy(stock_paths[date, :, :]).double()\\nprint(\"final\", torch.from_numpy(final_payoff).double())\\nprint(\"current\", current_payoff.size(),current_payoff)\\nprint(\"X_inopts\", X_inputs)\\nprint(\"values\", final_payoff)\\nprint(\"future\", future_payoff.size(), future_payoff)\\nprint(network(X_inputs))\\nnetwork.train(True)\\nones = torch.ones(S.paths)\\n\\n\\nfor epoch in range(epochs):\\n  optimizer.zero_grad()\\n  outputs = network(X_inputs).reshape(-1) # probabilities\\n  reward = (current_payoff * outputs +\\n              future_payoff * (ones - outputs)) # reward function\\n  print(len(outputs) ,type(outputs), len(current_payoff), len(future_payoff))            \\n  loss = -torch.mean(reward) # loss function\\n  loss.backward() # gradient calculation of the loss function\\n  optimizer.step() # gradient descent update\\n\\n\\nnetwork.train(False)\\nX_inputs = torch.from_numpy(stock_paths[date, :, :]).double()\\noutputs = network(X_inputs)\\nstopping_probablities =outputs.view(X_inputs.size()[0]).detach().numpy()\\nprint(stopping_probablities)\\n\\nprint(date, \":\", np.min(stopping_probablities),\" , \", np.max(stopping_probablities), \" , \", len([1 for l in stopping_probablities if l > 0.5]))\\nF_theta_train[date,:]=(stopping_probablities > 0.5)*1.0   \\nwhich = stopping_probablities > 0.5\\nprint(len(stopping_probablities), model.paths)\\nprofit_testing = Profit_testing(model)\\nprint(which)\\nfor m in range(0, model.paths):\\n         old_regime = regime_path[date +1, m]\\n         if  which[m] == True :\\n           regime_path[date , m] = regimes[~int(old_regime)] \\n           gamma = profit_testing.g(date, m, stock_paths) + 0.7\\n         else:\\n           regime_path[date , m] = old_regime\\n           gamma = -profit_testing.g(date, m, X)\\n         Y_train[date, m] = profit_testing.running(Y = Y_train, date = date, \\n                                                path = m, S=model, \\n                                                X = stock_paths,  switch = F_theta_train, \\n                                                gamma = gamma)\\nvalues = Y_train[S.periods, :]\\nimmediate_exercise_value = Y_train[date, :]       \\nvalues[which] = immediate_exercise_value[which] # when we switch we take the current profit\\nvalues[~which] *= disc_factor           # when we don\\'t switch we take final profit discounted (?)\\n\\nprint(\"immediate\", immediate_exercise_value)\\nprint(\"values\", values)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}