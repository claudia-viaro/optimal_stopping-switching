{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "optimal_switching_V1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOqDkliiN9wSHREJW3gXXUu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudia-viaro/optimal_stopping-switching/blob/main/optimal_switching_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbFaWc55v5HL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "np.random.seed(234198)\n",
        "import itertools\n",
        "import random\n",
        "import time\n",
        "import scipy.stats\n",
        "\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as tdata"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem Formulation\n",
        "Let $(\\Omega, \\mathcal{F}, \\mathbb{P})$ be a fixed probability space on which an adapted stochastic process is defined $X=(X_t)_{0 \\leq t \\leq T}$ whose natural filtration is $(\\mathcal{F}_t^0 := \\sigma \\{ X_s, s \\leq t \\})_{0 \\leq t \\leq T}$. Let $\\mathbf{F}=(\\mathcal{F}_0)_{0 \\leq t \\leq t}$ be the complete filtration of $(\\mathcal{F}_t^0 := \\sigma \\{ X_s, s \\leq t \\})_{0 \\leq t \\leq T}$. with $P$-null sets of $\\mathcal{F}$.\n",
        "\n",
        "The stochastic process $X$ is $\\mathbb{R}^d$-valued and represents the market price of $d$ financial assets (Bermudan call options). Assume $(X^i)_{i=1}^d$ follows a geometric Brownian motion satisfying the SDE:\n",
        "\\begin{equation}\n",
        "dX_t^i = (b-\\delta_i)dt + \\sigma_i dW_t^i\n",
        "\\end{equation}\n",
        "where $W$ is a standard Brownian otion on a filtered probability space $(\\Omega, \\mathcal{F}, (\\mathcal{F}_t)_{t \\geq 0}, \\mathbb{P})$ and $b$, $d_i$, $\\sigma_i >0$ are the drift, dividend yield and volatility of the system at time $t$.\n",
        "\n",
        "We will consider a discrete time approximization (Euler schema) on an equidistant time grid $0=t_0 < t_1 < \\ldots < t_N = T$, where $t_n = n \\cdot T/N$. For $i = 1, \\ldots, d$ we simulate $p$ paths\n",
        "\\begin{equation}\n",
        "x^p_{n,i} = x_{0,i} \\cdot \\exp \\Big\\{ \\sum_{k=0}^n \\big( (b-\\delta_i - \\sigma^2_i /2)\\Delta t + \\sigma_{i} \\sqrt{\\Delta t} \\cdot Z_{k, i}^p \\big)     \\Big\\}\n",
        "\\end{equation}\n",
        "where $\\Delta t = T/N$ and $Z_{k, i}^{p} \\sim \\mathcal{N} (0,1)$."
      ],
      "metadata": {
        "id": "89LzXc4edbqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' \n",
        "underlying process - Geometric brownian motion\n",
        "\n",
        "both functions here are equivalent\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BlackScholes:\n",
        "  def __init__(self, drift, sigma, delta, spot, assets,  paths, periods,\n",
        "         maturity, strike, dividend=0):\n",
        "\n",
        "    self.drift = drift - dividend\n",
        "    self.sigma = sigma\n",
        "    self.delta = delta\n",
        "    self.spot = spot\n",
        "    self.assets = assets\n",
        "    self.paths = paths\n",
        "    self.periods = periods\n",
        "    self.maturity = maturity\n",
        "    self.strike = strike\n",
        "    self.dt = self.maturity / self.periods\n",
        "    self.df = math.exp(-self.drift * self.dt)\n",
        "\n",
        "  def drift_fct(self, x, t):\n",
        "    del t\n",
        "    return self.drift * x\n",
        "\n",
        "  def diffusion_fct(self, x, t, v=0):\n",
        "    del t\n",
        "    return self.sigma * x\n",
        "\n",
        "\n",
        "\n",
        "  def simulate_process(self):\n",
        "    \"\"\"Returns a nparray (nb_paths * assets * nb_dates) with prices.\"\"\"\n",
        "    paths = self.paths\n",
        "    spot_paths = np.empty((self.periods+1, paths, self.assets ))\n",
        "\n",
        "    spot_paths[0, :, :] = self.spot\n",
        "    random_numbers = np.random.normal(\n",
        "        0, 1, (self.periods, paths, self.assets ))\n",
        "    dW = random_numbers * np.sqrt(self.dt)\n",
        "    drift = self.drift\n",
        "    r = np.repeat(np.repeat(np.repeat(\n",
        "        np.reshape(drift, (-1, 1, 1)), self.periods, axis=0),\n",
        "        paths, axis=1), self.assets, axis=2)\n",
        "    sig = np.ones((self.periods, paths, self.assets))*self.sigma\n",
        "    #sig = np.repeat(np.repeat(np.repeat(\n",
        "    #    np.reshape(self.sigma, (-1, 1, 1)), self.periods+1, axis=2),\n",
        "    #    paths, axis=1), self.assets, axis=0)\n",
        "    \n",
        "    spot_paths[1:, :,  :] = np.repeat(\n",
        "        spot_paths[0:1, :, :], self.periods, axis=0)* np.exp(np.cumsum((r-self.delta) * self.dt - (sig ** 2) * self.dt / 2 + sig * dW, axis=0))\n",
        "\n",
        "    return spot_paths #.reshape(spot_paths.shape[2], spot_paths.shape[0], spot_paths.shape[1])\n",
        "\n",
        "\n",
        "\n",
        "class GBM:\n",
        "    def __init__(self, drift, sigma, delta, spot, assets,  paths, periods,\n",
        "         maturity, strike = 100,dividend=0):\n",
        "        self.maturity = maturity\n",
        "        self.strike = strike\n",
        "        self.assets = assets\n",
        "        self.sigma=sigma *np.ones(self.assets)\n",
        "        self.delta=delta\n",
        "        self.spot = spot*np.ones(self.assets)\n",
        "        self.drift = drift - dividend\n",
        "        self.paths = paths\n",
        "        self.periods = periods\n",
        "        self.dt = self.maturity / self.periods\n",
        "    \n",
        "    def simulate_process(self):\n",
        "        \n",
        "        dt = self.maturity / self.periods\n",
        "        So_vec=self.spot*np.ones((1,S.paths, S.assets))\n",
        "        \n",
        "        Z=np.random.standard_normal((self.periods,self.paths, self.assets))\n",
        "        s=self.spot*np.exp(np.cumsum((self.drift-self.delta-0.5*self.sigma**2)*dt+self.sigma*np.sqrt(dt)*Z, axis=0))\n",
        "        \n",
        "        s=np.append(So_vec, s, axis=0)\n",
        "        return s  \n",
        "    "
      ],
      "metadata": {
        "id": "Eba9zKxNwIv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PLOT\n",
        "'''\n",
        "\n",
        "def draw_stock_model(stockmodel):\n",
        "    stock_paths = stockmodel\n",
        "\n",
        "    # draw a path\n",
        "    one_path = stock_paths[:, 0, 0]\n",
        "    dates = np.array([i for i in range(len(one_path))])\n",
        "    plt.plot(dates, one_path, label='stock path')\n",
        "    plt.ylabel('Stock price')\n",
        "    plt.ylabel('Time')\n",
        "    plt.legend()\n",
        "    return plt.show()   "
      ],
      "metadata": {
        "id": "bjcflzhmwbOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "set the parameters of the process \n",
        "'''\n",
        "\n",
        "hyperparam_test_stock_models = {'drift': 0.2, 'sigma': 0.05, 'delta': 0.1,  'paths':5000, 'periods': 9, 'maturity': 3., 'strike' : 100,'assets':2,  'spot':110,}\n",
        "\n",
        "'''\n",
        "generate paths\n",
        "'''\n",
        "\n",
        "#S = GBM(**hyperparam_test_stock_models)\n",
        "#X=S.simulate_process()\n",
        "S=BlackScholes(**hyperparam_test_stock_models)\n",
        "X=S.simulate_process()\n",
        "\n",
        "'''\n",
        "generate paths\n",
        "'''\n",
        "\n",
        "print(X.shape) # (date, path, asset)\n",
        "draw_stock_model(X) "
      ],
      "metadata": {
        "id": "5GOfCwrFwbqc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "37304ad3-6594-45fe-cb73-6c5aca813db7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 5000, 2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Switching regime\n",
        "\n",
        "The stochastic system can operate in $m$ modes or regimes from the finite set $\\mathbb{I}=\\{1, \\ldots , q \\}$. In this case we consider $q=2$ with $\\mathbb{I}=\\{\\text{on}, \\text{off} \\}$. \n",
        "\n",
        "The regimes can be switched at a sequence of stopping times over a finite horizon $[0, \\ldots , T]$.\n",
        "\n",
        "There is a payoff rate per unit of time when the system is in mode $i \\in \\mathbb{I}$ at time $t$ as a mapping $\\Psi_i(t, X_t): \\Omega \\times [0, T] \\rightarrow \\mathbb{R}$. The payoff function for the call option used is of the form $( \\max_{i \\in \\{ 1, \\ldots , d \\}} X_t^i - K) ^{+}$, where $K$ is the strike price at any point in the time grid $0 = t_0 < t_1 < \\ldots < t_N = T$. the system also outputs a final reward for being in mode $i \\in \\mathbb{I}$ at time $T$ given by $\\Gamma_i$.\n",
        "\n",
        "There is a cost for switching from regime $i$ to $j$ given by the function $\\gamma_{i, j} : \\Omega \\times [0, T] \\rightarrow \\mathbb{R} $ to cover for the extra costs due to the change of the regime.\n",
        "\n",
        "A strategy $\\alpha$ for the power plant will be a combination of two sequences:\n",
        "- non decreasing sequence of $\\mathbb{F}$-stopping times $(\\tau_n)_{n \\geq 1}$, $n \\in \\mathbb{N} \\backslash \\{0\\}$, where at $\\tau_n$ the production is swithced from the current mode $i$ to $j$. we also assume: $\\tau_0=t$ and $\\tau_n \\leq \\tau_{n+1}$.\n",
        "- a sequence of indicators $(\\iota)_{n \\geq 1}$, $n \\in \\mathbb{N} \\backslash \\{0\\}$, $\\mathcal{F}_{\\tau_n}$- measurable valued in $\\mathbb{I}_q$. At time $t=\\tau_n$ the system is switched from the current regime $\\iota_{n-1}$ to $\\iota_{n}$, with $\\iota_{0}=i$.\n",
        "\n",
        "We denote by $\\mathcal{A}_{t, i}$ the set of admissible strategies to switch at time $\\tau_n$, $n \\geq 1$, from the current regime $\\iota_{n-1}$ to $\\iota_{n}$. \n"
      ],
      "metadata": {
        "id": "PLbRtpLxf-YK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neuarl Network\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1Z7-v5l_iE7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Ftheta_NN(nn.Module):\n",
        "  def __init__(self, assets):\n",
        "    super(Ftheta_NN, self).__init__()\n",
        "    H = assets + 40\n",
        "    self.bn0 = nn.BatchNorm1d(num_features=assets)\n",
        "    self.layer1 = nn.Linear(assets, H)\n",
        "    self.leakyReLU = nn.LeakyReLU(0.5)\n",
        "    self.Softplus = nn.Softplus()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.tanh = nn.Tanh()\n",
        "    self.relu = nn.ReLU()\n",
        "    self.bn1 = nn.BatchNorm1d(num_features=H)\n",
        "    self.layer2 = nn.Linear(H, H)\n",
        "    self.bn2 = nn.BatchNorm1d(num_features=H)\n",
        "    self.layer3 = nn.Linear(H, 1)\n",
        "    self.bn3 = nn.BatchNorm1d(num_features=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.bn0(x)\n",
        "    x = self.layer1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.bn2(x)\n",
        "    x = self.layer3(x)\n",
        "    x = self.sigmoid(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "class Ftheta_NN(nn.Module):\n",
        "  def __init__(self, assets, hidden_size):\n",
        "    super(Ftheta_NN, self).__init__()\n",
        "    self.l1 = nn.Linear(assets, hidden_size) \n",
        "    self.relu = nn.ReLU()\n",
        "    self.l2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.l3 = nn.Linear(hidden_size, 1)  \n",
        "    self.sigmoid=nn.Sigmoid()\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = self.l1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.l2(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.l3(out)\n",
        "    out = self.sigmoid(out)\n",
        "    return out\n",
        "'''\n",
        "# set initial weights of a linear layer of the NN with uniform values and bias=0.01 (or choose zero initial weights)\n",
        "def init_weights(m):\n",
        "  if isinstance(m, torch.nn.Linear):\n",
        "    torch.manual_seed(42)\n",
        "    # torch.nn.init.zeros_(m.weight)\n",
        "    torch.nn.init.xavier_uniform_(m.weight)\n",
        "    m.bias.data.fill_(0.01)"
      ],
      "metadata": {
        "id": "cmfIHVbswjlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class \"Profit\"\n",
        "This class contains various payoff and costs elements that define the reward. The final profit value is computed for each date and path.\n",
        "\n",
        "### terminal reward\n",
        "The terminal function $\\Gamma$ is set to an option payoff function of choice regardless of the regime in which the process is at, in this case we have a Max Call. (other choices can be made as well). The terminal payoff is received at maturity, with no other costs nor payoffs.\n",
        "\\begin{equation}\n",
        "\\Gamma(n) = \\Big(\\max_{i \\in \\{1, \\ldots, d \\}} x^i - K   \\Big)^{+} \\tag{1}\n",
        "\\end{equation}\n",
        "\n",
        "### running reward\n",
        "The function $\\Psi_q = (\\Psi_q(n))_{n \\in \\mathbb{N}}$ represents the running reward received while in mode $q \\in \\mathbb{I}$. \n",
        "\\begin{equation}\n",
        "\\Psi_q(n) = \\Big[\\Big(\\max_{i \\in \\{1, \\ldots, d\\}} x^i_n - K   \\Big)^{+} \\Big]^{k_q} \\;\\;\\;\\;\\; k \\in \\{.4, .7 \\} \\tag{2}\n",
        "\\end{equation}\n",
        "\n",
        "### switching cost\n",
        "The function $\\gamma_{i, j} = (\\gamma_{i, j}(n))_{n \\in \\mathbb{N}}$ with $i,j \\in \\mathbb{I} = \\{0, 1 \\}$ represents the cost for switching from mode $i \\in \\mathbb{I}$ to mode $j \\in \\mathbb{I}$.\n",
        "\\begin{equation} \\tag{3}\n",
        "\\gamma_{0,0} \\equiv \\gamma_{1,1} \\equiv 0 \\\\\n",
        "\\gamma_{0,1}(n) = \\Big(\\max_{i \\in \\{1, \\ldots, d \\}} x^i - K   \\Big)^{+} + \\delta  \\;\\;\\;\\;\\; \\delta = .7   \\\\ \n",
        "\\gamma_{1, 0}(n) = \\Big(\\max_{i \\in \\{1, \\ldots, d \\}} x^i - K   \\Big)^{+} \n",
        "\\end{equation}\n",
        "\n",
        "### the full expression for the profit\n",
        "The entire expression for the value of the process at each time $n$ can be represented as: \n",
        "\\begin{equation} \\tag{4}\n",
        "\\check{Y}_{N}^i = \\Gamma \\\\\n",
        "\\check{Y}_{n}^i = \\Psi_i(n) + \\max_{j \\in \\{0, 1 \\}} \n",
        "\\{- \\gamma_{i, j}(n) + e^{-\\rho h} \\check{Y}_{n+1}^j   \\} \\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\text{for } n=N-1, \\ldots, 0\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "### Note\n",
        "There are actually two profit classes:\n",
        "- Profit_training (actually only the function \"running\") is used to compute $(\\check{Y}_{n}^i, \\check{Y}_{n}^j)$ at each date which is needed to in the loss function for the optimization of the parameters. As the optimization of the parameters will tell about switching/not I have to assume the profit can occur under both regimes [which is not fully correct then]\n",
        "- Profit_testing (function \"running\") is used to compute $(\\check{Y}_{n}^i), i \\in \\mathbb{I}$, once the new regime of the system is known  \n"
      ],
      "metadata": {
        "id": "bBR2f-4mYFZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Profit_training:\n",
        "  def __init__(self, model):\n",
        "    self.strike = model.strike\n",
        "\n",
        "  def terminal(self, X):\n",
        "    payoff = np.max(X, axis=1) - self.strike\n",
        "    return payoff.clip(0, None)\n",
        "\n",
        "  def g(self, date,path,X):\n",
        "    X=torch.from_numpy(X).float()\n",
        "    max1=torch.max(X[int(date) , path , : ].float()-S.strike)\n",
        "    return np.exp(-S.drift*S.dt*date)*torch.max(max1,torch.tensor([0.0])) \n",
        " \n",
        "\n",
        "  def running(self, Y, X):\n",
        "    gamma = np.array([-self.terminal(X), self.terminal(X) + 0.7]) # there are two rows, the first for \\gamma_{0,1}, the second for \\gamma_{1,0}\n",
        "    r_benefit = self.terminal(X)\n",
        "    return torch.from_numpy(r_benefit+Y-gamma)  \n",
        " \n",
        "class Profit_testing:\n",
        "  def __init__(self, model):\n",
        "    self.strike = model.strike\n",
        "\n",
        "  def terminal(self, X):\n",
        "    terminal = np.max(X, axis=1) - self.strike\n",
        "    return terminal.clip(0, None)\n",
        "\n",
        "  def g(self, date,path,X):\n",
        "    X=torch.from_numpy(X).float()\n",
        "    max1=torch.max(X[int(date) , path , : ].float()-S.strike)\n",
        "    return np.exp(-S.drift*S.dt*date)*torch.max(max1,torch.tensor([0.0])) \n",
        "\n",
        "\n",
        "  # switch is F_theta_train \n",
        "  def running(self, Y, date, path, S, X, switch, gamma):\n",
        "    val=Y[date+1, path]- gamma  \n",
        "    k = np.array([0.4, 0.7])\n",
        "    r_benefit = self.g(date, path, X)\n",
        "    return val*int(switch[date, path])+r_benefit.numpy()"
      ],
      "metadata": {
        "id": "OK5Zq9ly3PQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class \"OptimizationPart\"\n",
        "This class contains the building blocks necessary to train the neural network which allows to perform a gradient ascent optimization algorithm on the parameters $\\theta_n$, thus approximating the stopping times $\\tau_{n+1}$.\n",
        "The main components of this class are:\n",
        "\n",
        "### 1) train_network (the loss function) \n",
        "\n",
        "In order to use the Back Propagation algorithm from Pytorch, the maximization task is transformed into minimizing the negative of the objective function.\n",
        "\n",
        "\\begin{equation}\n",
        "L = -\\frac{1}{M} \\sum_{m=1}^M r_n^m (\\theta_n) \\tag{5}\n",
        "\\end{equation}\n",
        "\n",
        "where we specify the reward function as being\n",
        "\\begin{equation}\n",
        "r_n^m (\\theta_n) = g(n, x_n^m)F^{\\theta}(x_n^m)+ g(\\bar{\\tau}_{n+1}^m, x_{\\bar{\\tau}_{n+1}^m}^m)(1-F^{\\theta}(x_n^m) ) \\tag{6}\n",
        "\\end{equation}\n",
        "where the $g()$ function is $(3)$.\n",
        "\n",
        "### *To recap*\n",
        "At time $n$ along path $m$, the reward is equal to $(2)$ taking into account the possible events of switching or not regime with probability given by $F^{\\theta_n}$.\n",
        "\n",
        "For a large number of paths $M$, we can say that $(1)$ approximates $\\mathbb{E}[g(n, X_n)f^{\\theta}(X_n)+ g(\\tau_{n+1}, X_{\\tau_{n+1}})(1-f^{\\theta}(X_n) ) ]$. This is the function we optimize using gradient descent algorithm with respect to $\\theta_n$. The algorithm outputs the probability values of switching $F^{\\theta_n}$ for each date and for all paths considered, which we then translate into $0-1$ switching decisions $f^{\\theta_n}$.\n",
        "\n",
        "For each epoch, we use the set $\\theta_n$ from the previous epoch (where the very first set is initialized according to a uniform distribution) to compute $F^{\\theta_n}$. Then the updated (\"loss.step()\") $\\theta_n$ are obtained via backpropagation by the gradient of the loss function (\"loss.backward()\"). \n",
        "\n",
        "\n",
        "### 2) evaluate_network\n",
        "After training, the algorithm proceeds with the evaluation phase where it outputs the probability values of switching $F^{\\theta_n}$. These corresponds to the object \"stopping_probability\"."
      ],
      "metadata": {
        "id": "24zW3TAlMNxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OptimizationPart(object):\n",
        "\n",
        "  def __init__(self, assets, paths, epochs=50, batch_size=2000):\n",
        "    self.assets = assets\n",
        "    self.paths = paths\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = batch_size\n",
        "    self.network = Ftheta_NN(self.assets).double()\n",
        "    self.network.apply(init_weights)\n",
        "\n",
        "\n",
        "  def train_network(self,  stock_values, current_payoff,\n",
        "                    future_payoff):\n",
        "    optimizer = optim.Adam(self.network.parameters())\n",
        "    #future_payoff = torch.from_numpy(future_payoff).double()\n",
        "    #current_payoff = torch.from_numpy(current_payoff).double()\n",
        "    X_inputs = torch.from_numpy(stock_values).double()\n",
        "\n",
        "    self.network.train(True)\n",
        "    ones = torch.ones(self.paths)\n",
        "    for epoch in range(self.epochs):\n",
        "      optimizer.zero_grad()\n",
        "      outputs = self.network(X_inputs).reshape(-1) # probabilities\n",
        "      reward = (current_payoff * outputs ) +future_payoff * (ones - outputs) # reward function\n",
        "      loss = -torch.mean(reward) # loss function\n",
        "      loss.backward() # gradient calculation of the loss function\n",
        "      optimizer.step() # gradient descent update\n",
        "\n",
        "  def evaluate_network(self, X_inputs):\n",
        "    self.network.train(False)\n",
        "    X_inputs = torch.from_numpy(X_inputs).double()\n",
        "    outputs = self.network(X_inputs)\n",
        "    return outputs.view(X_inputs.size()[0]).detach().numpy()\n",
        "   "
      ],
      "metadata": {
        "id": "IA1la3-vzY85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class \"Recursion\"\n",
        "This class contains the main calculations of the algorithm, hence the recursion.\n",
        "\n",
        "### price\n",
        "First we start with some elements:\n",
        "1. we simulate $d$ asset prices $\\{X^i \\}^d_{i=1}$ along $m$ paths according to a geometric Brownian motion process. We consider the iys discretized version on an equidistant time grid, $t_n = n \\cdot T/N$ for $n=0, \\ldots, N$:\n",
        "\\begin{equation} \\tag{7}\n",
        "x_{n,i}^m = x_{0,i} \\cdot \\exp \\Big\\{\\sum_{k=0}^n \\Big((r-\\delta_i - \\sigma_i^2 /2) \\Delta t + \\sigma_i \\sqrt{\\Delta t} \\cdot Z_{k, i}^m  \\Big) \\Big\\}\n",
        "\\end{equation}\n",
        "where $(r-\\delta_i) \\in \\mathbb{R}$ and $\\sigma_i >0$ are the drift and volatility of the system $X$, $\\Delta t =T/N$ and $Z_{k, i}^m \\sim \\mathcal{N}(0,1)$\n",
        "\n",
        "2. we define the discount factor $\\exp \\{(r-\\delta_i )\\Delta t \\}$\n",
        "3. we create the \"regime\" object $\\mathbb{I} = \\{0, 1 \\}$\n",
        "4. we create the empty objects: \"regime_path\" that records the regime in which the process is at for every time step and path; \"Y_train\" that records the profit value for each time step and path; \"F_theta_train\" that records the presence of a stopping time\n",
        "\n",
        "\n",
        "Then we can start the recursion:\n",
        "\n",
        "*At maturity $N$*\n",
        "1. we sample a regime to specify in which regime the process is and record it in \"regime_path\". The sampled regime is the same across all paths\n",
        "2. we compute the profit at $N$ \"final_payoff\" $\\check{Y}_{N}^i$ according to $(4)$ and record it in \"Y_train\"\n",
        "\n",
        "*Before maturity, for each date $ n=N-1, \\ldots, 0$*\n",
        "1. we compute \"current_payoff\" using \"Profit_training.running()\"\n",
        "2. we obtain a stopping rule for each path, using as arguments the current payoff, the discounted final payoff (now called \"values\") and the entire process. Record then the stopping rules in \"F_theta_train\" **[I am not sure about the discounting]**\n",
        "3. we determine the current regime on the basis of the regime from the previous time step and if there has been or not a stopping time. Hence if at $n$ we have $F_n^{\\theta}=1$ and $q_{n+1} = i$, we choose $q_n=j$. We then record this in \"regime_path\"\n",
        "4. we compute $\\check{Y}_{n}^i$ according to $(4)$, using \"Profit_training.running()\" appropriate for its current regime and record it under \"Y_train\". Hence we have a matrix with dimension periods x paths, recording $\\check{Y}_{n}^i$\n",
        "5. we compute the mean estimate across paths for each date (Y_test_mean) and the standard error so that we can plot the values with a 95% CI\n"
      ],
      "metadata": {
        "id": "5anu8qXoXXOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Recursion:\n",
        "  def __init__(self, model, training, testing, nb_epochs=50):\n",
        "\n",
        "    self.model = model # argument is S    \n",
        "    self.neural_stopping = OptimizationPart(model.assets, model.paths) \n",
        "    self.profit_training = Profit_training(self.model)\n",
        "    self.profit_testing = Profit_testing(self.model)\n",
        "\n",
        "  def price(self):\n",
        "    model = self.model\n",
        "    stock_paths = self.model.simulate_process()    \n",
        "    disc_factor = np.math.exp((-model.drift) * model.maturity/(model.periods))\n",
        "    \n",
        "    # create empty objects to store values\n",
        "    k = np.array([0.4, 0.7])\n",
        "    regimes = [0, 1]\n",
        "    regime_path=np.zeros((model.periods+1, model.paths)) # record at which regime we're at at each n\n",
        "    Y_train=np.zeros((model.periods+1, model.paths))\n",
        "    F_theta_train=np.zeros((model.periods+1,model.paths)) # record switching events for each n\n",
        "\n",
        "    # at maturity N\n",
        "    final_payoff = np.array([self.profit_training.terminal(stock_paths[-1, :, :]), self.profit_training.terminal(stock_paths[-1, :, :])])   # payoff of the last date for each path.\n",
        "    future_payoff = torch.from_numpy(final_payoff*disc_factor).double() \n",
        "    Y_train[model.periods, :]= final_payoff[0]\n",
        "    F_theta_train[model.periods,:]=1 # at maturity we switch (does it matter?)\n",
        "    regime_path[model.periods, :] = random.sample(regimes, 1)[0] # sample a regime at maturity\n",
        "    values = Y_train[model.periods, :]\n",
        "    print(\"date\", model.periods, \":\", 1,\" , \", 1, \" , \", model.paths)\n",
        "\n",
        "    # recursive calc. before maturity\n",
        "    for date in range(stock_paths.shape[0] - 2, 0, -1): \n",
        "      current_payoff = self.profit_training.running(Y_train[date+1, :], stock_paths[date, :, :])\n",
        "\n",
        "      stopping_probability = self.stop(stock_paths[date, : , :], \n",
        "                                current_payoff,\n",
        "                                future_payoff)\n",
        "      \n",
        "      print(\"date\", date, \":\", round(np.min(stopping_probability),3),\" , \", round(np.max(stopping_probability),3), \" , \", len([1 for l in stopping_probability if l > 0.5])) # print the min/max stopping probabilities and the count of stopping times for each date\n",
        "      F_theta_train[date,:]=(stopping_probability > 0.5)*1.0   # transform stopping probabilities in 0-1 decision\n",
        "      which = stopping_probability > 0.5\n",
        "\n",
        "      for m in range(0,model.paths-1):\n",
        "        old_regime = regime_path[date +1, m]\n",
        "        if  which[m] == True :\n",
        "          regime_path[date , m] = regimes[~int(old_regime)] \n",
        "          gamma = self.profit_testing.g(date, m, X) + 0.7\n",
        "        else:\n",
        "          regime_path[date , m] = old_regime\n",
        "          gamma = -self.profit_testing.g(date, m, X)\n",
        "        Y_train[date, m] = self.profit_testing.running(Y = Y_train, date = date, \n",
        "                                                path = m, S=model, \n",
        "                                                X = stock_paths,  switch = F_theta_train, \n",
        "                                                gamma = gamma)\n",
        "      immediate_exercise_value = Y_train[date, :]       \n",
        "      values[which] = immediate_exercise_value[which] # when we switch we take the current profit\n",
        "      values[~which] *= disc_factor           # when we don't switch we take final profit discounted \n",
        "      Y_train[date, :] = values\n",
        "\n",
        "\n",
        "    payoff_0 = Y_train[0, :]\n",
        "    return round(max(payoff_0[0], np.mean(values)) * disc_factor,3), payoff_0, Y_train \n",
        "\n",
        "\n",
        "\n",
        "  def stop(self, stock_values, current_payoff,\n",
        "           future_payoff):\n",
        "    \n",
        "    self.neural_stopping.train_network(\n",
        "      stock_values,\n",
        "      current_payoff ,\n",
        "      future_payoff)\n",
        "\n",
        "    inputs = stock_values\n",
        "    stopping_probability = self.neural_stopping.evaluate_network(inputs)\n",
        "    return stopping_probability    "
      ],
      "metadata": {
        "id": "N2Tje8ll4AUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_begin = time.time()\n",
        "\n",
        "pricing = Recursion(S, Profit_training, Profit_testing, nb_epochs=50)\n",
        "'''\n",
        "arguments are:\n",
        "- path process\n",
        "- Profit training and profit testing classes\n",
        "- number of epochs to be used for the gradient descent algorithm\n",
        "\n",
        "'''\n",
        "duration = time.time() - t_begin\n",
        "output, payoff_0, values = pricing.price()\n",
        "\n",
        "print(round(duration, 3))\n",
        "\n",
        "'''\n",
        "here we are printing:\n",
        "- date\n",
        "- min and max probability of switching across the 5000 paths\n",
        "- number paths that will switch for each date\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "xf_lbl45l91-",
        "outputId": "714fe849-17fd-4864-a488-9b0fecbe8302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date 9 : 1  ,  1  ,  5000\n",
            "8 : 9.153968329118165e-06  ,  0.9522900785963158  ,  3832\n",
            "7 : 4.4716234911242014e-11  ,  0.9930842450265744  ,  4091\n",
            "6 : 2.35941609686681e-19  ,  0.9944858998330752  ,  4559\n",
            "5 : 3.2356072608001305e-37  ,  0.9989254859378646  ,  4771\n",
            "4 : 4.1785701648906726e-60  ,  0.9996030062723464  ,  4911\n",
            "3 : 1.5689476799351864e-63  ,  0.9999872521455605  ,  4786\n",
            "2 : 2.3104497208329197e-65  ,  0.9999999144240876  ,  4478\n",
            "1 : 6.8170872018228175e-68  ,  0.9999999995815247  ,  4506\n",
            "0.013\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nhere we are printing:\\n- date\\n- min and max probability of switching across the 5000 paths\\n- number paths that will switch for each date\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output, payoff_0)\n",
        "print(len(values))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7PJT61dgfvY",
        "outputId": "54c79502-030e-46b4-de3c-5fffc61adbdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50.35 [0. 0. 0. ... 0. 0. 0.]\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "produce table with estimated values for different spot prices and state dimension (d)\n",
        "'''\n",
        "\n",
        "\n",
        "dict ={}\n",
        " \n",
        "# Insert data into dictionary\n",
        "dict1 = {\n",
        "     1: [\"2\", 90, 26.116, 0.008],\n",
        "     2: [\"2\", 100, 38.914, 0.007],\n",
        "     3: [\"2\", 110, 50.35 , 0.013],\n",
        "     4: [\"3\", 90, 27.261, 0.009],\n",
        "     5: [\"3\", 100, 40.255, 0.01],\n",
        "     6: [\"3\", 110, 51.849, 0.007],\n",
        "     7: [\"4\", 90, 26.365 , 0.005],\n",
        "     8: [\"4\", 100, 38.953, 0.008],\n",
        "     9: [\"4\", 110, 51.501, 0.009],\n",
        "     10: [\"5\", 90, 28.973, 0.005],\n",
        "     11: [\"5\", 100, 40.239, 0.007],\n",
        "     12: [\"5\", 110, 53.313, 0.008],\n",
        "     13: [\"10\", 90, 30.754, 0.008],\n",
        "     14: [\"10\", 100, 44.809, 0.007],\n",
        "     15: [\"10\", 110, 58.627, 0.01],\n",
        "     16: [\"20\", 90, 33.514, 0.012],\n",
        "     17: [\"20\", 100, 48.429, 0.011],\n",
        "     18: [\"20\", 110, 62.239,0.011],\n",
        "     }\n",
        " \n",
        "# Print the names of the columns.\n",
        "print (\"{:<10} {:<10} {:<10} {:<10}\".format('assets', 'spot', 'L', 'timeL'))\n",
        " \n",
        "# print each data item.\n",
        "for key, value in dict1.items():\n",
        "    assets, spot, L, timeL = value\n",
        "    print (\"{:<10} {:<10} {:<10} {:<10}\".format(assets, spot, L, timeL))\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "pricing_df = pd.DataFrame.from_dict(dict1, orient='index')\n",
        "pricing_df.columns = ['assets', 'spot', 'L', 'timeL']\n",
        "pricing_df    "
      ],
      "metadata": {
        "id": "7T0HyrCyH5L6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 944
        },
        "outputId": "3ffef8f9-8367-4fcb-d744-bc79fe90bf2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assets     spot       L          timeL     \n",
            "2          90         26.116     0.008     \n",
            "2          100        38.914     0.007     \n",
            "2          110        50.35      0.013     \n",
            "3          90         27.261     0.009     \n",
            "3          100        40.255     0.01      \n",
            "3          110        51.849     0.007     \n",
            "4          90         26.365     0.005     \n",
            "4          100        38.953     0.008     \n",
            "4          110        51.501     0.009     \n",
            "5          90         28.973     0.005     \n",
            "5          100        40.239     0.007     \n",
            "5          110        53.313     0.008     \n",
            "10         90         30.754     0.008     \n",
            "10         100        44.809     0.007     \n",
            "10         110        58.627     0.01      \n",
            "20         90         33.514     0.012     \n",
            "20         100        48.429     0.011     \n",
            "20         110        62.239     0.011     \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   assets  spot       L  timeL\n",
              "1       2    90  26.116  0.008\n",
              "2       2   100  38.914  0.007\n",
              "3       2   110  50.350  0.013\n",
              "4       3    90  27.261  0.009\n",
              "5       3   100  40.255  0.010\n",
              "6       3   110  51.849  0.007\n",
              "7       4    90  26.365  0.005\n",
              "8       4   100  38.953  0.008\n",
              "9       4   110  51.501  0.009\n",
              "10      5    90  28.973  0.005\n",
              "11      5   100  40.239  0.007\n",
              "12      5   110  53.313  0.008\n",
              "13     10    90  30.754  0.008\n",
              "14     10   100  44.809  0.007\n",
              "15     10   110  58.627  0.010\n",
              "16     20    90  33.514  0.012\n",
              "17     20   100  48.429  0.011\n",
              "18     20   110  62.239  0.011"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1752110d-f08b-43b4-9d28-473aae7264a4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>assets</th>\n",
              "      <th>spot</th>\n",
              "      <th>L</th>\n",
              "      <th>timeL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>90</td>\n",
              "      <td>26.116</td>\n",
              "      <td>0.008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>100</td>\n",
              "      <td>38.914</td>\n",
              "      <td>0.007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>110</td>\n",
              "      <td>50.350</td>\n",
              "      <td>0.013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>90</td>\n",
              "      <td>27.261</td>\n",
              "      <td>0.009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3</td>\n",
              "      <td>100</td>\n",
              "      <td>40.255</td>\n",
              "      <td>0.010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>110</td>\n",
              "      <td>51.849</td>\n",
              "      <td>0.007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>4</td>\n",
              "      <td>90</td>\n",
              "      <td>26.365</td>\n",
              "      <td>0.005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>4</td>\n",
              "      <td>100</td>\n",
              "      <td>38.953</td>\n",
              "      <td>0.008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4</td>\n",
              "      <td>110</td>\n",
              "      <td>51.501</td>\n",
              "      <td>0.009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>5</td>\n",
              "      <td>90</td>\n",
              "      <td>28.973</td>\n",
              "      <td>0.005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>5</td>\n",
              "      <td>100</td>\n",
              "      <td>40.239</td>\n",
              "      <td>0.007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>5</td>\n",
              "      <td>110</td>\n",
              "      <td>53.313</td>\n",
              "      <td>0.008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>10</td>\n",
              "      <td>90</td>\n",
              "      <td>30.754</td>\n",
              "      <td>0.008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>10</td>\n",
              "      <td>100</td>\n",
              "      <td>44.809</td>\n",
              "      <td>0.007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>10</td>\n",
              "      <td>110</td>\n",
              "      <td>58.627</td>\n",
              "      <td>0.010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>20</td>\n",
              "      <td>90</td>\n",
              "      <td>33.514</td>\n",
              "      <td>0.012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>20</td>\n",
              "      <td>100</td>\n",
              "      <td>48.429</td>\n",
              "      <td>0.011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>20</td>\n",
              "      <td>110</td>\n",
              "      <td>62.239</td>\n",
              "      <td>0.011</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1752110d-f08b-43b4-9d28-473aae7264a4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1752110d-f08b-43b4-9d28-473aae7264a4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1752110d-f08b-43b4-9d28-473aae7264a4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "in case I need to check steps\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "model = S # argument is S    \n",
        "profit_training = Profit_training(model) #class profit. the argument is Profit, then you can call profit.terminal, etc\n",
        "epochs = 50\n",
        "stock_paths = model.GBM()    \n",
        "disc_factor = np.math.exp((-model.drift) * model.maturity/(model.periods))\n",
        "\n",
        "k = np.array([0.4, 0.7])\n",
        "regimes = [0, 1]\n",
        "regime_path=np.zeros((S.periods+1, S.paths)) # record at which regime we're at at each n\n",
        "Y_train=np.zeros((S.periods+1, S.paths))\n",
        "F_theta_train=np.zeros((S.periods+1,S.paths)) # record switching events for each n\n",
        "\n",
        "# at maturity N\n",
        "final_payoff = np.array([profit_training.terminal(stock_paths[-1, :, :]), profit_training.terminal(stock_paths[-1, :, :])])   # payoff of the last date for each path. \n",
        "Y_train[S.periods, :]= final_payoff[0]\n",
        "\n",
        "F_theta_train[S.periods,:]=1 # at maturity we switch (does it matter?)\n",
        "regime_path[S.periods, :] = random.sample(regimes, 1)[0] # sample a regime at maturity\n",
        "neural_stopping = OptimizationPart(model.assets, model.paths) \n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "date = stock_paths.shape[0]-2\n",
        "current_payoff =  profit_training.running(Y_train[date+1, :], stock_paths[date, :, :])\n",
        "\n",
        "network = Ftheta_NN(model.assets).double()\n",
        "network.apply(init_weights)\n",
        "optimizer = optim.Adam(network.parameters())\n",
        "print(type(final_payoff))\n",
        "future_payoff = torch.from_numpy(final_payoff*disc_factor).double()\n",
        "X_inputs = torch.from_numpy(stock_paths[date, :, :]).double()\n",
        "print(\"final\", torch.from_numpy(final_payoff).double())\n",
        "print(\"current\", current_payoff.size(),current_payoff)\n",
        "print(\"X_inopts\", X_inputs)\n",
        "print(\"values\", final_payoff)\n",
        "print(\"future\", future_payoff.size(), future_payoff)\n",
        "print(network(X_inputs))\n",
        "network.train(True)\n",
        "ones = torch.ones(S.paths)\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  optimizer.zero_grad()\n",
        "  outputs = network(X_inputs).reshape(-1) # probabilities\n",
        "  reward = (current_payoff * outputs +\n",
        "              future_payoff * (ones - outputs)) # reward function\n",
        "  print(len(outputs) ,type(outputs), len(current_payoff), len(future_payoff))            \n",
        "  loss = -torch.mean(reward) # loss function\n",
        "  loss.backward() # gradient calculation of the loss function\n",
        "  optimizer.step() # gradient descent update\n",
        "\n",
        "\n",
        "network.train(False)\n",
        "X_inputs = torch.from_numpy(stock_paths[date, :, :]).double()\n",
        "outputs = network(X_inputs)\n",
        "stopping_probablities =outputs.view(X_inputs.size()[0]).detach().numpy()\n",
        "print(stopping_probablities)\n",
        "\n",
        "print(date, \":\", np.min(stopping_probablities),\" , \", np.max(stopping_probablities), \" , \", len([1 for l in stopping_probablities if l > 0.5]))\n",
        "F_theta_train[date,:]=(stopping_probablities > 0.5)*1.0   \n",
        "which = stopping_probablities > 0.5\n",
        "print(len(stopping_probablities), model.paths)\n",
        "profit_testing = Profit_testing(model)\n",
        "print(which)\n",
        "for m in range(0, model.paths):\n",
        "         old_regime = regime_path[date +1, m]\n",
        "         if  which[m] == True :\n",
        "           regime_path[date , m] = regimes[~int(old_regime)] \n",
        "           gamma = profit_testing.g(date, m, stock_paths) + 0.7\n",
        "         else:\n",
        "           regime_path[date , m] = old_regime\n",
        "           gamma = -profit_testing.g(date, m, X)\n",
        "         Y_train[date, m] = profit_testing.running(Y = Y_train, date = date, \n",
        "                                                path = m, S=model, \n",
        "                                                X = stock_paths,  switch = F_theta_train, \n",
        "                                                gamma = gamma)\n",
        "values = Y_train[S.periods, :]\n",
        "immediate_exercise_value = Y_train[date, :]       \n",
        "values[which] = immediate_exercise_value[which] # when we switch we take the current profit\n",
        "values[~which] *= disc_factor           # when we don't switch we take final profit discounted (?)\n",
        "\n",
        "print(\"immediate\", immediate_exercise_value)\n",
        "print(\"values\", values)\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "jugaRK4rXBDR",
        "outputId": "a6f70bd3-c0ff-4460-bc5a-90f93bfa62da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nmodel = S # argument is S    \\nprofit_training = Profit_training(model) #class profit. the argument is Profit, then you can call profit.terminal, etc\\nepochs = 50\\nstock_paths = model.GBM()    \\ndisc_factor = np.math.exp((-model.drift) * model.maturity/(model.periods))\\n\\nk = np.array([0.4, 0.7])\\nregimes = [0, 1]\\nregime_path=np.zeros((S.periods+1, S.paths)) # record at which regime we\\'re at at each n\\nY_train=np.zeros((S.periods+1, S.paths))\\nF_theta_train=np.zeros((S.periods+1,S.paths)) # record switching events for each n\\n\\n# at maturity N\\nfinal_payoff = np.array([profit_training.terminal(stock_paths[-1, :, :]), profit_training.terminal(stock_paths[-1, :, :])])   # payoff of the last date for each path. \\nY_train[S.periods, :]= final_payoff[0]\\n\\nF_theta_train[S.periods,:]=1 # at maturity we switch (does it matter?)\\nregime_path[S.periods, :] = random.sample(regimes, 1)[0] # sample a regime at maturity\\nneural_stopping = OptimizationPart(model.assets, model.paths) \\n\\n    \\n\\n\\ndate = stock_paths.shape[0]-2\\ncurrent_payoff =  profit_training.running(Y_train[date+1, :], stock_paths[date, :, :])\\n\\nnetwork = Ftheta_NN(model.assets).double()\\nnetwork.apply(init_weights)\\noptimizer = optim.Adam(network.parameters())\\nprint(type(final_payoff))\\nfuture_payoff = torch.from_numpy(final_payoff*disc_factor).double()\\nX_inputs = torch.from_numpy(stock_paths[date, :, :]).double()\\nprint(\"final\", torch.from_numpy(final_payoff).double())\\nprint(\"current\", current_payoff.size(),current_payoff)\\nprint(\"X_inopts\", X_inputs)\\nprint(\"values\", final_payoff)\\nprint(\"future\", future_payoff.size(), future_payoff)\\nprint(network(X_inputs))\\nnetwork.train(True)\\nones = torch.ones(S.paths)\\n\\n\\nfor epoch in range(epochs):\\n  optimizer.zero_grad()\\n  outputs = network(X_inputs).reshape(-1) # probabilities\\n  reward = (current_payoff * outputs +\\n              future_payoff * (ones - outputs)) # reward function\\n  print(len(outputs) ,type(outputs), len(current_payoff), len(future_payoff))            \\n  loss = -torch.mean(reward) # loss function\\n  loss.backward() # gradient calculation of the loss function\\n  optimizer.step() # gradient descent update\\n\\n\\nnetwork.train(False)\\nX_inputs = torch.from_numpy(stock_paths[date, :, :]).double()\\noutputs = network(X_inputs)\\nstopping_probablities =outputs.view(X_inputs.size()[0]).detach().numpy()\\nprint(stopping_probablities)\\n\\nprint(date, \":\", np.min(stopping_probablities),\" , \", np.max(stopping_probablities), \" , \", len([1 for l in stopping_probablities if l > 0.5]))\\nF_theta_train[date,:]=(stopping_probablities > 0.5)*1.0   \\nwhich = stopping_probablities > 0.5\\nprint(len(stopping_probablities), model.paths)\\nprofit_testing = Profit_testing(model)\\nprint(which)\\nfor m in range(0, model.paths):\\n         old_regime = regime_path[date +1, m]\\n         if  which[m] == True :\\n           regime_path[date , m] = regimes[~int(old_regime)] \\n           gamma = profit_testing.g(date, m, stock_paths) + 0.7\\n         else:\\n           regime_path[date , m] = old_regime\\n           gamma = -profit_testing.g(date, m, X)\\n         Y_train[date, m] = profit_testing.running(Y = Y_train, date = date, \\n                                                path = m, S=model, \\n                                                X = stock_paths,  switch = F_theta_train, \\n                                                gamma = gamma)\\nvalues = Y_train[S.periods, :]\\nimmediate_exercise_value = Y_train[date, :]       \\nvalues[which] = immediate_exercise_value[which] # when we switch we take the current profit\\nvalues[~which] *= disc_factor           # when we don\\'t switch we take final profit discounted (?)\\n\\nprint(\"immediate\", immediate_exercise_value)\\nprint(\"values\", values)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ]
}